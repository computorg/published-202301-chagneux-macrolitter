<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.21">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mathis Chagneux">
<meta name="author" content="Sylvain Le Corff">
<meta name="author" content="Pierre Gloaguen">
<meta name="author" content="Charles Ollion">
<meta name="author" content="Océane Lepâtre">
<meta name="author" content="Antoine Bruge">
<meta name="dcterms.date" content="2023-02-16">

<title>Macrolitter video counting on riverbanks using state space models and moving cameras</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="published-202301-chagneux-macrolitter_files/libs/clipboard/clipboard.min.js"></script>
<script src="published-202301-chagneux-macrolitter_files/libs/quarto-html/quarto.js"></script>
<script src="published-202301-chagneux-macrolitter_files/libs/quarto-html/popper.min.js"></script>
<script src="published-202301-chagneux-macrolitter_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="published-202301-chagneux-macrolitter_files/libs/quarto-html/anchor.min.js"></script>
<link href="published-202301-chagneux-macrolitter_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="published-202301-chagneux-macrolitter_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="published-202301-chagneux-macrolitter_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="published-202301-chagneux-macrolitter_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="published-202301-chagneux-macrolitter_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #FFFFFF;
      }

      .quarto-title-block .quarto-title-banner {
        color: #FFFFFF;
background: #034E79;
      }
</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="Macrolitter video counting on riverbanks using state space models and moving cameras ">
<meta name="citation_abstract" content="Litter is a known cause of degradation in marine environments and most of it travels in rivers before reaching the oceans. In this paper, we present a novel algorithm to assist waste monitoring along watercourses. While several attempts have been made to quantify litter using neural object detection in photographs of floating items, we tackle the more challenging task of counting directly in videos using boat-embedded cameras. We rely on multi-object tracking (MOT) but focus on the key pitfalls of false and redundant counts which arise in typical scenarios of poor detection performance. Our system only requires supervision at the image level and performs Bayesian filtering via a state space model based on optical flow. We present a new open image dataset gathered through a crowdsourced campaign and used to train a center-based anchor-free object detector. Realistic video footage assembled by water monitoring experts is annotated and provided for evaluation. Improvements in count quality are demonstrated against systems built from state-of-the-art multi-object trackers sharing the same detection capabilities. A precise error decomposition allows clear analysis and highlights the remaining challenges.
">
<meta name="citation_author" content="Mathis Chagneux">
<meta name="citation_author" content="Sylvain Le Corff">
<meta name="citation_author" content="Pierre Gloaguen">
<meta name="citation_author" content="Charles Ollion">
<meta name="citation_author" content="Océane Lepâtre">
<meta name="citation_author" content="Antoine Bruge">
<meta name="citation_publication_date" content="2023-02-16">
<meta name="citation_cover_date" content="2023-02-16">
<meta name="citation_year" content="2023">
<meta name="citation_online_date" content="2023-02-16">
<meta name="citation_fulltext_html_url" content="https://computo.sfds.asso.fr/published-202301-chagneux-macrolitter/">
<meta name="citation_pdf_url" content="https://computo.sfds.asso.fr/published-202301-chagneux-macrolitter/published-202301-chagneux-macrolitter.pdf">
<meta name="citation_doi" content="10.57750/845m-f805">
<meta name="citation_issn" content="2824-7795">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="Computo">
<meta name="citation_publisher" content="French Statistical Society">
<meta name="citation_reference" content="citation_title=A metric on the space of finite sets of trajectories for evaluation of multi-target tracking algorithms;,citation_abstract=In this paper, we propose a metric on the space of finite sets of trajectories for assessing multi-target tracking algorithms in a mathematically sound way. The main use of the metric is to compare estimates of trajectories from different algorithms with the ground truth of trajectories. The proposed metric includes intuitive costs associated to localization error for properly detected targets, missed and false targets and track switches at each time step. The metric computation is based on solving a multi-dimensional assignment problem. We also propose a lower bound for the metric, which is also a metric for sets of trajectories and is computable in polynomial time using linear programming. We also extend the proposed metrics on sets of trajectories to random finite sets of trajectories.;,citation_author=Ángel F. García-Fernández;,citation_author=Abu Sajana Rahmathullah;,citation_author=Lennart Svensson;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.1109/TSP.2020.3005309;,citation_issn=1053-587X, 1941-0476;,citation_volume=68;,citation_journal_title=IEEE Transactions on Signal Processing;">
<meta name="citation_reference" content="citation_title=Multitarget bayes filtering via first-order multitarget moments;,citation_author=R. P. S. Mahler;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_issue=4;,citation_doi=10.1109/TAES.2003.1261119;,citation_volume=39;,citation_journal_title=IEEE Transactions on Aerospace and Electronic Systems;">
<meta name="citation_reference" content="citation_title=Deep layer aggregation;,citation_author=Fisher Yu;,citation_author=Dequan Wang;,citation_author=Evan Shelhamer;,citation_author=Trevor Darrell;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.1109/CVPR.2018.00255;,citation_conference_title=2018 IEEE/CVF conference on computer vision and pattern recognition;">
<meta name="citation_reference" content="citation_title=Handbook of mathematical models in computer vision;,citation_author=Nikos Paragios;,citation_author=Yunmei Chen;,citation_author=Olivier D Faugeras;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;">
<meta name="citation_reference" content="citation_title=Sources, fate and effects of microplastics in the marine environment: Part 2 of a global assessment;,citation_author=Chelsea Rochman;,citation_author=Anthony Andrady;,citation_author=Sarah Dudas;,citation_author=Joan Fabres;,citation_author=François Galgani;,citation_author=Denise lead;,citation_author=Valeria Hidalgo-Ruz;,citation_author=Sunny Hong;,citation_author=Peter Kershaw;,citation_author=Laurent Lebreton;,citation_author=Amy Lusher;,citation_author=Ramani Narayan;,citation_author=Sabine Pahl;,citation_author=James Potemra;,citation_author=Chelsea Rochman;,citation_author=Sheck Sherif;,citation_author=Joni Seager;,citation_author=Won Shim;,citation_author=Paula Sobral;,citation_author=Linda Amaral-Zettler;,citation_publication_date=2016-12;,citation_cover_date=2016-12;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=Floating macrolitter leaked from europe into the ocean;,citation_author=D. González-Fernández;,citation_author=A. Cózar;,citation_author=G. Hanke;,citation_author=J. Viejo;,citation_author=C. Morales-Caselles;,citation_author=R. Bakiu;,citation_author=D. Barcelo;,citation_author=F. Bessa;,citation_author=Antoine Bruge;,citation_author=M. Cabrera;,citation_author=J. Castro-Jiménez;,citation_author=M. Constant;,citation_author=R. Crosti;,citation_author=Yuri Galletti;,citation_author=A. Kideyş;,citation_author=N. Machitadze;,citation_author=Joana Pereira Brito;,citation_author=M. Pogojeva;,citation_author=N. Ratola;,citation_author=J. Rigueira;,citation_author=E. Rojo-Nieto;,citation_author=O. Savenko;,citation_author=R. I. Schöneich-Argent;,citation_author=G. Siedlewicz;,citation_author=Giuseppe Suaria;,citation_author=Myrto Tourgeli;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=4;,citation_journal_title=Nature Sustainability;">
<meta name="citation_reference" content="citation_title=Marine pollution. Plastic waste inputs from land into the ocean;,citation_author=Jenna Jambeck;,citation_author=Roland Geyer;,citation_author=Chris Wilcox;,citation_author=Theodore Siegler;,citation_author=Miriam Perryman;,citation_author=Anthony Andrady;,citation_author=Ramani Narayan;,citation_author=Kara Law;,citation_publication_date=2015-02;,citation_cover_date=2015-02;,citation_year=2015;,citation_doi=10.1126/science.1260352;,citation_volume=347;,citation_journal_title=Science (New York, N.Y.);">
<meta name="citation_reference" content="citation_title=The environmental impacts of plastic pollution;,citation_author=Natalie Welden;,citation_publication_date=2020-01;,citation_cover_date=2020-01;,citation_year=2020;,citation_doi=10.1016/B978-0-12-817880-5.00008-6;,citation_isbn=9780128178805;">
<meta name="citation_reference" content="citation_title=Plastic pollution in the marine environment;,citation_author=Thushari Gamage;,citation_author=J. D. M. Senevirathna;,citation_publication_date=2020-08;,citation_cover_date=2020-08;,citation_year=2020;,citation_doi=10.1016/j.heliyon.2020.e04709;,citation_volume=6;,citation_journal_title=Heliyon;">
<meta name="citation_reference" content="citation_title=Production, use, and fate of all plastics ever made;,citation_author=Roland Geyer;,citation_author=Jenna Jambeck;,citation_author=Kara Law;,citation_publication_date=2017-07;,citation_cover_date=2017-07;,citation_year=2017;,citation_doi=10.1126/sciadv.1700782;,citation_volume=3;,citation_journal_title=Science Advances;">
<meta name="citation_reference" content="citation_title=Bayesian filtering and smoothing;,citation_author=S. Särkkä;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;">
<meta name="citation_reference" content="citation_title=Maximum likelihood from incomplete data via the EM algorithm;,citation_author=Arthur P Dempster;,citation_author=Nan M Laird;,citation_author=Donald B Rubin;,citation_publication_date=1977;,citation_cover_date=1977;,citation_year=1977;,citation_issue=1;,citation_volume=39;,citation_journal_title=Journal of the Royal Statistical Society: Series B (Methodological);,citation_publisher=Wiley Online Library;">
<meta name="citation_reference" content="citation_title=Nonlinear time series: Theory, methods and applications with r examples;,citation_author=R. Douc;,citation_author=É. Moulines;,citation_author=D. Stoffer;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Two-frame motion estimation based on polynomial expansion;,citation_author=G. Farnebäck;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_conference_title=Scandinavian conference on image analysis;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=The hungarian method for the assignment problem;,citation_abstract=Abstract Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.;,citation_author=H. W. Kuhn;,citation_publication_date=1955;,citation_cover_date=1955;,citation_year=1955;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109;,citation_issue=1-2;,citation_doi=https://doi.org/10.1002/nav.3800020109;,citation_volume=2;,citation_journal_title=Naval Research Logistics Quarterly;">
<meta name="citation_reference" content="citation_title=Advances in machine learning, first asian conference on machine learning, ACML 2009, nanjing, china, november 2-4, 2009. proceedings;,citation_editor=Zhi-Hua Zhou;,citation_editor=Takashi Washio;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_fulltext_html_url=http://dx.doi.org/10.1007/978-3-642-05224-8;,citation_doi=10.1007/978-3-642-05224-8;,citation_isbn=978-3-642-05223-1;,citation_volume=5828;,citation_series_title=Lecture notes in computer science;">
<meta name="citation_reference" content="citation_title=Proceedings of the 2nd asian conference on machine learning, ACML 2010, tokyo, japan, november 8-10, 2010;,citation_editor=Masashi Sugiyama;,citation_editor=Qiang Yang;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_fulltext_html_url=http://jmlr.org/proceedings/papers/v13/;,citation_volume=13;,citation_series_title=JMLR proceedings;">
<meta name="citation_reference" content="citation_title=Proceedings of the 3rd asian conference on machine learning, ACML 2011, taoyuan, taiwan, november 13-15, 2011;,citation_editor=Chun-Nan Hsu;,citation_editor=Wee Sun Lee;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_fulltext_html_url=http://jmlr.org/proceedings/papers/v20/;,citation_volume=20;,citation_series_title=JMLR proceedings;">
<meta name="citation_reference" content="citation_title=Proceedings of the 4th asian conference on machine learning, ACML 2012, singapore, singapore, november 4-6, 2012;,citation_editor=Steven C. H. Hoi;,citation_editor=Wray L. Buntine;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_fulltext_html_url=http://jmlr.org/proceedings/papers/v25/;,citation_volume=25;,citation_series_title=JMLR proceedings;">
<meta name="citation_reference" content="citation_title=Asian conference on machine learning, ACML 2013, canberra, ACT, australia, november 13-15, 2013;,citation_editor=Cheng Soon Ong;,citation_editor=Tu Bao Ho;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_fulltext_html_url=http://jmlr.org/proceedings/papers/v29/;,citation_volume=29;,citation_series_title=JMLR proceedings;">
<meta name="citation_reference" content="citation_title=Robust Fruit Counting: Combining Deep Learning, Tracking, and Structure from Motion;,citation_abstract=We present a novel fruit counting pipeline that combines deep segmentation, frame to frame tracking, and 3D localization to accurately count visible fruits across a sequence of images. Our pipeline works on image streams from a monocular camera, both in natural light, as well as with controlled illumination at night. We first train a Fully Convolutional Network (FCN) and segment video frame images into fruit and non-fruit pixels. We then track fruits across frames using the Hungarian Algorithm where the objective cost is determined from a Kalman Filter corrected Kanade-Lucas-Tomasi (KLT) Tracker. In order to correct the estimated count from tracking process, we combine tracking results with a Structure from Motion (SfM) algorithm to calculate relative 3D locations and size estimates to reject outliers and double counted fruit tracks. We evaluate our algorithm by comparing with ground-truth human-annotated visual counts. Our results demonstrate that our pipeline is able to accurately and reliably count fruits across image sequences, and the correction step can significantly improve the counting accuracy and robustness. Although discussed in the context of fruit counting, our work can extend to detection, tracking, and counting of a variety of other stationary features of interest such as leaf-spots, wilt, and blossom.;,citation_author=Xu Liu;,citation_author=Steven W. Chen;,citation_author=Shreyas Aditya;,citation_author=Nivedha Sivakumar;,citation_author=Sandeep Dcunha;,citation_author=Chao Qu;,citation_author=Camillo J. Taylor;,citation_author=Jnaneshwar Das;,citation_author=Vijay Kumar;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://arxiv.org/abs/1804.00307;,citation_doi=10.1109/IROS.2018.8594239;,citation_isbn=9781538680940;,citation_issn=21530866;,citation_conference_title=IEEE international conference on intelligent robots and systems;">
<meta name="citation_reference" content="citation_title=Objects as points;,citation_abstract=Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point — the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1;,citation_author=Xingyi Zhou;,citation_author=Dequan Wang;,citation_author=Philipp Krähenbühl;,citation_publication_date=2019-04;,citation_cover_date=2019-04;,citation_year=2019;,citation_fulltext_html_url=http://arxiv.org/abs/1904.07850;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=CenterNet: Keypoint triplets for object detection;,citation_abstract=In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efficient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of 47.0;,citation_author=Kaiwen Duan;,citation_author=Song Bai;,citation_author=Lingxi Xie;,citation_author=Honggang Qi;,citation_author=Qingming Huang;,citation_author=Qi Tian;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://github.com/;">
<meta name="citation_reference" content="citation_title=Deep learning in video multi-object tracking: A survey;,citation_abstract=The problem of Multiple Object Tracking (MOT) consists in following the trajectory of different objects in a sequence, usually a video. In recent years, with the rise of Deep Learning, the algorithms that provide a solution to this problem have benefited from the representational power of deep models. This paper provides a comprehensive survey on works that employ Deep Learning models to solve the task of MOT on single-camera videos. Four main steps in MOT algorithms are identified, and an in-depth review of how Deep Learning was employed in each one of these stages is presented. A complete experimental comparison of the presented works on the three MOTChallenge datasets is also provided, identifying a number of similarities among the top-performing methods and presenting some possible future research directions.;,citation_author=Gioele Ciaparrone;,citation_author=Francisco Luque Sánchez;,citation_author=Siham Tabik;,citation_author=Luigi Troiano;,citation_author=Roberto Tagliaferri;,citation_author=Francisco Herrera;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/1907.12740;,citation_doi=10.1016/j.neucom.2019.11.023;,citation_issn=18728286;,citation_volume=381;,citation_journal_title=Neurocomputing;">
<meta name="citation_reference" content="citation_title=Fairmot: On the fairness of detection and re-identification in multiple object tracking;,citation_author=Yifu Zhang;,citation_author=Chunyu Wang;,citation_author=Xinggang Wang;,citation_author=Wenjun Zeng;,citation_author=Wenyu Liu;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=International Journal of Computer Vision;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Tracking objects as points;,citation_author=Xingyi Zhou;,citation_author=Vladlen Koltun;,citation_author=Philipp Krähenbühl;,citation_publication_date=2020-10;,citation_cover_date=2020-10;,citation_year=2020;,citation_doi=10.1007/978-3-030-58548-8_28;,citation_isbn=978-3-030-58547-1;">
<meta name="citation_reference" content="citation_title=River plastic emissions to the world’s oceans;,citation_abstract=Plastics in the marine environment have become a major concern because of their persistence at sea, and adverse consequences to marine life and potentially human health. Implementing mitigation strategies requires an understanding and quantification of marine plastic sources, taking spatial and temporal variability into account. Here we present a global model of plastic inputs from rivers into oceans based on waste management, population density and hydrological information. Our model is calibrated against measurements available in the literature. We estimate that between 1.15 and 2.41 million tonnes of plastic waste currently enters the ocean every year from rivers, with over 74;,citation_author=Laurent C. M. Lebreton;,citation_author=Joost Van Der Zwet;,citation_author=Jan Willem Damsteeg;,citation_author=Boyan Slat;,citation_author=Anthony Andrady;,citation_author=Julia Reisser;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=http://dx.doi.org/10.1038/ncomms15611;,citation_doi=10.1038/ncomms15611;,citation_issn=20411723;,citation_pmid=28589961;,citation_volume=8;,citation_journal_title=Nature Communications;,citation_publisher=Nature Publishing Group;">
<meta name="citation_reference" content="citation_title=Monitoring litter inputs from the Adour river (southwest France) to the marine environment;,citation_abstract=Rivers are major pathways for litter to enter the ocean, especially plastic debris. Yet, further research is needed to improve knowledge on rivers contribution, increase data availability, refine litter origins, and develop relevant solutions to limit riverine litter inputs. This study presents the results of three years of aquatic litter monitoring on the Adour river catchment (southwest of France). Litter monitoring consisted of collecting all litter stranded on river banks or stuck in the riparian vegetation in defined areas identified from cartographic and hydromorphological analyses, and with the support of local stakeholders. Litter samples were then sorted and counted according to a list of items containing 130 categories. Since 2014, 278 litter samplings were carried out, and 120,632 litter items were collected, sorted, and counted. 41;,citation_author=Antoine Bruge;,citation_author=Cristina Barreau;,citation_author=Jérémy Carlot;,citation_author=Hélène Collin;,citation_author=Clément Moreno;,citation_author=Philippe Maison;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=1;,citation_doi=10.3390/jmse6010024;,citation_issn=20771312;,citation_volume=6;,citation_journal_title=Journal of Marine Science and Engineering;">
<meta name="citation_reference" content="citation_title=Seine Plastic Debris Transport Tenfolded During Increased River Discharge;,citation_abstract=Rivers transport land-based plastic waste into the ocean. Current efforts to quantify riverine plastic emission come with uncertainty as field observations are scarce. One of the challenging aspects is the lack of consistent measurement methods that allow for comparing rivers over space and time. Recent studies have shown that simple visual observations provide a robust first-order characterization of floating and superficially suspended plastic transport, both in quantity, spatiotemporal distribution and composition. For this study, we applied this method to the river Seine, France, to provide new insights in the spatiotemporal variation in riverine plastic transport. First, we studied the response of plastic flow to increased river discharge by comparing measurements taken during low flow and high flow periods. Second, we investigated the variation of riverine plastic transport over the river length to improve our understanding of the origin and fate of riverine plastics. We demonstrate that during a period with higher river discharge, plastic transport increased up to a factor ten at the observation point closest to the river mouth. This suggests that the plastic emission into the ocean from the Seine may also be considerably higher during increased discharge. Upstream of Paris plastic transport increased only with a factor 1.5, suggesting that most plastics originate from Paris or areas further downstream. With this paper we aim to shed additional light on the seasonal variation in riverine plastic transport and its distribution along the river length, which may benefit future long-term monitoring efforts and plastic pollution mitigation strategies.;,citation_author=Tim Emmerik;,citation_author=Romain Tramoy;,citation_author=Caroline Calcar;,citation_author=Soline Alligant;,citation_author=Robin Treilles;,citation_author=Bruno Tassin;,citation_author=Johnny Gasperi;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=October;,citation_doi=10.3389/fmars.2019.00642;,citation_issn=22967745;,citation_volume=6;,citation_journal_title=Frontiers in Marine Science;">
<meta name="citation_reference" content="citation_title=Plastic debris in rivers;,citation_abstract=Abstract Plastic pollution in aquatic ecosystems is an emerging environmental risk, as it may negatively impacts ecology, endangers aquatic species, and causes economic damage. Rivers are known to play a crucial role in transporting land-based plastic waste to the world’s oceans, but riverine ecosystems are also directly affected by plastic pollution. To better quantify global plastic pollution transport and to effectively reduce sources and risks, a thorough understanding of origin, transport, fate, and effects of riverine plastic debris is crucial. In this overview paper, we discuss the current scientific state on plastic debris in rivers and evaluate existing knowledge gaps. We present a brief background of plastics, polymer types typically found in rivers, and the risk posed to aquatic ecosystems. Additionally, we elaborate on the origin and fate of riverine plastics, including processes and factors influencing plastic debris transport and its spatiotemporal variation. We present an overview of monitoring and modeling efforts to characterize riverine plastic transport, and give examples of typical values from around the world. Finally, we present an outlook to riverine plastic research. With this paper, we aim to present an inclusive and comprehensive overview of riverine plastic debris research to date and suggest multiple ways forward for future research. This article is categorized under: Science of Water &amp;amp;amp;gt; Water Quality Water and Life &amp;gt; Stresses and Pressures on Ecosystems;,citation_author=Tim Emmerik;,citation_author=Anna Schwarz;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1398;,citation_issue=1;,citation_doi=https://doi.org/10.1002/wat2.1398;,citation_volume=7;,citation_journal_title=WIREs Water;">
<meta name="citation_reference" content="citation_title=Seine plastic debris transport tenfolded during increased river discharge;,citation_abstract=Rivers transport land-based plastic waste into the ocean. Current efforts to quantify riverine plastic emission come with uncertainty as field observations are scarce. One of the challenging aspects is the lack of consistent measurement methods that allow for comparing rivers over space and time. Recent studies have shown that simple visual observations provide a robust first-order characterization of floating and superficially suspended plastic transport, both in quantity, spatiotemporal distribution and composition. For this study, we applied this method to the river Seine, France, to provide new insights in the spatiotemporal variation in riverine plastic transport. First, we studied the response of plastic flow to increased river discharge by comparing measurements taken during low flow and high flow periods. Second, we investigated the variation of riverine plastic transport over the river length to improve our understanding of the origin and fate of riverine plastics. We demonstrate that during a period with higher river discharge, plastic transport increased up to a factor ten at the observation point closest to the river mouth. This suggests that the plastic emission into the ocean from the Seine may also be considerably higher during increased discharge. Upstream of Paris plastic transport increased only with a factor 1.5, suggesting that most plastics originate from Paris or areas further downstream. With this paper we aim to shed additional light on the seasonal variation in riverine plastic transport and its distribution along the river length, which may benefit future long-term monitoring efforts and plastic pollution mitigation strategies.;,citation_author=Tim Emmerik;,citation_author=Romain Tramoy;,citation_author=Caroline Calcar;,citation_author=Soline Alligant;,citation_author=Robin Treilles;,citation_author=Bruno Tassin;,citation_author=Johnny Gasperi;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://www.frontiersin.org/article/10.3389/fmars.2019.00642;,citation_doi=10.3389/fmars.2019.00642;,citation_issn=2296-7745;,citation_volume=6;,citation_journal_title=Frontiers in Marine Science;">
<meta name="citation_reference" content="citation_title=Macro-litter in surface waters from the Rhone River: Plastic pollution and loading to the NW Mediterranean Sea;,citation_abstract=We present here the first estimates of floating macro-litter in surface waters from the Rhone River, based on monthly visual observations during 1-year period (2016–2017). Plastic represented 77;,citation_author=Javier Castro-Jiménez;,citation_author=Daniel González-Fernández;,citation_author=Michel Fornier;,citation_author=Natascha Schmidt;,citation_author=Richard Sempéré;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://doi.org/10.1016/j.marpolbul.2019.05.067;,citation_issue=May;,citation_doi=10.1016/j.marpolbul.2019.05.067;,citation_issn=18793363;,citation_pmid=31426199;,citation_volume=146;,citation_journal_title=Marine Pollution Bulletin;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Estimating Norway lobster abundance from deep-water videos: An automatic approach;,citation_abstract=Video technology has been playing an increasing role in marine science, both for habitat mapping and estimating commercial species abundance. However, when quantification is needed, it is usually based on manual counting, a subjective and time-consuming task. The present work proposes a methodology to automatically quantify the abundance of Norway lobsters, Nephrops norvegicus, by counting lobsters or their burrows from video sequences, as a reliable complement to the currently used operator-based approach. The methodology is validated using a set of test video sequences captured at the Portuguese continental slope, using a monochrome camera mounted on a trawl gear, being characterised by non-uniform illumination, artefacts at image border, noise and marine snow. The analysis includes, after a pre-processing stage, the segmentation of regions of interest and the corresponding classification into one of the three targeted classes: Norway lobsters, burrows and others (including trawl impact marks). The developed software prototype, named IT-IPIMAR N. norvegicus (I2N2), is able to provide an objective, detailed and comprehensive analysis to complement manual evaluation, for lobster and burrow density estimation.  2012 The Institution of Engineering and Technology.;,citation_author=P. Y. Lau;,citation_author=P. L. Correia;,citation_author=P. Fonseca;,citation_author=A. Campos;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=1;,citation_doi=10.1049/iet-ipr.2009.0426;,citation_issn=17519659;,citation_volume=6;,citation_journal_title=IET Image Processing;">
<meta name="citation_reference" content="citation_title=AquaVision: Automating the detection of waste in water bodies using deep transfer learning;,citation_abstract=Water pollution is one of serious threats in the society. More than 8 million tons of plastic are dumped in the oceans each year. In addition to that beaches are littered by tourists and residents all around the world. It is no secret that the aquatic life ecosystem is at a risk and soon the ratio of plastic to fish will be 1:1. In this paper, we have proposed a data set known as AquaTrash which is based on TACO data set. Further, we have applied proposed state-of-the-art deep learning-based object detection model known as AquaVision over AquaTrash dataset. Proposed model detects and classifies the different pollutants and harmful waste items floating in the oceans and on the seashores with mean Average Precision (mAP) of 0.8148. The propose method localizes the waste object that help in cleaning the water bodies and contributes to environment by maintaining the aquatic ecosystem.;,citation_author=Harsh Panwar;,citation_author=P K Gupta;,citation_author=Mohammad Khubeb Siddiqui;,citation_author=Ruben Morales-Menendez;,citation_author=Prakhar Bhardwaj;,citation_author=Sudhansh Sharma;,citation_author=Iqbal H Sarker;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=http://www.sciencedirect.com/science/article/pii/S2666016420300244;,citation_doi=https://doi.org/10.1016/j.cscee.2020.100026;,citation_issn=2666-0164;,citation_journal_title=Case Studies in Chemical and Environmental Engineering;">
<meta name="citation_reference" content="citation_title=Machine learning for aquatic plastic litter detection, classification and quantification (APLASTIC–Q);,citation_abstract=Large quantities of mismanaged plastic waste are polluting and threatening the health of the blue planet. Vast amounts of this plastic waste found in the oceans originates from land. It finds its way to the open ocean through rivers, waterways and estuarine systems. Here we present a novel machine learning algorithm based on convolutional neural networks (CNNs) that is capable of detecting and quantifying floating and washed ashore plastic litter. The aquatic plastic litter detector and quantifier system (APLASTIC–Q) was developed and trained using very high geo–spatial resolution imagery (5 pixels/cm = 0.002 m/pixel) captured from aerial surveys in Cambodia. APLASTIC–Q comprises two machine learning algorithms components (i) plastic litter detector (PLD–CNN) and (ii) plastic litter quantifier (PLQ–CNN). PLD–CNN managed to categorize targets as water, sand, vegetation and plastic litter with an 83 also provided a qualitative count of litter as low or high based on a thresholding approach. PLQ–CNN further distinguished and enumerated the litter items in each of the classes define as water bottles, Styrofoam, canisters, cartons, bowls, shoes, polystyrene packaging, cups, textile, carry bags small or large. The types and amounts of plastic litter provide benchmark information that is urgently needed for decision making by policymakers, citizens and stakeholders especially for developing plastic policies. Quasi–quantification was based on automated counts of items present in the imagery with caveats of underlying object in case of aggregated litter. Our scientific evidence–based algorithm based on machine learning complement net trawl surveys, field campaigns and clean–up activities for improved quantification of plastic litter. APLASTIC–Q will be an open–source smart algorithm that is easy to adapt for fast and automated detection as well as quantification of floating or washed ashore plastic litter from aerial, high–altitude pseudo satellites and space missions.;,citation_author=Mattis Wolf;,citation_author=Katelijn Berg;,citation_author=Shungudzemwoyo Pascal Garaba;,citation_author=Nina Gnann;,citation_author=Klaus Sattler;,citation_author=Frederic Theodor Stahl;,citation_author=Oliver Zielinski;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.1088/1748-9326/abbd01;,citation_journal_title=Environmental Research Letters;">
<meta name="citation_reference" content="citation_title=Counting everyday objects in everyday scenes;,citation_abstract=We are interested in counting the number of instances of object classes in natural, everyday images. Previous counting approaches tackle the problem in restricted domains such as counting pedestrians in surveillance videos. Counts can also be estimated from outputs of other vision tasks like object detection. In this work, we build dedicated models for counting designed to tackle the large variance in counts, appearances, and scales of objects found in natural scenes. Our approach is inspired by the phenomenon of subitizing -the ability of humans to make quick assessments of counts given a perceptual signal, for small count values. Given a natural scene, we employ a divide and conquer strategy while incorporating context across the scene to adapt the subitizing idea to counting. Our approach offers consistent improvements over numerous baseline approaches for counting on the PASCAL VOC 2007 and COCO datasets. Subsequently, we study how counting can be used to improve object detection. We then show a proof of concept application of our counting methods to the task of Visual Question Answering, by studying the ’how many?’ questions in the VQA and COCO-QA datasets.;,citation_author=Prithvijit Chattopadhyay;,citation_author=Ramakrishna Vedantam;,citation_author=Ramprasaath R Selvaraju;,citation_author=Dhruv Batra;,citation_author=Devi Parikh;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1604.03505;,citation_doi=10.1109/CVPR.2017.471;,citation_isbn=9781538604571;,citation_volume=2017-Janua;,citation_conference_title=Proceedings - 30th IEEE conference on computer vision and pattern recognition, CVPR 2017;">
<meta name="citation_reference" content="citation_title=CornerNet: Detecting objects as paired keypoints;,citation_author=Hei Law;,citation_author=Jia Deng;,citation_editor=Vittorio Ferrari;,citation_editor=Martial Hebert;,citation_editor=Cristian Sminchisescu;,citation_editor=Yair Weiss;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.1007/978-3-030-01264-9\_45;,citation_doi=10.1007/978-3-030-01264-9\_45;,citation_volume=11218;,citation_conference_title=Computer vision - ECCV 2018 - 15th european conference, munich, germany, september 8-14, 2018, proceedings, part XIV;,citation_conference=Springer;,citation_series_title=Lecture notes in computer science;">
<meta name="citation_reference" content="citation_title=The Danube so colourful: A potpourri of plastic litter outnumbers fish larvae in Europe’s second largest river;,citation_author=Aaron Lechner;,citation_author=Hubert Keckeis;,citation_author=Franz Lumesberger-Loisl;,citation_author=Bernhard Zens;,citation_author=Reinhard Krusch;,citation_author=Michael Tritthart;,citation_author=Martin Glas;,citation_author=Elisabeth Schludermann;,citation_publication_date=2014-05;,citation_cover_date=2014-05;,citation_year=2014;,citation_fulltext_html_url=https://linkinghub.elsevier.com/retrieve/pii/S0269749114000475;,citation_doi=10.1016/j.envpol.2014.02.006;,citation_issn=02697491;,citation_volume=188;,citation_journal_title=Environmental Pollution;">
<meta name="citation_reference" content="citation_title=Assessment of floating plastic debris in surface water along the seine river;,citation_abstract=This study is intended to examine the quality and quantity of floating plastic debris in the River Seine through use of an extensive regional network of floating debris-retention booms; it is one of the first attempts to provide reliable information on such debris at a large regional scale. Plastic debris represented between 0.8% and 5.1% of total debris collected by weight. A significant proportion consisted of food wrappers/containers and plastic cutlery, probably originating from voluntary or involuntary dumping, urban discharges and surface runoff. Most plastic items are made of polypropylene, polyethylene and, to a lesser extent, polyethylene terephthalate. By extrapolation, some 27 tons of floating plastic debris are intercepted annually by this network; corresponding to 2.3 g per Parisian inhabitant per year. Such data could serve to provide a first evaluation of floating plastic inputs conveyed by rivers.;,citation_author=Johnny Gasperi;,citation_author=Rachid Dris;,citation_author=Tiffany Bonin;,citation_author=Vincent Rocher;,citation_author=Bruno Tassin;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=http://www.sciencedirect.com/science/article/pii/S0269749114003807;,citation_doi=https://doi.org/10.1016/j.envpol.2014.09.001;,citation_issn=0269-7491;,citation_volume=195;,citation_journal_title=Environmental Pollution;">
<meta name="citation_reference" content="citation_title=Plastic in the thames: A river runs through it;,citation_abstract=Although contamination of the marine ecosystems by plastics is becoming recognised as a serious pollution problem, there are few studies that demonstrate the contribution made by freshwater catchments. Over a three month period from September to December 2012, at seven localities in the upper Thames estuary, 8490 submerged plastic items were intercepted in eel fyke nets anchored to the river bed. Whilst there were significant differences in the numbers of items at these locations, the majority were some type of plastic. Additionally in excess of 20;,citation_author=David Morritt;,citation_author=Paris V. Stefanoudis;,citation_author=Dave Pearce;,citation_author=Oliver A. Crimmen;,citation_author=Paul F. Clark;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S0025326X13006565;,citation_issue=1;,citation_doi=https://doi.org/10.1016/j.marpolbul.2013.10.035;,citation_issn=0025-326X;,citation_volume=78;,citation_journal_title=Marine Pollution Bulletin;">
<meta name="citation_reference" content="citation_title=Automated River Plastic Monitoring Using Deep Learning and Cameras;,citation_abstract=Abstract Quantifying plastic pollution on surface water is essential to understand and mitigate the impact of plastic pollution to the environment. Current monitoring methods such as visual counting are labor intensive. This limits the feasibility of scaling to long-term monitoring at multiple locations. We present an automated method for monitoring plastic pollution that overcomes this limitation. Floating macroplastics are detected from images of the water surface using deep learning. We perform an experimental evaluation of our method using images from bridge-mounted cameras at five different river locations across Jakarta, Indonesia. The four main results of the experimental evaluation are as follows. First, we realize a method that obtains a reliable estimate of plastic density (68.7% precision). Our monitoring method successfully distinguishes plastics from environmental elements, such as water surface reflection and organic waste. Second, when trained on one location, the method generalizes well to new locations with relatively similar conditions without retraining (≈50% average precision). Third, generalization to new locations with considerably different conditions can be boosted by retraining on only 50 objects of the new location (improving precision from ≈20% to ≈42%). Fourth, our method matches visual counting methods and detects ≈35% more plastics, even more so during periods of plastic transport rates of above 10 items per meter per minute. Taken together, these results demonstrate that our method is a promising way of monitoring plastic pollution. By extending the variety of the data set the monitoring method can be readily applied at a larger scale.;,citation_author=Colin Lieshout;,citation_author=Kees Oeveren;,citation_author=Tim Emmerik;,citation_author=Eric Postma;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=8;,citation_doi=10.1029/2019EA000960;,citation_issn=23335084;,citation_volume=7;,citation_journal_title=Earth and Space Science;">
<meta name="citation_reference" content="citation_title=CNN-based Density Estimation and Crowd Counting: A Survey;,citation_abstract=Accurately estimating the number of objects in a single image is a challenging yet meaningful task and has been applied in many applications such as urban planning and public safety. In the various object counting tasks, crowd counting is particularly prominent due to its specific significance to social security and development. Fortunately, the development of the techniques for crowd counting can be generalized to other related fields such as vehicle counting and environment survey, if without taking their characteristics into account. Therefore, many researchers are devoting to crowd counting, and many excellent works of literature and works have spurted out. In these works, they are must be helpful for the development of crowd counting. However, the question we should consider is why they are effective for this task. Limited by the cost of time and energy, we cannot analyze all the algorithms. In this paper, we have surveyed over 220 works to comprehensively and systematically study the crowd counting models, mainly CNN-based density map estimation methods. Finally, according to the evaluation metrics, we select the top three performers on their crowd counting datasets and analyze their merits and drawbacks. Through our analysis, we expect to make reasonable inference and prediction for the future development of crowd counting, and meanwhile, it can also provide feasible solutions for the problem of object counting in other fields. We provide the density maps and prediction results of some mainstream algorithm in the validation set of NWPU dataset for comparison and testing. Meanwhile, density map generation and evaluation tools are also provided. All the codes and evaluation results are made publicly available at https://github.com/gaoguangshuai/survey-for-crowd-counting.;,citation_author=Guangshuai Gao;,citation_author=Junyu Gao;,citation_author=Qingjie Liu;,citation_author=Qi Wang;,citation_author=Yunhong Wang;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_fulltext_html_url=https://github.com/gaoguangshuai/;">
<meta name="citation_reference" content="citation_title=Counting in the wild;,citation_author=Carlos Arteta;,citation_author=Victor Lempitsky;,citation_author=Andrew Zisserman;,citation_publication_date=2016-10;,citation_cover_date=2016-10;,citation_year=2016;,citation_doi=10.1007/978-3-319-46478-7_30;,citation_isbn=978-3-319-46477-0;,citation_volume=9911;">
<meta name="citation_reference" content="citation_title=Fast video crowd counting with a temporal aware network;,citation_abstract=Crowd counting aims to count the number of instantaneous people in a crowded space, and many promising solutions have been proposed for single image crowd counting. With the ubiquitous video capture devices in public safety field, how to effectively apply the crowd counting technique to video content has become an urgent problem. In this paper, we introduce a novel framework based on temporal aware modeling of the relationship between video frames. The proposed network contains a few dilated residual blocks, and each of them consists of the layers that compute the temporal convolutions of features from the adjacent frames to improve the prediction. To alleviate the expensive computation and satisfy the demand of fast video crowd counting, we also introduce a lightweight network to balance the computational cost with representation ability. We conduct experiments on the crowd counting benchmarks and demonstrate its superiority in terms of effectiveness and efficiency over previous video-based approaches.;,citation_author=Xingjiao Wu;,citation_author=Baohan Xu;,citation_author=Yingbin Zheng;,citation_author=Hao Ye;,citation_author=Jing Yang;,citation_author=Liang He;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S0925231220306561;,citation_doi=https://doi.org/10.1016/j.neucom.2020.04.071;,citation_issn=0925-2312;,citation_volume=403;,citation_journal_title=Neurocomputing;">
<meta name="citation_reference" content="citation_title=Faster r-CNN: Towards real-time object detection with region proposal networks;,citation_author=Shaoqing Ren;,citation_author=Kaiming He;,citation_author=Ross Girshick;,citation_author=Jian Sun;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=Proceedings of the 28th international conference on neural information processing systems - volume 1;,citation_conference=MIT Press;,citation_series_title=NIPS’15;">
<meta name="citation_reference" content="citation_title=TACO: Trash Annotations in Context for Litter Detection;,citation_abstract=TACO is an open image dataset for litter detection and segmentation, which is growing through crowdsourcing. Firstly, this paper describes this dataset and the tools developed to support it. Secondly, we report instance segmentation performance using Mask R-CNN on the current version of TACO. Despite its small size (1500 images and 4784 annotations), our results are promising on this challenging problem. However, to achieve satisfactory trash detection in the wild for deployment, TACO still needs much more manual annotations. These can be contributed using: http://tacodataset.org/;,citation_author=Pedro F Proença;,citation_author=Pedro Simões;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=http://tacodataset.org/ http://arxiv.org/abs/2003.06975;">
<meta name="citation_reference" content="citation_title=Tracking without bells and whistles;,citation_author=P. Bergmann;,citation_author=T. Meinhardt;,citation_author=L. Leal-Taixe;,citation_publication_date=2019-11;,citation_cover_date=2019-11;,citation_year=2019;,citation_fulltext_html_url=https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00103;,citation_doi=10.1109/ICCV.2019.00103;,citation_conference_title=2019 IEEE/CVF international conference on computer vision (ICCV);,citation_conference=IEEE Computer Society;">
<meta name="citation_reference" content="citation_title=Hota: A higher order metric for evaluating multi-object tracking;,citation_author=Jonathon Luiten;,citation_author=Aljosa Osep;,citation_author=Patrick Dendorfer;,citation_author=Philip Torr;,citation_author=Andreas Geiger;,citation_author=Laura Leal-Taixé;,citation_author=Bastian Leibe;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_volume=129;,citation_journal_title=International journal of computer vision;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Robotic detection of marine litter using deep visual detection models;,citation_author=Michael Fulton;,citation_author=Jungseok Hong;,citation_author=Md Jahidul Islam;,citation_author=Junaed Sattar;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=2019 international conference on robotics and automation (ICRA);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=A Generative Approach Towards Improved Robotic Detection of Marine Litter;,citation_abstract=This paper presents an approach to address data scarcity problems in underwater image datasets for visual detection of marine debris. The proposed approach relies on a two-stage variational autoencoder (VAE) and a binary classifier to evaluate the generated imagery for quality and realism. From the images generated by the two-stage VAE, the binary classifier selects good quality images and augments the given dataset with them. Lastly, a multi-class classifier is used to evaluate the impact of the augmentation process by measuring the accuracy of an object detector trained on combinations of real and generated trash images. Our results show that the classifier trained with the augmented data outperforms the one trained only with the real data. This approach will not only be valid for the underwater trash classification problem presented in this paper, but it will also be useful for any data-dependent task for which collecting more images is challenging or infeasible.;,citation_author=Jungseok Hong;,citation_author=Michael Fulton;,citation_author=Junaed Sattar;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/1910.04754;,citation_doi=10.1109/ICRA40945.2020.9197575;,citation_isbn=9781728173955;,citation_issn=10504729;,citation_conference_title=Proceedings - IEEE international conference on robotics and automation;">
<meta name="citation_reference" content="citation_title=Spatiotemporal modeling for crowd counting in videos;,citation_author=Feng Xiong;,citation_author=Xingjian Shi;,citation_author=Dit-Yan Yeung;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_doi=10.1109/ICCV.2017.551;,citation_conference_title=2017 IEEE international conference on computer vision (ICCV);">
<meta name="citation_reference" content="citation_title=ST-CNN: Spatial-Temporal Convolutional Neural Network for crowd counting in videos;,citation_abstract=The task of crowd counting and density maps estimating from videos is challenging due to severe occlusions, scene perspective distortions and diverse crowd distributions. Conventional crowd counting methods via deep learning technique process each video frame independently with no consideration of the intrinsic temporal correlation among neighboring frames, thus making the performance lower than the required level of real-world applications. To overcome this shortcoming, a new end-to-end deep architecture named Spatial-Temporal Convolutional Neural Network (ST-CNN) is proposed, which unifies 2D convolutional neural network (C2D) and 3D convolutional neural network (C3D) to learn spatial-temporal features in the same framework. On top of that, a merging scheme is performed on the resulting density maps, taking advantages of the spatial-temporal information simultaneously for the crowd counting task. Experimental results on two benchmark data sets â Mall dataset and WorldExpo′10 dataset show that our ST-CNN outperforms the state-of-the-art models in terms of mean absolutely error (MAE) and mean squared error (MSE).;,citation_author=Yunqi Miao;,citation_author=Jungong Han;,citation_author=Yongsheng Gao;,citation_author=Baochang Zhang;,citation_publication_date=2019-07;,citation_cover_date=2019-07;,citation_year=2019;,citation_doi=10.1016/j.patrec.2019.04.012;,citation_issn=01678655;,citation_volume=125;,citation_journal_title=Pattern Recognition Letters;,citation_publisher=Elsevier B.V.;">
<meta name="citation_reference" content="citation_title=Multiple object tracking: A literature review;,citation_abstract=Multiple Object Tracking (MOT) has gained increasing attention due to its academic and commercial potential. Although different approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in an MOT system, including formulation, categorization, key principles, evaluation of MOT are discussed; 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks; 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative and comprehensive comparisons. By analyzing the results from different perspectives, we have verified some basic agreements in the field; and 4) We provide a discussion about issues of MOT research, as well as some interesting directions which will become potential research effort in the future.;,citation_author=Wenhan Luo;,citation_author=Junliang Xing;,citation_author=Anton Milan;,citation_author=Xiaoqin Zhang;,citation_author=Wei Liu;,citation_author=Tae-Kyun Kim;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S0004370220301958;,citation_doi=https://doi.org/10.1016/j.artint.2020.103448;,citation_issn=0004-3702;,citation_volume=293;,citation_journal_title=Artificial Intelligence;">
<meta name="citation_reference" content="citation_title=Simple online and realtime tracking;,citation_abstract=This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9;,citation_author=Alex Bewley;,citation_author=Zongyuan Ge;,citation_author=Lionel Ott;,citation_author=Fabio Ramos;,citation_author=Ben Upcroft;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_fulltext_html_url=https://github.com/abewley/sort;,citation_doi=10.1109/ICIP.2016.7533003;,citation_isbn=9781467399616;,citation_issn=15224880;,citation_volume=2016-Augus;,citation_conference_title=Proceedings - international conference on image processing, ICIP;">
<meta name="citation_reference" content="citation_title=Nuscenes: A multimodal dataset for autonomous driving;,citation_abstract=Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online1;,citation_author=Holger Caesar;,citation_author=Varun Bankiti;,citation_author=Alex H Lang;,citation_author=Sourabh Vora;,citation_author=Venice Erin Liong;,citation_author=Qiang Xu;,citation_author=Anush Krishnan;,citation_author=Yu Pan;,citation_author=Giancarlo Baldan;,citation_author=Oscar Beijbom;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/1903.11027;,citation_doi=10.1109/CVPR42600.2020.01164;,citation_issn=10636919;,citation_conference_title=Proceedings of the IEEE computer society conference on computer vision and pattern recognition;">
<meta name="citation_reference" content="citation_title=MOT20: A benchmark for multi object tracking in crowded scenes;,citation_author=Patrick Dendorfer;,citation_author=Hamid Rezatofighi;,citation_author=Anton Milan;,citation_author=Javen Shi;,citation_author=Daniel Cremers;,citation_author=Ian Reid;,citation_author=Stefan Roth;,citation_author=Konrad Schindler;,citation_author=Laura Leal-Taixé;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/2003.09003;">
<meta name="citation_reference" content="citation_title=Simple online and realtime tracking with a deep association metric;,citation_abstract=Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a largescale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45;,citation_author=Nicolai Wojke;,citation_author=Alex Bewley;,citation_author=Dietrich Paulus;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://arxiv.org/abs/1703.07402;,citation_doi=10.1109/ICIP.2017.8296962;,citation_isbn=9781509021758;,citation_issn=15224880;,citation_volume=2017-Septe;,citation_conference_title=Proceedings - international conference on image processing, ICIP;">
<meta name="citation_reference" content="citation_title=TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking;,citation_abstract=Tracking multiple objects in videos relies on modeling the spatial-temporal interactions of the objects. In this paper, we propose a solution named TransMOT, which leverages powerful graph transformers to efficiently model the spatial and temporal interactions among the objects. TransMOT effectively models the interactions of a large number of objects by arranging the trajectories of the tracked objects as a set of sparse weighted graphs, and constructing a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial graph transformer decoder layer based on the graphs. TransMOT is not only more computationally efficient than the traditional Transformer, but it also achieves better tracking accuracy. To further improve the tracking speed and accuracy, we propose a cascade association framework to handle low-score detections and long-term occlusions that require large computational resources to model in TransMOT. The proposed method is evaluated on multiple benchmark datasets including MOT15, MOT16, MOT17, and MOT20, and it achieves state-of-the-art performance on all the datasets.;,citation_author=Peng Chu;,citation_author=Jiang Wang;,citation_author=Quanzeng You;,citation_author=Haibin Ling;,citation_author=Zicheng Liu;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=http://arxiv.org/abs/2104.00194;">
<meta name="citation_reference" content="citation_title=Joint object detection and multi-object tracking with graph neural networks;,citation_author=Yongxin Wang;,citation_author=Kris Kitani;,citation_author=Xinshuo Weng;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Evaluating multiple object tracking performance: The CLEAR MOT metrics;,citation_author=Keni Bernardin;,citation_author=Rainer Stiefelhagen;,citation_publication_date=2008-01;,citation_cover_date=2008-01;,citation_year=2008;,citation_doi=10.1155/2008/246309;,citation_volume=2008;,citation_journal_title=EURASIP Journal on Image and Video Processing;">
<meta name="citation_reference" content="citation_title=Performance measures and a data set for multi-target, multi-camera tracking;,citation_author=Ergys Ristani;,citation_author=Francesco Solera;,citation_author=Roger Zou;,citation_author=Rita Cucchiara;,citation_author=Carlo Tomasi;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_conference_title=European conference on computer vision;,citation_conference=Springer;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; Macrolitter video counting on riverbanks using state space models and moving cameras</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliations</div>
          
          <div class="quarto-title-meta-contents">
        <a href="https://www.linkedin.com/in/mathis-chagneux-140245158/?originalSubdomain=fr">Mathis Chagneux</a> 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.telecom-paris.fr/fr/recherche/laboratoires/laboratoire-traitement-et-communication-de-linformation-ltci">
                  Telecom Paris, LTCI
                  </a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://sylvainlc.github.io/">Sylvain Le Corff</a> <a href="https://orcid.org/0000-0001-5211-2328" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.lpsm.paris/">
                  Sorbonne Université, UMR 8001 (LPSM)
                  </a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://papayoun.github.io/">Pierre Gloaguen</a> <a href="https://orcid.org/0000-0003-2239-5413" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://mia-ps.inrae.fr/">
                  AgroParisTech, UMR MIA 518
                  </a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://charlesollion.github.io/">Charles Ollion</a> <a href="https://orcid.org/0000-0002-6763-701X" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Naia Science
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://fr.linkedin.com/in/oc%C3%A9ane-lep%C3%A2tre-675b38116">Océane Lepâtre</a> <a href="https://orcid.org/None" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://surfrider.eu/">
                  Surfrider Foundation Europe
                  </a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://www.linkedin.com/in/antoinebruge/">Antoine Bruge</a> <a href="https://orcid.org/0000-0002-0548-234X" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://surfrider.eu/">
                  Surfrider Foundation Europe
                  </a>
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 16, 2023</p>
      </div>
    </div>
                                    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">February 25, 2024</p>
      </div>
    </div>
      
                  
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <a href="https://github.com/computorg/published-202301-chagneux-macrolitter"><img src="https://github.com/computorg/published-202301-chagneux-macrolitter/actions/workflows/build.yml/badge.svg" alt="build status"></a>
                    <p class="date"></p>
        <a href="https://github.com/computorg/published-202301-chagneux-macrolitter/issues?q=is%3Aopen+is%3Aissue+label%3Areview"><img src="https://img.shields.io/badge/reviews-reports-blue" alt="reviews"></a>
            </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>Litter is a known cause of degradation in marine environments and most of it travels in rivers before reaching the oceans. In this paper, we present a novel algorithm to assist waste monitoring along watercourses. While several attempts have been made to quantify litter using neural object detection in photographs of floating items, we tackle the more challenging task of counting directly in videos using boat-embedded cameras. We rely on multi-object tracking (MOT) but focus on the key pitfalls of false and redundant counts which arise in typical scenarios of poor detection performance. Our system only requires supervision at the image level and performs Bayesian filtering via a state space model based on optical flow. We present a new open image dataset gathered through a crowdsourced campaign and used to train a center-based anchor-free object detector. Realistic video footage assembled by water monitoring experts is annotated and provided for evaluation. Improvements in count quality are demonstrated against systems built from state-of-the-art multi-object trackers sharing the same detection capabilities. A precise error decomposition allows clear analysis and highlights the remaining challenges.</p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#related-works" id="toc-related-works" class="nav-link" data-scroll-target="#related-works"><span class="header-section-number">2</span> Related works</a>
  <ul class="collapse">
  <li><a href="#ai-automated-counting" id="toc-ai-automated-counting" class="nav-link" data-scroll-target="#ai-automated-counting"><span class="header-section-number">2.1</span> AI-automated counting</a></li>
  <li><a href="#computer-vision-for-macro-litter-monitoring" id="toc-computer-vision-for-macro-litter-monitoring" class="nav-link" data-scroll-target="#computer-vision-for-macro-litter-monitoring"><span class="header-section-number">2.2</span> Computer vision for macro litter monitoring</a></li>
  <li><a href="#multi-object-tracking" id="toc-multi-object-tracking" class="nav-link" data-scroll-target="#multi-object-tracking"><span class="header-section-number">2.3</span> Multi-object tracking</a></li>
  </ul></li>
  <li><a href="#datasets-for-training-and-evaluation" id="toc-datasets-for-training-and-evaluation" class="nav-link" data-scroll-target="#datasets-for-training-and-evaluation"><span class="header-section-number">3</span> Datasets for training and evaluation</a>
  <ul class="collapse">
  <li><a href="#images" id="toc-images" class="nav-link" data-scroll-target="#images"><span class="header-section-number">3.1</span> Images</a>
  <ul class="collapse">
  <li><a href="#data-collection" id="toc-data-collection" class="nav-link" data-scroll-target="#data-collection"><span class="header-section-number">3.1.1</span> Data collection</a></li>
  <li><a href="#bounding-box-annotation" id="toc-bounding-box-annotation" class="nav-link" data-scroll-target="#bounding-box-annotation"><span class="header-section-number">3.1.2</span> Bounding box annotation</a></li>
  </ul></li>
  <li><a href="#video-sequences" id="toc-video-sequences" class="nav-link" data-scroll-target="#video-sequences"><span class="header-section-number">3.2</span> Video sequences</a>
  <ul class="collapse">
  <li><a href="#data-collection-1" id="toc-data-collection-1" class="nav-link" data-scroll-target="#data-collection-1"><span class="header-section-number">3.2.1</span> Data collection</a></li>
  <li><a href="#track-annotation" id="toc-track-annotation" class="nav-link" data-scroll-target="#track-annotation"><span class="header-section-number">3.2.2</span> Track annotation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions" id="toc-optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions" class="nav-link" data-scroll-target="#optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions"><span class="header-section-number">4</span> Optical flow-based counting via Bayesian filtering and confidence regions</a>
  <ul class="collapse">
  <li><a href="#detector" id="toc-detector" class="nav-link" data-scroll-target="#detector"><span class="header-section-number">4.1</span> Detector</a>
  <ul class="collapse">
  <li><a href="#center-based-anchor-free-detection" id="toc-center-based-anchor-free-detection" class="nav-link" data-scroll-target="#center-based-anchor-free-detection"><span class="header-section-number">4.1.1</span> Center-based anchor-free detection</a></li>
  </ul></li>
  <li><a href="#sec-detector_training" id="toc-sec-detector_training" class="nav-link" data-scroll-target="#sec-detector_training"><span class="header-section-number">4.2</span> Training</a></li>
  <li><a href="#sec-bayesian_tracking" id="toc-sec-bayesian_tracking" class="nav-link" data-scroll-target="#sec-bayesian_tracking"><span class="header-section-number">4.3</span> Bayesian tracking with optical flow</a>
  <ul class="collapse">
  <li><a href="#optical-flow" id="toc-optical-flow" class="nav-link" data-scroll-target="#optical-flow"><span class="header-section-number">4.3.1</span> Optical flow</a></li>
  <li><a href="#sec-state_space_model" id="toc-sec-state_space_model" class="nav-link" data-scroll-target="#sec-state_space_model"><span class="header-section-number">4.3.2</span> State space model</a></li>
  <li><a href="#approximations-of-the-filtering-distributions" id="toc-approximations-of-the-filtering-distributions" class="nav-link" data-scroll-target="#approximations-of-the-filtering-distributions"><span class="header-section-number">4.3.3</span> Approximations of the filtering distributions</a></li>
  <li><a href="#generating-potential-object-tracks" id="toc-generating-potential-object-tracks" class="nav-link" data-scroll-target="#generating-potential-object-tracks"><span class="header-section-number">4.3.4</span> Generating potential object tracks</a></li>
  </ul></li>
  <li><a href="#sec-data_association" id="toc-sec-data_association" class="nav-link" data-scroll-target="#sec-data_association"><span class="header-section-number">4.4</span> Data association using confidence regions</a></li>
  <li><a href="#counting" id="toc-counting" class="nav-link" data-scroll-target="#counting"><span class="header-section-number">4.5</span> Counting</a></li>
  </ul></li>
  <li><a href="#metrics-for-mot-based-counting" id="toc-metrics-for-mot-based-counting" class="nav-link" data-scroll-target="#metrics-for-mot-based-counting"><span class="header-section-number">5</span> Metrics for MOT-based counting</a>
  <ul class="collapse">
  <li><a href="#count-related-mot-metrics" id="toc-count-related-mot-metrics" class="nav-link" data-scroll-target="#count-related-mot-metrics"><span class="header-section-number">5.1</span> Count-related MOT metrics</a>
  <ul class="collapse">
  <li><a href="#detection" id="toc-detection" class="nav-link" data-scroll-target="#detection"><span class="header-section-number">5.1.1</span> Detection</a></li>
  <li><a href="#association" id="toc-association" class="nav-link" data-scroll-target="#association"><span class="header-section-number">5.1.2</span> Association</a></li>
  </ul></li>
  <li><a href="#count-metrics" id="toc-count-metrics" class="nav-link" data-scroll-target="#count-metrics"><span class="header-section-number">5.2</span> Count metrics</a>
  <ul class="collapse">
  <li><a href="#count-decomposition" id="toc-count-decomposition" class="nav-link" data-scroll-target="#count-decomposition"><span class="header-section-number">5.2.1</span> Count decomposition</a></li>
  <li><a href="#statistics" id="toc-statistics" class="nav-link" data-scroll-target="#statistics"><span class="header-section-number">5.2.2</span> Statistics</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments"><span class="header-section-number">6</span> Experiments</a>
  <ul class="collapse">
  <li><a href="#detection-1" id="toc-detection-1" class="nav-link" data-scroll-target="#detection-1"><span class="header-section-number">6.1</span> Detection</a></li>
  <li><a href="#counts" id="toc-counts" class="nav-link" data-scroll-target="#counts"><span class="header-section-number">6.2</span> Counts</a></li>
  </ul></li>
  <li><a href="#practical-impact-and-future-goals" id="toc-practical-impact-and-future-goals" class="nav-link" data-scroll-target="#practical-impact-and-future-goals"><span class="header-section-number">7</span> Practical impact and future goals</a></li>
  <li><a href="#supplements" id="toc-supplements" class="nav-link" data-scroll-target="#supplements"><span class="header-section-number">8</span> Supplements</a>
  <ul class="collapse">
  <li><a href="#sec-image_dataset_appendix" id="toc-sec-image_dataset_appendix" class="nav-link" data-scroll-target="#sec-image_dataset_appendix"><span class="header-section-number">8.1</span> Details on the image dataset</a>
  <ul class="collapse">
  <li><a href="#categories" id="toc-categories" class="nav-link" data-scroll-target="#categories"><span class="header-section-number">8.1.1</span> Categories</a></li>
  </ul></li>
  <li><a href="#sec-video-dataset-appendix" id="toc-sec-video-dataset-appendix" class="nav-link" data-scroll-target="#sec-video-dataset-appendix"><span class="header-section-number">8.2</span> Details on the evaluation videos</a>
  <ul class="collapse">
  <li><a href="#river-segments" id="toc-river-segments" class="nav-link" data-scroll-target="#river-segments"><span class="header-section-number">8.2.1</span> River segments</a></li>
  </ul></li>
  <li><a href="#sec-tracking_module_appendix" id="toc-sec-tracking_module_appendix" class="nav-link" data-scroll-target="#sec-tracking_module_appendix"><span class="header-section-number">8.3</span> Implementation details for the tracking module</a>
  <ul class="collapse">
  <li><a href="#sec-covariance_matrices" id="toc-sec-covariance_matrices" class="nav-link" data-scroll-target="#sec-covariance_matrices"><span class="header-section-number">8.3.1</span> Covariance matrices for state and observation noises</a></li>
  <li><a href="#sec-tau_kappa_appendix" id="toc-sec-tau_kappa_appendix" class="nav-link" data-scroll-target="#sec-tau_kappa_appendix"><span class="header-section-number">8.3.2</span> Influence of <span class="math inline">\tau</span> and <span class="math inline">\kappa</span></a></li>
  </ul></li>
  <li><a href="#sec-bayesian_filtering" id="toc-sec-bayesian_filtering" class="nav-link" data-scroll-target="#sec-bayesian_filtering"><span class="header-section-number">8.4</span> Bayesian filtering</a></li>
  <li><a href="#sec-confidence_regions_appendix" id="toc-sec-confidence_regions_appendix" class="nav-link" data-scroll-target="#sec-confidence_regions_appendix"><span class="header-section-number">8.5</span> Computing the confidence regions</a>
  <ul class="collapse">
  <li><a href="#sec-impact-algorithm-appendix" id="toc-sec-impact-algorithm-appendix" class="nav-link" data-scroll-target="#sec-impact-algorithm-appendix"><span class="header-section-number">8.5.1</span> Impact of the filtering algorithm</a></li>
  <li><a href="#smc-based-tracking" id="toc-smc-based-tracking" class="nav-link" data-scroll-target="#smc-based-tracking"><span class="header-section-number">8.5.2</span> SMC-based tracking</a></li>
  <li><a href="#performance-comparison" id="toc-performance-comparison" class="nav-link" data-scroll-target="#performance-comparison"><span class="header-section-number">8.5.3</span> Performance comparison</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="published-202301-chagneux-macrolitter.pdf"><i class="bi bi-file-pdf"></i>PDF (computo)</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Litter pollution concerns every part of the globe. Each year, almost ten thousand million tons of plastic waste is generated, among which 80% ends up in landfills or in nature (<span class="citation" data-cites="geyer2017">Geyer, Jambeck, and Law (<a href="#ref-geyer2017" role="doc-biblioref">2017</a>)</span>), notably threatening all of the world’s oceans, seas and aquatic environments (<span class="citation" data-cites="welden2020">Welden (<a href="#ref-welden2020" role="doc-biblioref">2020</a>)</span>, <span class="citation" data-cites="gamage2020">Gamage and Senevirathna (<a href="#ref-gamage2020" role="doc-biblioref">2020</a>)</span>). Plastic pollution is known to already impact more than 3763 marine species worldwide (see <a href="https://litterbase.awi.de/">this</a> detailed analysis) with risk of proliferation through the whole food chain. This accumulation of waste is the endpoint of the largely misunderstood path of trash, mainly coming from land-based sources (<span class="citation" data-cites="rochman2016">Rochman et al. (<a href="#ref-rochman2016" role="doc-biblioref">2016</a>)</span>), yet rivers have been identified as a major pathway for the introduction of waste into marine environments (<span class="citation" data-cites="jambeck2015">Jambeck et al. (<a href="#ref-jambeck2015" role="doc-biblioref">2015</a>)</span>). Therefore, field data on rivers and monitoring are strongly needed to assess the impact of measures that can be taken. The analysis of such field data over time is pivotal to understand the efficiency of the actions implemented such as choosing zero-waste alternatives to plastic, designing new products to be long-lasting or reusable, introducing policies to reduce over-packing.</p>
<p>Different methods have already been tested to monitor waste in rivers: litter collection and sorting on riverbanks (<span class="citation" data-cites="Bruge2018">Bruge et al. (<a href="#ref-Bruge2018" role="doc-biblioref">2018</a>)</span>), visual counting of drifting litter from bridges (<span class="citation" data-cites="gonzales2021">González-Fernández et al. (<a href="#ref-gonzales2021" role="doc-biblioref">2021</a>)</span>), floating booms (<span class="citation" data-cites="gasperi2014">Gasperi et al. (<a href="#ref-gasperi2014" role="doc-biblioref">2014</a>)</span>), and nets (<span class="citation" data-cites="moritt2014">Morritt et al. (<a href="#ref-moritt2014" role="doc-biblioref">2014</a>)</span>). All are helpful to understand the origin and typology of litter pollution yet hardly compatible with long term monitoring at country scales. Monitoring tools need to be reliable, easy to set up on various types of rivers, and should give an overview of plastic pollution during peak discharge to help locate hotspots and provide trends. Newer studies suggest that plastic debris transport could be better understood by counting litter trapped on river banks, providing a good indication of the local macrolitter pollution especially after increased river discharge (<span class="citation" data-cites="VanEmmerik2019">Emmerik et al. (<a href="#ref-VanEmmerik2019" role="doc-biblioref">2019</a>)</span>, <span class="citation" data-cites="VanEmmerik2020">Emmerik and Schwarz (<a href="#ref-VanEmmerik2020" role="doc-biblioref">2020</a>)</span>). Based on these findings, we propose a new method for litter monitoring which relies on videos of river banks directly captured from moving boats.</p>
<p>In this case, object detection with deep neural networks (DNNs) may be used, but new challenges arise. First, available data is still scarce. When considering entire portions of river banks from many different locations, the variety of scenes, viewing angles and/or light conditions is not well covered by existing plastic litter datasets like (<span class="citation" data-cites="Proenca2020">Proença and Simões (<a href="#ref-Proenca2020" role="doc-biblioref">2020</a>)</span>), where litter is usually captured from relatively close distances and many times in urban or domestic backgrounds. Therefore, achieving robust object detection across multiple conditions is still delicate.</p>
<p>Second, counting from videos is a different task than counting from independent images, because individual objects will typically appear in several consecutive frames, yet they must only be counted once. This last problem of association has been extensively studied for the multi-object tracking (MOT) task, which aims at recovering individual trajectories for objects in videos. When successful MOT is achieved, counting objects in videos is equivalent to counting the number of estimated trajectories. Deep learning has been increasingly used to improve MOT solutions (<span class="citation" data-cites="Ciaparrone2020b">Ciaparrone et al. (<a href="#ref-Ciaparrone2020b" role="doc-biblioref">2020</a>)</span>). However, newer state-of-the-art techniques require increasingly heavy and costly supervision, typically all object positions provided at every frame. In addition, many successful techniques (<span class="citation" data-cites="bergmann2019">Bergmann, Meinhardt, and Leal-Taixe (<a href="#ref-bergmann2019" role="doc-biblioref">2019</a>)</span>) can hardly be used in scenarios with abrupt and nonlinear camera motion. Finally, while research is still active to rigorously evaluate performance at multi-object <em>tracking</em> (<span class="citation" data-cites="luiten2020">Luiten et al. (<a href="#ref-luiten2020" role="doc-biblioref">2021</a>)</span>), most but not all aspects of the latter may affect global video counts, which calls for a separate evaluation protocol dedicated to multi-object <em>counting</em>.</p>
<p>Our contribution can be summarized as follows.</p>
<ol type="1">
<li>We provide a novel open-source image dataset of macro litter, which includes various objects seen from different rivers and different contexts. This dataset was produced with a new open-sourced platform for data gathering and annotation developed in conjunction with Surfrider Foundation Europe, continuously growing with more data.</li>
<li>We propose a new algorithm specifically tailored to count in videos with fast camera movements. In a nutshell, DNN-based object detection is paired with a robust state space movement model which uses optical flow to perform Bayesian filtering, while confidence regions built on posterior predictive distributions are used for data association. This framework does not require video annotations at training time: the multi-object tracking module does not require supervision, only the DNN-based object detection does require annotated images. It also fully leverages optical flow estimates and the uncertainty provided by Bayesian predictions to recover object identities even when detection recall is low. Contrary to existing MOT solutions, this method ensures that tracks are stable enough to avoid repeated counting of the same object.</li>
<li>We provide a set of video sequences where litter counts are known and depicted in real conditions. For these videos only, litter positions are manually annotated at every frame in order to carefully analyze performance. This allows us to build new informative count metrics. We compare the count performance of our method against other MOT-based alternatives.</li>
</ol>
<p>A first visual illustration of the second claim is presented via the following code chunks: on three selected frames, we present a typical scenario where our strategy can avoid overcounting the same object (we depict internal workings of our solution against the end result of the competitors).</p>
<div id="cell-fig-demo" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.prepare_data <span class="im">import</span> download_data</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.track <span class="im">import</span> default_args <span class="im">as</span> args</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {<span class="st">'legend.fontsize'</span>: <span class="st">'xx-large'</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>         <span class="st">'axes.labelsize'</span>: <span class="st">'xx-large'</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>         <span class="st">'axes.titlesize'</span>:<span class="st">'xx-large'</span>,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>         <span class="st">'xtick.labelsize'</span>:<span class="st">'xx-large'</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>         <span class="st">'ytick.labelsize'</span>:<span class="st">'xx-large'</span>}</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.rcParams.update(params)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># download frames and detections from a given deep detector model</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>download_data()</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare arguments</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>args.external_detections <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>args.data_dir <span class="op">=</span> <span class="st">'data/external_detections/part_1_segment_0'</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>args.output_dir <span class="op">=</span> <span class="st">'surfnet/results'</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>args.noise_covariances_path <span class="op">=</span> <span class="st">'surfnet/data/tracking_parameters'</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>args.confidence_threshold <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>args.algorithm <span class="op">=</span> <span class="st">'EKF'</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>args.ratio <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>args.display <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.tracking.utils <span class="im">import</span> resize_external_detections, write_tracking_results_to_file</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.tools.video_readers <span class="im">import</span> FramesWithInfo</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.tracking.trackers <span class="im">import</span> get_tracker</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.track <span class="im">import</span> track_video</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize variances</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>transition_variance <span class="op">=</span> np.load(os.path.join(args.noise_covariances_path, <span class="st">'transition_variance.npy'</span>))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>observation_variance <span class="op">=</span> np.load(os.path.join(args.noise_covariances_path, <span class="st">'observation_variance.npy'</span>))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Get tracker algorithm</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>engine <span class="op">=</span> get_tracker(args.algorithm)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Open data: detections and frames</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(os.path.join(args.data_dir, <span class="st">'saved_detections.pickle'</span>),<span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    detections <span class="op">=</span> pickle.load(f)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(os.path.join(args.data_dir, <span class="st">'saved_frames.pickle'</span>),<span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    frames <span class="op">=</span> pickle.load(f)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Create frame reader and resize detections</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>reader <span class="op">=</span> FramesWithInfo(frames)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>detections <span class="op">=</span> resize_external_detections(detections, args.ratio)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Start tracking, storing intermediate tracklets</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>results, frame_to_trackers <span class="op">=</span> track_video(reader, detections, args, engine, </span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>                                         transition_variance, observation_variance, return_trackers<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Write final results</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>write_tracking_results_to_file(results, ratio_x<span class="op">=</span>args.ratio, ratio_y<span class="op">=</span>args.ratio, output_filename<span class="op">=</span>args.output_dir)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.track <span class="im">import</span> build_image_trackers</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose a few indices to display (same for our algorithm and SORT)</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> [<span class="dv">108</span>, <span class="dv">112</span>, <span class="dv">117</span>]</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>considered_frames <span class="op">=</span> [frames[i] <span class="cf">for</span> i <span class="kw">in</span> idxs]</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>considered_trackers <span class="op">=</span> [frame_to_trackers[i] <span class="cf">for</span> i <span class="kw">in</span> idxs]</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> build_image_trackers(considered_frames, considered_trackers, args, reader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>---Downloading: saved_detections.pickle
---Downloading: saved_frames.pickle
EKF will be used for tracking.
Tracking...
Tracking done.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-demo" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-202301-chagneux-macrolitter_files/figure-html/fig-demo-output-2.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <em>Our method</em>: one object (red dot) is correctly detected at every frame and given a consistent identity throughout the sequence with low location uncertainty (red ellipse). Next to it, a false positive detection is generated at the first frame (brown dot) but immediatly lost in the following frames: the associated uncertainty grows fast (brown ellipse). In our solution, this type of track will not be counted. A third correctly detected object (pink) appears in the third frame and begins a new track.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="d9e57eba" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Tracker with SORT </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sort.sort <span class="im">import</span> track <span class="im">as</span> sort_tracker</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Tracking with SORT...'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'--- Begin SORT internal logs'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>sort_tracker(detections_dir<span class="op">=</span><span class="st">'data/external_detections'</span>, output_dir<span class="op">=</span><span class="st">'sort/results'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'--- End'</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> read_sort_output(filename):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Reads the output .txt of Sort (or other tracking algorithm)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    dict_frames <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename) <span class="im">as</span> f:</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            items <span class="op">=</span> line[:<span class="op">-</span><span class="dv">1</span>].split(<span class="st">","</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            frame <span class="op">=</span> <span class="bu">int</span>(items[<span class="dv">0</span>])</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            objnum <span class="op">=</span> <span class="bu">int</span>(items[<span class="dv">1</span>])</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="bu">float</span>(items[<span class="dv">2</span>])</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> <span class="bu">float</span>(items[<span class="dv">3</span>])</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            dict_frames[<span class="bu">int</span>(items[<span class="dv">0</span>])].append((objnum, x, y))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>  dict_frames</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_image(frames, trackers, image_shape<span class="op">=</span>(<span class="dv">135</span>,<span class="dv">240</span>), downsampling<span class="op">=</span><span class="dv">2</span><span class="op">*</span><span class="dv">4</span>):</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Builds a full image with consecutive frames and their displayed trackers</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co">    frames: a list of K np.array</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co">    trackers: a list of K trackers. Each tracker is a per frame list of tracked objects</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> <span class="bu">len</span>(frames)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(trackers) <span class="op">==</span> K</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    font <span class="op">=</span> cv2.FONT_HERSHEY_COMPLEX</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    output_img<span class="op">=</span>np.zeros((image_shape[<span class="dv">0</span>], image_shape[<span class="dv">1</span>]<span class="op">*</span>K, <span class="dv">3</span>), dtype<span class="op">=</span>np.uint8)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    object_ids <span class="op">=</span> []</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tracker <span class="kw">in</span> trackers:</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> detection <span class="kw">in</span> tracker:</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>            object_ids.append(detection[<span class="dv">0</span>])</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    min_object_id <span class="op">=</span> <span class="bu">min</span>(object_ids)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        frame <span class="op">=</span> cv2.cvtColor(cv2.resize(frames[i], image_shape[::<span class="op">-</span><span class="dv">1</span>]), cv2.COLOR_BGR2RGB)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> detection <span class="kw">in</span> trackers[i]:</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>            cv2.putText(frame, <span class="ss">f'</span><span class="sc">{</span>detection[<span class="dv">0</span>]<span class="op">-</span>min_object_id <span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, (<span class="bu">int</span>(detection[<span class="dv">1</span>]<span class="op">/</span>downsampling)<span class="op">+</span><span class="dv">10</span>, <span class="bu">int</span>(detection[<span class="dv">2</span>]<span class="op">/</span>downsampling)<span class="op">+</span><span class="dv">10</span>), font, <span class="fl">0.5</span>, (<span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dv">1</span>, cv2.LINE_AA)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        output_img[:,i<span class="op">*</span>image_shape[<span class="dv">1</span>]:(i<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>image_shape[<span class="dv">1</span>],:] <span class="op">=</span> frame</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Tracking with SORT...
--- Begin SORT internal logs
Total Tracking took: 0.101 seconds for 360 frames or 3553.8 FPS
--- End</code></pre>
</div>
</div>
<div id="cell-fig-demo-sort" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># open sort output</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>tracker_file <span class="op">=</span> <span class="st">"sort/results/part_1_segment_0.txt"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>frame_to_track <span class="op">=</span> read_sort_output(tracker_file)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>condisered_frames <span class="op">=</span> [frames[idx] <span class="cf">for</span> idx <span class="kw">in</span> idxs]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>considered_tracks <span class="op">=</span> [frame_to_track[i] <span class="cf">for</span> i <span class="kw">in</span> idxs]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>out_img <span class="op">=</span> build_image(condisered_frames, considered_tracks)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">6</span>))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>plt.imshow(out_img)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)<span class="op">;</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-demo-sort" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-sort-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-202301-chagneux-macrolitter_files/figure-html/fig-demo-sort-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-sort-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <em>SORT</em>: the resulting count is also 2, but both counts arise from tracks generated by the same object, the latter not re-associated at all in the second frame. Additionally, the third object is discarded (in post-processing) by their strategy.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="related-works" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Related works</h1>
<section id="ai-automated-counting" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="ai-automated-counting"><span class="header-section-number">2.1</span> AI-automated counting</h2>
<p>Counting from images has been an ongoing challenge in computer vision. Most works can be divided into (i) detection-based methods where objects are individually located for counting, (ii) density-based methods where counts are obtained by summing a predicted density map, and (iii) regression-based methods where counts are directly regressed from input images (<span class="citation" data-cites="Chattopadhyay">Chattopadhyay et al. (<a href="#ref-Chattopadhyay" role="doc-biblioref">2017</a>)</span>). While some of these works tackled the problem of counting in wild scenes (<span class="citation" data-cites="Arteta2016">Arteta, Lempitsky, and Zisserman (<a href="#ref-Arteta2016" role="doc-biblioref">2016</a>)</span>), most are focused on pedestrian and crowd counting. Though several works (<span class="citation" data-cites="wu2020fast">Wu et al. (<a href="#ref-wu2020fast" role="doc-biblioref">2020</a>)</span>, <span class="citation" data-cites="Xiong2017">Xiong, Shi, and Yeung (<a href="#ref-Xiong2017" role="doc-biblioref">2017</a>)</span>, <span class="citation" data-cites="Miao2019">Miao et al. (<a href="#ref-Miao2019" role="doc-biblioref">2019</a>)</span>) showed the relevance of leveraging sequential inter-frame information to achieve better counts at every frame, none of these methods actually attempt to produce global video counts.</p>
</section>
<section id="computer-vision-for-macro-litter-monitoring" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="computer-vision-for-macro-litter-monitoring"><span class="header-section-number">2.2</span> Computer vision for macro litter monitoring</h2>
<p>Automatic macro litter monitoring in rivers is still a relatively nascent initiative, yet there have already been several attempts at using DNN-based object recognition tools to count plastic trash. Recently, (<span class="citation" data-cites="Proenca2020">Proença and Simões (<a href="#ref-Proenca2020" role="doc-biblioref">2020</a>)</span>) used a combination of two Convolutional Neural Networks (CNNs) to detect and quantify plastic litter using geospatial images from Cambodia. In (<span class="citation" data-cites="Wolf2020">Wolf et al. (<a href="#ref-Wolf2020" role="doc-biblioref">2020</a>)</span>), reliable estimates of plastic density were obtained using Faster R-CNN (<span class="citation" data-cites="ren2016faster">Ren et al. (<a href="#ref-ren2016faster" role="doc-biblioref">2015</a>)</span>) on images extracted from bridge-mounted cameras. For underwater waste monitoring, (<span class="citation" data-cites="vanlieshout2020automated">Lieshout et al. (<a href="#ref-vanlieshout2020automated" role="doc-biblioref">2020</a>)</span>) assembled a dataset with bounding box annotations, and showed promising performance with several object detectors. They later turned to generative models to obtain more synthetic data from a small dataset @(Hong2020). While proving the practicality of deep learning for automatic waste detection in various contexts, these works only provide counts for separate images of photographed litter. To the best of our knowledge, no solution has been proposed to count litter directly in videos.</p>
</section>
<section id="multi-object-tracking" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="multi-object-tracking"><span class="header-section-number">2.3</span> Multi-object tracking</h2>
<p>Multi-object tracking usually involves object detection, data association and track management, with a very large number of methods already existing before DNNs (<span class="citation" data-cites="luo2021">Luo et al. (<a href="#ref-luo2021" role="doc-biblioref">2021</a>)</span>). MOT approaches now mostly differ in the level of supervision they require for each step: until recently, most successful methods (like <span class="citation" data-cites="Bewley2016">Bewley et al. (<a href="#ref-Bewley2016" role="doc-biblioref">2016</a>)</span> have been detection-based, i.e.&nbsp;involving only a DNN-based object detector trained at the image level and coupled with an unsupervised data association step. In specific fields such as pedestrian tracking or autonomous driving, vast datasets now provide precise object localisation and identities throughout entire videos (<span class="citation" data-cites="Caesar2020">Caesar et al. (<a href="#ref-Caesar2020" role="doc-biblioref">2020</a>)</span>) <span class="citation" data-cites="Dendorfer2020">Dendorfer et al. (<a href="#ref-Dendorfer2020" role="doc-biblioref">2020</a>)</span>). Current state-of-the-art methods leverage this supervision via deep visual feature extraction (<span class="citation" data-cites="Wojke2018">Wojke, Bewley, and Paulus (<a href="#ref-Wojke2018" role="doc-biblioref">2018</a>)</span>, <span class="citation" data-cites="Zhanga">Zhang et al. (<a href="#ref-Zhanga" role="doc-biblioref">2021</a>)</span>) or even self-attention (<span class="citation" data-cites="Chu2021">Chu et al. (<a href="#ref-Chu2021" role="doc-biblioref">2021</a>)</span>) and graph neural networks (<span class="citation" data-cites="Wang2021">Wang, Kitani, and Weng (<a href="#ref-Wang2021" role="doc-biblioref">2021</a>)</span>). For these applications, motion prediction may be required, yet well-trained appearance models are usually enough to deal with detection failures under simple motion, therefore the linear constant-velocity assumption often prevails (<span class="citation" data-cites="Ciaparrone2020b">Ciaparrone et al. (<a href="#ref-Ciaparrone2020b" role="doc-biblioref">2020</a>)</span>).</p>
<p>In the case of macrolitter monitoring, however, available image datasets are still orders of magnitude smaller, and annotated video datasets do not exist at all. Even more so, real shooting conditions induce chaotic movements on the boat-embedded cameras. A close work of ours is that of (<span class="citation" data-cites="Fulton2018">Fulton et al. (<a href="#ref-Fulton2018" role="doc-biblioref">2019</a>)</span>), who paired Kalman filtering with optical flow to yield fruit count estimates on entire video sequences captured by moving robots. However, their video footage is captured at night with consistent lighting conditions, backgrounds are largely similar across sequences, and camera movements are less challenging. In our application context, we find that using MOT for the task of counting objects requires a new movement model, to take into account missing detections and large camera movements.</p>
</section>
</section>
<section id="datasets-for-training-and-evaluation" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Datasets for training and evaluation</h1>
<p>Our main dataset of annotated images is used to train the object detector. Then, only for evaluation purposes, we provide videos with annotated object positions and known global counts. Our motivation is to avoid relying on training data that requires this resource-consuming process.</p>
<section id="images" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="images"><span class="header-section-number">3.1</span> Images</h2>
<section id="data-collection" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="data-collection"><span class="header-section-number">3.1.1</span> Data collection</h3>
<p>With help from volunteers, we compile photographs of litter stranded on river banks after increased river discharge, shot directly from kayaks navigating at varying distances from the shore. Images span multiple rivers with various levels of water current, on different seasons, mostly in southwestern France. The resulting pictures depict trash items under the same conditions as the video footage we wish to count on, while spanning a wide variety of backgrounds, light conditions, viewing angles and picture quality.</p>
</section>
<section id="bounding-box-annotation" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="bounding-box-annotation"><span class="header-section-number">3.1.2</span> Bounding box annotation</h3>
<p>For object detection applications, the images are annotated using a custom online platform where each object is located using a bounding box. In this work, we focus only on litter counting without classification, however the annotated objects are already classified into specific categories which are described in <a href="#fig-trash-categories-image" class="quarto-xref">Figure&nbsp;4</a>.</p>
<p>A few samples are depicted below:</p>
<div id="c086babb" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ExifTags</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pycocotools.coco <span class="im">import</span> COCO</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_bbox(image, anns, ratio):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Display the specified annotations.</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ann <span class="kw">in</span> anns:</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        [bbox_x, bbox_y, bbox_w, bbox_h] <span class="op">=</span> (ratio<span class="op">*</span>np.array(ann[<span class="st">'bbox'</span>])).astype(<span class="bu">int</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        cv2.rectangle(image, (bbox_x,bbox_y),(bbox_x<span class="op">+</span>bbox_w,bbox_y<span class="op">+</span>bbox_h), color<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">255</span>),thickness<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span> <span class="op">=</span> <span class="st">'surfnet/data/images'</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>ann_dir <span class="op">=</span> os.path.join(<span class="bu">dir</span>,<span class="st">'annotations'</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> os.path.join(<span class="bu">dir</span>,<span class="st">'images'</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>ann_file <span class="op">=</span> os.path.join(ann_dir, <span class="st">'subset_of_annotations.json'</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>coco <span class="op">=</span> COCO(ann_file)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>imgIds <span class="op">=</span> np.array(coco.getImgIds())</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="sc">{}</span><span class="st"> images loaded'</span>.<span class="bu">format</span>(<span class="bu">len</span>(imgIds)))</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> imgId <span class="kw">in</span> imgIds:</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> coco.loadImgs(ids<span class="op">=</span>[imgId])[<span class="dv">0</span>]</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(os.path.join(data_dir,image[<span class="st">'file_name'</span>]))</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rotation of the picture in the Exif tags</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> orientation <span class="kw">in</span> ExifTags.TAGS.keys():</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ExifTags.TAGS[orientation]<span class="op">==</span><span class="st">'Orientation'</span>:</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        exif <span class="op">=</span> image._getexif()</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exif <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> exif[orientation] <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>                image<span class="op">=</span>image.rotate(<span class="dv">180</span>, expand<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> exif[orientation] <span class="op">==</span> <span class="dv">6</span>:</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>                image<span class="op">=</span>image.rotate(<span class="dv">270</span>, expand<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> exif[orientation] <span class="op">==</span> <span class="dv">8</span>:</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>                image<span class="op">=</span>image.rotate(<span class="dv">90</span>, expand<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> (<span class="pp">AttributeError</span>, <span class="pp">KeyError</span>, <span class="pp">IndexError</span>):</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># cases: image don't have getexif</span></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> cv2.cvtColor(np.array(image.convert(<span class="st">'RGB'</span>)),  cv2.COLOR_RGB2BGR)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    annIds <span class="op">=</span> coco.getAnnIds(imgIds<span class="op">=</span>[imgId])</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    anns <span class="op">=</span> coco.loadAnns(ids<span class="op">=</span>annIds)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    h,w <span class="op">=</span> image.shape[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    target_h <span class="op">=</span> <span class="dv">1080</span></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    ratio <span class="op">=</span> target_h<span class="op">/</span>h</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    target_w <span class="op">=</span> <span class="bu">int</span>(ratio<span class="op">*</span>w) </span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> cv2.resize(image,(target_w,target_h))</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> draw_bbox(image,anns,ratio)</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> cv2.cvtColor(image,cv2.COLOR_BGR2RGB)</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>    plt.imshow(image)</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
15 images loaded</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-4.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-5.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-6.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-7.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-8.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-9.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-10.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-11.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-12.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-13.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-14.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-15.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-5-output-16.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="video-sequences" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="video-sequences"><span class="header-section-number">3.2</span> Video sequences</h2>
<section id="data-collection-1" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="data-collection-1"><span class="header-section-number">3.2.1</span> Data collection</h3>
<p>For evaluation, an on-field study was conducted with 20 volunteers to manually count litter along three different riverbank sections in April 2021, on the Gave d’Oloron near Auterrive (Pyrénées-Atlantiques, France), using kayaks. The river sections, each 500 meters long, were precisely defined for their differences in background, vegetation, river current, light conditions and accessibility (see <a href="#sec-video-dataset-appendix" class="quarto-xref">Section&nbsp;8.2</a> for aerial views of the shooting site and details on the river sections). In total, the three videos amount to 20 minutes of footage at 24 frames per second (fps) and a resolution of 1920x1080 pixels.</p>
</section>
<section id="track-annotation" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="track-annotation"><span class="header-section-number">3.2.2</span> Track annotation</h3>
<p>On video footage, we manually recovered all visible object trajectories on each river section using an online video annotation tool (more details in <a href="#sec-video-dataset-appendix" class="quarto-xref">Section&nbsp;8.2</a> for the precise methodology). From that, we obtained a collection of distinct object tracks spanning the entire footage.</p>
</section>
</section>
</section>
<section id="optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Optical flow-based counting via Bayesian filtering and confidence regions</h1>
<p>Our counting method is divided into several interacting blocks. First, a detector outputs a set of predicted positions for objects in the current frame. The second block is a tracking module designing consistent trajectories of potential objects within the video. At each frame, a third block links the successive detections together using confidence regions provided by the tracking module, proposing distinct tracks for each object. A final postprocessing step only keeps the best tracks which are enumerated to yield the final count.</p>
<section id="detector" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="detector"><span class="header-section-number">4.1</span> Detector</h2>
<section id="center-based-anchor-free-detection" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="center-based-anchor-free-detection"><span class="header-section-number">4.1.1</span> Center-based anchor-free detection</h3>
<p>In most benchmarks, the prediction quality of object attributes like bounding boxes is often used to improve tracking. For counting, however, point detection is theoretically enough and advantageous in many ways. First, to build large datasets, a method which only requires the lightest annotation format may benefit from more data due to annotation ease. Second, contrary to previous popular methods (<span class="citation" data-cites="ren2016faster">Ren et al. (<a href="#ref-ren2016faster" role="doc-biblioref">2015</a>)</span>) involving intricate mechanisms for bounding box prediction, center-based and anchor-free detectors (<span class="citation" data-cites="Zhou2019">Zhou, Wang, and Krähenbühl (<a href="#ref-Zhou2019" role="doc-biblioref">2019</a>)</span>, <span class="citation" data-cites="Law">Law and Deng (<a href="#ref-Law" role="doc-biblioref">2018</a>)</span>) only use additional regression heads which can simply be removed for point detection. Adding to all this, (<span class="citation" data-cites="Zhanga">Zhang et al. (<a href="#ref-Zhanga" role="doc-biblioref">2021</a>)</span>) highlight conceptual and experimental reasons to favor anchor-free detection in tracking-related tasks.</p>
<p>For these reasons, we use a stripped version of CenterNet (<span class="citation" data-cites="Zhou2019">Zhou, Wang, and Krähenbühl (<a href="#ref-Zhou2019" role="doc-biblioref">2019</a>)</span>) where offset and bounding box regression heads are discarded to output bare estimates of center positions on a coarse grid. An encoder-decoder network takes an input image <span class="math inline">I \in [0,1]^{w \times h \times 3}</span> (an RGB image of width <span class="math inline">w</span> and height <span class="math inline">h</span>), and produces a heatmap <span class="math inline">\hat{Y} \in [0,1]^{\lfloor
w/p\rfloor \times \lfloor h/p\rfloor}</span> such that <span class="math inline">\hat{Y}_{xy}</span> is the probability that <span class="math inline">(x,y)</span> is the center of an object (<span class="math inline">p</span> being a stride coefficient). At inference, peak detection and thresholding are applied to <span class="math inline">\hat{Y}</span>, yielding the set of detections. The bulk of this detector relies on the DLA34 architecture (<span class="citation" data-cites="fisher2017">Yu et al. (<a href="#ref-fisher2017" role="doc-biblioref">2018</a>)</span>). In a video, for each frame <span class="math inline">I_n \in
[0,1]^{w \times h \times 3}</span> (where <span class="math inline">n</span> indexes the frame number), the detector outputs a set <span class="math inline">\mathcal{D}_n = \{z_n^i\}_{1 \leq i \leq D_n}</span> where each <span class="math inline">z_n^i = (x_n^i,y_n^i)</span> specifies the coordinates of one of the <span class="math inline">D_n</span> detected objects.</p>
</section>
</section>
<section id="sec-detector_training" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-detector_training"><span class="header-section-number">4.2</span> Training</h2>
<p>Training the detector is done similarly as in <span class="citation" data-cites="Proenca2020">Proença and Simões (<a href="#ref-Proenca2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>For every image, the corresponding set <span class="math inline">\mathcal{B} =
\{(c^w_i,c^h_i,w_i,h_i)\}_{1 \leq i\leq B}</span> of <span class="math inline">B</span> annotated bounding boxes – <em>i.e.</em> a center <span class="math inline">(c^w_i,c^h_i)</span>, a width <span class="math inline">w_i</span> and a height <span class="math inline">h_i</span>– is rendered into a ground truth heatmap <span class="math inline">Y \in [0,1]^{{\lfloor w/p\rfloor \times
\lfloor h/p\rfloor}}</span> by applying kernels at the bounding box centers and taking element-wise maximum. For all <span class="math inline">1 \leq x \leq w/p</span>, <span class="math inline">1 \leq y \leq h/p</span>, the ground truth at <span class="math inline">(x,y)</span> is</p>
<p><span class="math display">
  Y_{xy} =  \max\limits_{1\leq i\leq B}\left(\exp\left\{-\frac{(x-c_i^w)^2+(y-c_i^h)^2}{2\sigma^2_i}\right\}\right),
</span></p>
<p>where <span class="math inline">\sigma_i</span> is a parameter depending on the size of the object. Training the detector is done by minimizing a penalty-reduced weighted focal loss</p>
<p><span class="math display">
\mathcal{L}(\hat{Y},Y) = -\sum_{x,y} \gamma_{xy}^\beta\left(1-\hat{p}_{xy}\right)^\alpha \log{\left(\hat{p}_{xy}\right)},
</span></p>
<p>where <span class="math inline">\alpha</span>, <span class="math inline">\beta</span> are hyperparameters and</p>
<p><span class="math display">
(\hat{p}_{xy},\gamma_{xy}) = \left\{
    \begin{array}{ll}
        (\hat{Y}_{xy},1) &amp; \mbox{if } Y_{xy} = 1, \\
        (1 - \hat{Y}_{xy},1 - Y_{xy}) &amp; \mbox{otherwise.}
    \end{array}
\right.
</span></p>
</section>
<section id="sec-bayesian_tracking" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-bayesian_tracking"><span class="header-section-number">4.3</span> Bayesian tracking with optical flow</h2>
<section id="optical-flow" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="optical-flow"><span class="header-section-number">4.3.1</span> Optical flow</h3>
<p>Between two timesteps <span class="math inline">n-1</span> and <span class="math inline">n</span>, the optical flow <span class="math inline">\Delta_n</span> is a mapping satisfying the following consistency constraint (<span class="citation" data-cites="paragios2006">Paragios, Chen, and Faugeras (<a href="#ref-paragios2006" role="doc-biblioref">2006</a>)</span>):</p>
<p><span class="math display">
\widetilde{I}_n[u] = \widetilde{I}_{n-1}[u+\Delta_n(u)],
</span></p>
<p>where, in our case, <span class="math inline">\widetilde{I}_n</span> denotes the frame <span class="math inline">n</span> downsampled to dimensions <span class="math inline">\lfloor w/p\rfloor \times \lfloor h/p\rfloor</span> and <span class="math inline">u = (x,y)</span> is a coordinate on that grid. To estimate <span class="math inline">\Delta_n</span>, we choose a simple unsupervised Gunner-Farneback algorithm which does not require further annotations, see <span class="citation" data-cites="farneback2003two">Farnebäck (<a href="#ref-farneback2003two" role="doc-biblioref">2003</a>)</span> for details.</p>
</section>
<section id="sec-state_space_model" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="sec-state_space_model"><span class="header-section-number">4.3.2</span> State space model</h3>
<p>Using optical flow as a building block, we posit a state space model where estimates of <span class="math inline">\Delta_n</span> are used as a time and state-dependent offset for the state transition.</p>
<p>Let <span class="math inline">(X_k)_{k \geq 1}</span> and <span class="math inline">(Z_k)_{k \geq 1}</span> be the true (but hidden) and observed (detected) positions of a target object in <span class="math inline">\mathbb{R}^2</span>, respectively.</p>
<p>Considering the optical flow value associated with <span class="math inline">X_{k-1}</span> on the discrete grid of dimensions <span class="math inline">\lfloor w/p\rfloor \times \lfloor h/p\rfloor</span>, write</p>
<p><span id="eq-state-transition"><span class="math display">
X_k = X_{k-1} + \Delta_k(\lfloor X_{k-1} \rfloor) + \eta_k
\tag{1}</span></span></p>
<p>and</p>
<p><span class="math display">
Z_k = X_k + \varepsilon_k,
</span></p>
<p>where <span class="math inline">(\eta_k)_{k\geq 1}</span> are i.i.d. centered Gaussian random variables with covariance matrix <span class="math inline">Q</span> independent of <span class="math inline">(\varepsilon_k)_{k\geq 1}</span> i.i.d. centered Gaussian random variables with covariance matrix <span class="math inline">R</span>. In the following, <span class="math inline">Q</span> and <span class="math inline">R</span> are assumed to be diagonal, and are hyperparameters set to values given in <a href="#sec-covariance_matrices" class="quarto-xref">Section&nbsp;8.3.1</a>.</p>
</section>
<section id="approximations-of-the-filtering-distributions" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="approximations-of-the-filtering-distributions"><span class="header-section-number">4.3.3</span> Approximations of the filtering distributions</h3>
<p>Denoting <span class="math inline">u_{1:k} = (u_1,\ldots,u_k)</span> for any <span class="math inline">k</span> and sequence <span class="math inline">(u_i)_{i \geq
0}</span>, Bayesian filtering aims at computing the conditional distribution of <span class="math inline">X_k</span> given <span class="math inline">Z_{1:k}</span>, referred to as the filtering distribution. In the case of linear and Gaussian state space models, this distribution is known to be Gaussian, and Kalman filtering allows to update exactly the posterior mean <span class="math inline">\mu_k = \mathbb{E}[X_k|Z_{1:k}]</span> and posterior variance matrix <span class="math inline">\Sigma_k =
\mathbb{V}[X_k|Z_{1:k}]</span>. This algorithm and its extensions are prevalent and used extensively in time-series and sequential-data analysis. As the transition model proposed in <a href="#eq-state-transition" class="quarto-xref">Equation&nbsp;1</a> is nonlinear, Kalman updates cannot be implemented and solving the target tracking task requires resorting to alternatives. Many solutions have been proposed to deal with strong nonlinearities in the literature, such as unscented Kalman filters (UKF) or Sequential Monte Carlo (SMC) methods (see <span class="citation" data-cites="sarkka2013bayesian">Särkkä (<a href="#ref-sarkka2013bayesian" role="doc-biblioref">2013</a>)</span> and references therein). Most SMC methods have been widely studied and shown to be very effective even in presence of strongly nonlinear dynamics and/or non-Gaussian noise, however such sample-based solutions are computationally intensive, especially in settings where many objects have to be tracked and false positive detections involve unnecessary sampling steps. On the other hand, UKF requires fewer samples and provides an intermediary solution in presence of mild nonlinearities. In our setting, we find that a linearisation of the model <a href="#eq-state-transition" class="quarto-xref">Equation&nbsp;1</a> yields approximation which is computationally cheap and as robust on our data:</p>
<p><span class="math display">
X_k = X_{k-1} + \Delta_k(\lfloor \mu_{k-1} \rfloor) + \partial_X\Delta_k(\lfloor \mu_{k-1} \rfloor)(X_{k-1}-\mu_{k-1}) + \eta_k .
</span> where <span class="math inline">\partial_X</span> is the derivative operator with respect to the 2-dimensional spatial input <span class="math inline">X</span>.</p>
<p>This allows the implementation of Kalman updates on the linearised model, a technique named extended Kalman filtering (EKF). For a more complete presentation of Bayesian and Kalman filtering, please refer to <a href="#sec-bayesian_filtering" class="quarto-xref">Section&nbsp;8.4</a>. On the currently available data, we find that the optical flow estimates are very informative and accurate, making this approximation sufficient. For completeness, we present <a href="#sec-impact-algorithm-appendix" class="quarto-xref">Section&nbsp;8.5.1</a> an SMC-based solution and discuss the empirical differences and use-cases where the latter might be a more relevant choice.</p>
<p>In any case, the state space model naturally accounts for missing observations, as the contribution of <span class="math inline">\Delta_k</span> in every transition ensures that each filter can cope with arbitrary inter-frame motion to keep track of its target.</p>
</section>
<section id="generating-potential-object-tracks" class="level3" data-number="4.3.4">
<h3 data-number="4.3.4" class="anchored" data-anchor-id="generating-potential-object-tracks"><span class="header-section-number">4.3.4</span> Generating potential object tracks</h3>
<p>The full MOT algorithm consists of a set of single-object trackers following the previous model, but each provided with distinct observations at every frame. These separate filters provide track proposals for every object detected in the video.</p>
</section>
</section>
<section id="sec-data_association" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="sec-data_association"><span class="header-section-number">4.4</span> Data association using confidence regions</h2>
<p>Throughout the video, depending on various conditions on the incoming detections, existing trackers must be updated (with or without a new observation) and others might need to be created. This setup requires a third party data association block to link the incoming detections with the correct filters.</p>
<p>At the frame <span class="math inline">n</span>, a set of <span class="math inline">L_n</span> Bayesian filters track previously seen objects and a new set of detections <span class="math inline">\mathcal{D}_n</span> is provided by the detector. Denote by <span class="math inline">1 \leq \ell \leq L_n</span> the index of each filter at time <span class="math inline">n</span>, and by convention write <span class="math inline">Z^\ell_{1:n-1}</span> the previous observed positions associated with index <span class="math inline">\ell</span> (even if no observation is available at some past times for that object). Let <span class="math inline">\rho \in (0,1)</span> be a confidence level.</p>
<ol type="1">
<li><p>For every detected object <span class="math inline">z_n^i \in \mathcal{D}_n</span> and every filter <span class="math inline">\ell</span>, compute <span class="math inline">P(i,\ell) = \mathbb{P}(Z_n^\ell \in V_\delta(z_n^i)\mid
Z^\ell_{1:n-1})</span> where <span class="math inline">V_\delta(z)</span> is the neighborhood of <span class="math inline">z</span> defined as the squared area of width <span class="math inline">2\delta</span> centered on <span class="math inline">z</span> (see <a href="#sec-confidence_regions_appendix" class="quarto-xref">Section&nbsp;8.5</a> for exact computations).</p></li>
<li><p>Using the Hungarian algorithm (<span class="citation" data-cites="kuhn">Kuhn (<a href="#ref-kuhn" role="doc-biblioref">1955</a>)</span>), compute the assignment between detections and filters with <span class="math inline">P</span> as cost function, but discarding associations <span class="math inline">(i,\ell)</span> having <span class="math inline">P(i,\ell) &lt; \rho</span>. Formally, <span class="math inline">\rho</span> represents the level of a confidence region centered on detections and we use <span class="math inline">\rho = 0.5</span>. Denote <span class="math inline">a_{\rho}</span> the resulting assignment map defined as <span class="math inline">a_{\rho}(i) = \ell</span> if <span class="math inline">z_n^i</span> was associated with the <span class="math inline">\ell</span>-th filter, and <span class="math inline">a_{\rho}(i) = 0</span> if <span class="math inline">z_n^i</span> was not associated with any filter.</p></li>
<li><p>For <span class="math inline">1 \leq i \leq D_n</span>, if <span class="math inline">a_{\rho}(i) = \ell</span>, use <span class="math inline">z_n^i</span> as a new observation to update the <span class="math inline">\ell</span>-th filter. If <span class="math inline">a_{\rho}(i) = 0</span>, create a new filter initialized from the prior distribution, i.e. sample the true location as a Gaussian random variable with mean <span class="math inline">z_n^i</span> and variance <span class="math inline">R</span>.</p></li>
<li><p>For all filters <span class="math inline">\ell'</span> which were not provided a new observation, update only the predictive law of <span class="math inline">X^{\ell'}_{n}</span> given <span class="math inline">Z^{\ell'}_{1:n-1}</span>.</p></li>
</ol>
<p>In other words, we seek to associate filters and detections by maximising a global cost built from the predictive distributions of the available filters, but an association is only valid if its corresponding predictive probability is high enough. Though the Hungarian algorithm is a very popular algorithm in MOT, it is often used with the Euclidean distance or an Intersection-over-Union (IoU) criterion. Using confidence regions for the distributions of <span class="math inline">Z_n</span> given <span class="math inline">Z_{1:(n - 1)}</span> instead allows to naturally include uncertainty in the decision process. Note that we deactivate filters whose posterior mean estimates lie outside the image subspace in <span class="math inline">\mathbb{R}^2</span>.</p>
<p>A visual depiction of the entire pipeline (from detection to final association) is provided below. This way of combining a set of Bayesian filters with a data association step that resorts on the most likely hypothesis is a form of Global Nearest Neighbor (GNN) tracking. Another possibility is to perform multi-target filtering by including the data association step directly into the probabilistic model, as in <span class="citation" data-cites="mahler2003">Mahler (<a href="#ref-mahler2003" role="doc-biblioref">2003</a>)</span>. A generalisation of single-target recursive Bayesian filtering, this class of methods is grounded in the point process literature and well motivated theoretically. In case of strong false positive detection rates, close and/or reappearing objects, practical benefits may be obtained from these solutions. Finally, note that another well-motivated choice for <span class="math inline">P(i,\ell)</span> could be to use the marginal likelihood <span class="math inline">\mathbb{P}(Z_n^\ell \in
V_\delta(z_n^i))</span>, which is standard in modern MOT.</p>
<div id="fig-diagram" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/diagram.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Visual representation of the tracking pipeline.
</figcaption>
</figure>
</div>
</section>
<section id="counting" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="counting"><span class="header-section-number">4.5</span> Counting</h2>
<p>At the end of the video, the previous process returns a set of candidate tracks. For counting purposes, we find that simple heuristics can be further applied to filter out tracks that do not follow actual objects. More precisely, we observe that tracks of real objects usually contain more (i) observations and (ii) streams of uninterrupted observations. Denote by <span class="math inline">T_\ell
= \left\{n \in \mathbb{N} \mid \exists  z \in \mathcal{D}_n,  Z_n^{\ell} =
z\right\}</span> all timesteps where the <span class="math inline">\ell</span>-th object is observed. To discard false counts according to (i) and (ii), we compute the moving average <span class="math inline">M_\ell^\kappa</span> of <span class="math inline">1_{T_\ell}</span> using windows of size <span class="math inline">\kappa</span>, i.e.&nbsp;the sequence defined by <span class="math inline">M_\ell^\kappa[n] = \frac{1}{\kappa} \sum_{k \in [\![n -
\kappa, n + \kappa]\!]} 1_{T_\ell}[k]</span>. We then build <span class="math inline">T_\ell^\kappa =
\left\{n \in T_\ell \mid M_\ell^\kappa[n] &gt; \nu\right\}</span>, and defining <span class="math inline">\mathcal{N} = \left\{\ell \mid |T_\ell^\kappa| &gt; \tau\right\}</span>, the final object count is <span class="math inline">|\mathcal{N}|</span>. We choose <span class="math inline">\nu = 0.6</span> while <span class="math inline">\kappa,\tau</span> are optimized for best count performance (see <a href="#sec-tau_kappa_appendix" class="quarto-xref">Section&nbsp;8.3.2</a> for a more comprehensive study).</p>
</section>
</section>
<section id="metrics-for-mot-based-counting" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Metrics for MOT-based counting</h1>
<p>Counting in videos using embedded moving cameras is not a common task, and as such it requires a specific evaluation protocol to understand and compare the performance of competing methods. First, not all MOT metrics are relevant, even if some do provide insights to assist evaluation of count performance. Second, considering only raw counts on long videos gives little information on which of the final counts effectively arise from well detected objects.</p>
<section id="count-related-mot-metrics" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="count-related-mot-metrics"><span class="header-section-number">5.1</span> Count-related MOT metrics</h2>
<p>Popular MOT benchmarks usually report several sets of metrics such as ClearMOT (<span class="citation" data-cites="bernardin2008">Bernardin and Stiefelhagen (<a href="#ref-bernardin2008" role="doc-biblioref">2008</a>)</span>) or IDF1 (<span class="citation" data-cites="RistaniSZCT16">Ristani et al. (<a href="#ref-RistaniSZCT16" role="doc-biblioref">2016</a>)</span>) which can account for different components of tracking performance. Recently, (<span class="citation" data-cites="luiten2020">Luiten et al. (<a href="#ref-luiten2020" role="doc-biblioref">2021</a>)</span>) built the so-called HOTA metrics that allow separate evaluation of detection and association using the Jaccard index. The following components of their work are relevant to our task (we provide equation numbers in the original paper for formal definitions).</p>
<section id="detection" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="detection"><span class="header-section-number">5.1.1</span> Detection</h3>
<p>First, when considering all frames independently, traditional detection recall (<span class="math inline">\mathsf{DetRe}</span>) and precision (<span class="math inline">\mathsf{DetPr}</span>) can be computed to assess the capabilities of the object detector. Denoting with <span class="math inline">\mathsf{TP}_n</span>, <span class="math inline">\mathsf{FP}_n</span>, <span class="math inline">\mathsf{FN}_n</span> the number of true positive, false positive and false negative detections at frame <span class="math inline">n</span>, respectively, we define <span class="math inline">\mathsf{TP} = \sum_n \mathsf{TP}_n</span>, <span class="math inline">\mathsf{FP} = \sum_n \mathsf{FP}_n</span> and <span class="math inline">\mathsf{FN} =
\sum_n \mathsf{FN}_n</span>, then:</p>
<p><span class="math display">\mathsf{DetRe} = \frac{\mathsf{TP}}{\mathsf{TP} + \mathsf{FN}},</span></p>
<p><span class="math display">\mathsf{DetPr} = \frac{\mathsf{TP}}{\mathsf{TP} + \mathsf{FP}}.</span></p>
<p>In classical object detection, those metrics are the main target. In our context, as the first step of the system, this framewise performance impacts the difficulty of counting. However, we must keep in mind that these metrics are computed framewise and might not guarantee anything at a video scale. The next points illustrate that remark.</p>
<ol type="1">
<li>If both <span class="math inline">\mathsf{DetRe}</span> and <span class="math inline">\mathsf{DetPr}</span> are very high, objects are detected at nearly all frames and most detections come from actual objects. Therefore, robustness to missing observations is high, but even in this context computing associations may fail if camera movements are nontrivial.</li>
<li>For an ideal tracking algorithm which never counts individual objects twice and does not confuse separate objects in a video, a detector capturing each object for only one frame could theoretically be used. Thus, low <span class="math inline">\mathsf{DetRe}</span> could theoretically be compensated with robust tracking.</li>
<li>If our approach can rule out faulty tracks which do not follow actual objects, then good counts can still be obtained using a detector generating many false positives. Again, this suggests that low <span class="math inline">\mathsf{DetPr}</span> may allow decent counting performance.</li>
</ol>
</section>
<section id="association" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="association"><span class="header-section-number">5.1.2</span> Association</h3>
<p>HOTA association metrics are built to measure tracking performance irrespective of the detection capabilities, by comparing predicted tracks against true object trajectories. In our experiments, we compute the Association Recall (<span class="math inline">\mathsf{AssRe}</span>) and the Association Precision (<span class="math inline">\mathsf{AssPr}</span>). Several intermediate quantities are necessary to introduce these final metrics. Following <span class="citation" data-cites="luiten2020">Luiten et al. (<a href="#ref-luiten2020" role="doc-biblioref">2021</a>)</span>, we denote with <span class="math inline">\mathsf{prID}</span> the ID of a predicted track and <span class="math inline">\mathsf{gtID}</span> the ID of a ground truth track. Given <span class="math inline">C</span> all couples of <span class="math inline">\mathsf{prID}-\mathsf{gtID}</span> found among the true positive detections, and <span class="math inline">c \in C</span> one of these couples, <span class="math inline">\mathsf{TPA}(c)</span> is the number of frames where <span class="math inline">\mathsf{prID}</span> is also associated with <span class="math inline">\mathsf{gtID}</span>, <span class="math inline">\mathsf{FPA}(c)</span> is the number of frames where <span class="math inline">\mathsf{prID}</span> is associated with another ground truth ID or with no ground truth ID, and <span class="math inline">\mathsf{FNA}(c)</span> is the number of frames where <span class="math inline">\mathsf{gtID}</span> is associated with another predicted ID or with no predicted ID. Then:</p>
<p><span class="math display">\mathsf{AssPr} = \frac{1}{\mathsf{TP}} \sum_{c \in C} \frac{\mathsf{TPA}(c)}{\mathsf{TPA}(c) + \mathsf{FPA}(c)},</span></p>
<p><span class="math display">\mathsf{AssRe} = \frac{1}{\mathsf{TP}} \sum_{c \in C} \frac{\mathsf{TPA}(c)}{\mathsf{TPA}(c) + \mathsf{FNA}(c)}.</span></p>
<p>See <span class="citation" data-cites="luiten2020">Luiten et al. (<a href="#ref-luiten2020" role="doc-biblioref">2021</a>)</span> (fig.&nbsp;2) for a clear illustration of these quantities.</p>
<p>In brief, a low <span class="math inline">\mathsf{AssPr}</span> implies that several objects are often mingled into only one track, resulting in undercount. A low <span class="math inline">\mathsf{AssRe}</span> implies that single objects are often associated with multiple tracks. If no method is used to discard redundant tracks this results in overcount. Conversely, association precision (<span class="math inline">\mathsf{AssPr}</span>) measures how exclusive tracks are to each object (it decreases whenever a track covers multiple objects). Again, it is useful to reconsider and illustrate the meaning of these metrics in the context of MOT-based counting. Litter items are typically well separated on river banks, thus predicted tracks are not expected to interfere much. This suggests that reaching high <span class="math inline">\mathsf{AssPr}</span> on our footage is not challenging. Contrarily, <span class="math inline">\mathsf{AssRe}</span> is a direct measurement of the capability of the tracker to avoid producing multiple tracks despite missing detections and challenging motion. A high <span class="math inline">\mathsf{AssRe}</span> therefore typically avoids multiple counts for the same object, which is a key aspect of our work.</p>
<p>Nonetheless, association metrics are only computed for predicted tracks which can effectively be matched with ground truth tracks. Consequently, <span class="math inline">\mathsf{AssRe}</span> does not account for tracks predicted from streams of false positive detections generated by the detector (e.g. arising from rocks, water reflections, etc). Since such tracks induce false counts, a tracker which produces the fewest is better, but MOT metrics do not measure it.</p>
</section>
</section>
<section id="count-metrics" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="count-metrics"><span class="header-section-number">5.2</span> Count metrics</h2>
<p>Denoting by <span class="math inline">\mathsf{\hat{N}}</span> and <span class="math inline">\mathsf{N}</span> the respective predicted and ground truth counts for the validation material, the error <span class="math inline">\mathsf{\hat{N}} - \mathsf{N}</span> is misleading as no information is provided on the quality of the predicted counts. Additionally, results on the original validation footage do not measure the statistical variability of the proposed estimators.</p>
<section id="count-decomposition" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="count-decomposition"><span class="header-section-number">5.2.1</span> Count decomposition</h3>
<p>Define <span class="math inline">i \in [\![1, \mathsf{N}]\!]</span> and <span class="math inline">j \in [\![1, \mathsf{\hat{N}}]\!]</span> the labels of the annotated ground truth tracks and the predicted tracks, respectively. At evaluation, we assign each predicted track to either none or at most one ground truth track, writing <span class="math inline">j \rightarrow \emptyset</span> or <span class="math inline">j \rightarrow i</span> for the corresponding assignments. The association is made whenever a predicted track <span class="math inline">i</span> overlaps with a ground truth track <span class="math inline">j</span> at any frame, i.e.&nbsp;for a given frame a detection in <span class="math inline">i</span> is within a threshold <span class="math inline">\alpha</span> of an object in <span class="math inline">j</span>. We compute metrics for 20 values of <span class="math inline">\alpha \in [0.05 \alpha_{max}, 0.95
\alpha_{max}]</span>, with <span class="math inline">\alpha_{max} = 0.1 \sqrt{w^2 + h^2}</span>, then average the results, which is the default method in HOTA to combine results at different thresholds. We keep this default solution, in particular because our results are very consistent accross different thresholds in that range (we only observe a slight decrease in performance for <span class="math inline">\alpha = \alpha_{max}</span>, where occasional false detections probably start to lie below the threshold).</p>
<p>Denote <span class="math inline">A_i = \{j \in [\![1, \mathsf{\hat{N}}]\!] \mid j \rightarrow i\}</span> the set of predicted tracks assigned to the <span class="math inline">i</span>-th ground truth track. We define:</p>
<ol type="1">
<li><span class="math inline">\mathsf{\hat{N}_{true}} = \sum_{i=1}^{\mathsf{N}} 1_{|A_i| &gt; 0}</span> the number of ground truth objects successfully counted.</li>
<li><span class="math inline">\mathsf{\hat{N}_{red}} = \sum_{i=1}^{\mathsf{N}} |A_i| - \mathsf{\hat{N}_{true}}</span> the number of redundant counts per ground truth object.</li>
<li><span class="math inline">\mathsf{\hat{N}_{mis}} = \mathsf{N} - \mathsf{\hat{N}_{true}}</span> the number of ground truth objects that are never effectively counted.</li>
<li><span class="math inline">\mathsf{\hat{N}_{false}} = \sum_{j=1}^{\mathsf{\hat{N}}} 1_{j \rightarrow \emptyset}</span> the number of counts which cannot be associated with any ground truth object and are therefore considered as false counts.</li>
</ol>
<p>Using these metrics provides a much better understanding of <span class="math inline">\mathsf{\hat{N}}</span> as</p>
<p><span class="math display">
\mathsf{\hat{N}} = \mathsf{\hat{N}_{true}} + \mathsf{\hat{N}_{red}} + \mathsf{\hat{N}_{false}},
</span> while <span class="math inline">\mathsf{\hat{N}_{mis}}</span> completely summarises the number of undetected objects.</p>
<p>Conveniently, the quantities can be used to define the count precision (<span class="math inline">\mathsf{CountPR}</span>) and count recall (<span class="math inline">\mathsf{CountRe}</span>) as follows:</p>
<p><span class="math display">
\mathsf{CountPR} = \frac{\mathsf{\hat{N}_{true}}}{\mathsf{\hat{N}_{true}} + \mathsf{\hat{N}_{red}} + \mathsf{\hat{N}_{false}}},
</span></p>
<p><span class="math display">
\mathsf{CountRe} = \frac{\mathsf{\hat{N}_{true}}}{\mathsf{\hat{N}_{true}} + \mathsf{\hat{N}_{mis}}},
</span></p>
<p>which provide good summaries for the overall count quality, letting aside the tracking performance.</p>
<p>Note that these metrics and the associated decomposition are only defined if the previous assignment between predicted and ground truth tracks can be obtained. In our case, predicted tracks never overlap with several ground truth tracks (because true objects are well separated), and therefore this assignment is straightforward. More involved metrics have been studied at the trajectory level (see for example <span class="citation" data-cites="garcia2020">García-Fernández, Rahmathullah, and Svensson (<a href="#ref-garcia2020" role="doc-biblioref">2020</a>)</span> and the references therein), though not specifically tailored to the restricted task of counting. For more complicated data, an adaptation of such contributions into proper counting metrics could be valuable.</p>
</section>
<section id="statistics" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="statistics"><span class="header-section-number">5.2.2</span> Statistics</h3>
<p>Since the original validation set comprises only a few unequally long videos, only absolute results are available. Splitting the original sequences into shorter independent sequences of equal length allows to compute basic statistics. For any quantity <span class="math inline">\mathsf{\hat{N}}_\bullet</span> defined above, we provide <span class="math inline">\hat{\sigma}_{\mathsf{\hat{N}}_\bullet}</span> the associated empirical standard deviations computed on the set of short sequences.</p>
</section>
</section>
</section>
<section id="experiments" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Experiments</h1>
<p>We denote by <span class="math inline">S_1</span>, <span class="math inline">S_2</span> and <span class="math inline">S_3</span> the three river sections of the evaluation material and split the associated footage into independent segments of 30 seconds. We further divide this material into two distinct validation (6min30) and test (7min) splits.</p>
<p>To demonstrate the benefits of our work, we select two multi-object trackers and build competing counting systems from them. Our first choice is SORT <span class="citation" data-cites="Bewley2016">Bewley et al. (<a href="#ref-Bewley2016" role="doc-biblioref">2016</a>)</span>, which relies on Kalman filtering with velocity updated using the latest past estimates of object positions. Similar to our system, it only relies on image supervision for training, and though DeepSORT (<span class="citation" data-cites="Wojke2018">Wojke, Bewley, and Paulus (<a href="#ref-Wojke2018" role="doc-biblioref">2018</a>)</span>) is a more recent alternative with better performance, the associated deep appearance network cannot be used without additional video annotations. FairMOT (<span class="citation" data-cites="Zhanga">Zhang et al. (<a href="#ref-Zhanga" role="doc-biblioref">2021</a>)</span>), a more recent alternative, is similarly intended for use with video supervision but allows self-supervised training using only an image dataset. Built as a new baseline for MOT, it combines linear constant-velocity Kalman filtering with visual features computed by an additional network branch and extracted at the position of the estimated object centers, as introduced in CenterTrack (<span class="citation" data-cites="zhou2020">Zhou, Koltun, and Krähenbühl (<a href="#ref-zhou2020" role="doc-biblioref">2020</a>)</span>). We choose FairMOT to compare our method to a solution based on deep visual feature extraction.</p>
<p>Similar to our work, FairMOT uses CenterNet for the detection part and the latter is therefore trained as in <a href="#sec-detector_training" class="quarto-xref">Section&nbsp;4.2</a>. We train it using hyperparameters from the original paper. The detection outputs are then shared between all counting methods, allowing fair comparison of counting performance given a fixed object detector. We run all experiments at 12fps, an intermediate framerate to capture all objects while reducing the computational burden.</p>
<section id="detection-1" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="detection-1"><span class="header-section-number">6.1</span> Detection</h2>
<p>In the following section, we present the performance of the trained detector. Having annotated all frames of the evaluation videos, we directly compute <span class="math inline">\mathsf{DetRe}</span> and <span class="math inline">\mathsf{DetPr}</span> on those instead of a test split of the image dataset used for training. This allows realistic assessment of the detection quality of our system on true videos that may include blurry frames or artifacts caused by strong motion. We observe low <span class="math inline">\mathsf{DetRe}</span>, suggesting that objects are only captured on a fraction of the frames they appear on. To better focus on count performance in the next sections, we remove segments that do not generate any correct detection: performance on the remaining footage is increased and given by <span class="math inline">\mathsf{DetRe}^{*}</span> and <span class="math inline">\mathsf{DetPr}^{*}</span>.</p>
<div id="7ccd9f44" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>fps <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>fps <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>fps<span class="sc">}</span><span class="ss">fps'</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>split <span class="op">=</span> <span class="st">'test'</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>long_segments_names <span class="op">=</span> [<span class="st">'part_1_1'</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'part_1_2'</span>,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'part_2'</span>,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'part_3'</span>]</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>indices_test <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">7</span>,<span class="dv">9</span>,<span class="dv">13</span>]</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>indices_val <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">14</span>]</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>indices_det <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">17</span>,<span class="dv">24</span>,<span class="dv">38</span>]</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>alpha_type <span class="op">=</span> <span class="st">'___50'</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_split(split):</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> split <span class="op">==</span> <span class="st">'val'</span>:</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> indices_val</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> split <span class="op">==</span> <span class="st">'test'</span>:</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> indices_test</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    gt_dir_short <span class="op">=</span> <span class="ss">f'TrackEval/data/gt/surfrider_short_segments_</span><span class="sc">{</span>fps<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    eval_dir_short <span class="op">=</span> <span class="ss">f'TrackEval/data/trackers/surfrider_short_segments_</span><span class="sc">{</span>fps<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> split <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: </span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        gt_dir_short <span class="op">+=</span> <span class="ss">f'_</span><span class="sc">{</span>split<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        eval_dir_short <span class="op">+=</span> <span class="ss">f'_</span><span class="sc">{</span>split<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    gt_dir_short <span class="op">+=</span> <span class="st">'/surfrider-test'</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> indices, eval_dir_short, gt_dir_short</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>indices, eval_dir_short, gt_dir_short <span class="op">=</span> set_split(split)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_det_values(index_start<span class="op">=</span><span class="dv">0</span>, index_stop<span class="op">=-</span><span class="dv">1</span>):</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    results_for_det <span class="op">=</span> pd.read_csv(os.path.join(<span class="ss">f'TrackEval/data/trackers/surfrider_short_segments_</span><span class="sc">{</span>fps<span class="sc">}</span><span class="ss">'</span>,<span class="st">'surfrider-test'</span>,<span class="st">'ours_EKF_1_kappa_1_tau_0'</span>,<span class="st">'pedestrian_detailed.csv'</span>))</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    results_det <span class="op">=</span> results_for_det.loc[:,[<span class="ss">f'DetRe</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'DetPr</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>, <span class="ss">f'HOTA_TP</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'HOTA_FN</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'HOTA_FP</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>]].iloc[index_start:index_stop]</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    results_det.columns <span class="op">=</span> [<span class="st">'hota_det_re'</span>,<span class="st">'hota_det_pr'</span>,<span class="st">'hota_det_tp'</span>,<span class="st">'hota_det_fn'</span>,<span class="st">'hota_det_fp'</span>]</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    hota_det_re <span class="op">=</span> results_det[<span class="st">'hota_det_re'</span>]</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    hota_det_pr <span class="op">=</span> results_det[<span class="st">'hota_det_pr'</span>]</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    hota_det_tp <span class="op">=</span> results_det[<span class="st">'hota_det_tp'</span>]</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    hota_det_fn <span class="op">=</span> results_det[<span class="st">'hota_det_fn'</span>]</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    hota_det_fp <span class="op">=</span> results_det[<span class="st">'hota_det_fp'</span>]</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>    denom_hota_det_re <span class="op">=</span> hota_det_tp <span class="op">+</span> hota_det_fn </span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    denom_hota_det_pr <span class="op">=</span> hota_det_tp <span class="op">+</span> hota_det_fp </span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    hota_det_re_cb <span class="op">=</span> (hota_det_re <span class="op">*</span> denom_hota_det_re).<span class="bu">sum</span>() <span class="op">/</span> denom_hota_det_re.<span class="bu">sum</span>()</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>    hota_det_pr_cb <span class="op">=</span> (hota_det_pr <span class="op">*</span> denom_hota_det_pr).<span class="bu">sum</span>() <span class="op">/</span> denom_hota_det_pr.<span class="bu">sum</span>()</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [<span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>hota_det_re_cb<span class="sc">:.1f}</span><span class="ss">'</span>, <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>hota_det_pr_cb<span class="sc">:.1f}</span><span class="ss">'</span>]</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_table_det():</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    table_values <span class="op">=</span> [get_det_values(index_start, index_stop) <span class="cf">for</span> (index_start, index_stop) <span class="kw">in</span> <span class="bu">zip</span>(indices_det[:<span class="op">-</span><span class="dv">1</span>],indices_det[<span class="dv">1</span>:])]</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    table_values.append(get_det_values())</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(table_values)</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>table_det <span class="op">=</span> get_table_det()</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>table_det.columns <span class="op">=</span> [<span class="st">'DetRe*'</span>,<span class="st">'DetPr*'</span>]</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>table_det.index <span class="op">=</span> [<span class="st">'S1'</span>,<span class="st">'S2'</span>,<span class="st">'S3'</span>,<span class="st">'All'</span>]</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>display(table_det)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">DetRe*</th>
<th data-quarto-table-cell-role="th">DetPr*</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">S1</td>
<td>37.2</td>
<td>60.7</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">S2</td>
<td>29.4</td>
<td>38.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">S3</td>
<td>35.1</td>
<td>53.6</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">All</td>
<td>35.5</td>
<td>55.1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
</section>
<section id="counts" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="counts"><span class="header-section-number">6.2</span> Counts</h2>
<p>To fairly compare the three solutions, we calibrate the hyperparameters of our postprocessing block on the validation split and keep the values that minimize the overall count error <span class="math inline">\mathsf{\hat{N}}</span> for each of them separately (see <a href="#sec-tau_kappa_appendix" class="quarto-xref">Section&nbsp;8.3.2</a> for more information). All methods are found to work optimally at <span class="math inline">\kappa = 7</span>, but our solution requires <span class="math inline">\tau = 8</span> instead of <span class="math inline">\tau = 9</span> for other solutions: this lower level of thresholding suggests that raw output of our tracking system is more reliable.</p>
<p>We report results using the count-related tracking metrics and count decompositions defined in the previous section. To provide a clear but thorough summary of the performance, we report <span class="math inline">\mathsf{AssRe}</span>, <span class="math inline">\mathsf{CountRe}</span> and <span class="math inline">\mathsf{CountPR}</span> as tabled values (the first gives a simple overview of the quality of the predicted tacks while the latter two concisely summarise the count performance). For a more detailed visualisation of the different types of errors, we plot the count error decomposition for all sequences in a separate graph. Note that across all videos and all methods, we find <span class="math inline">\mathsf{AssPr}</span> between 98.6 and 99.2 which shows that this application context is unconcerned with tracks spanning multiple ground truth objects, therefore we do not conduct a more detailed interpretation of <span class="math inline">\mathsf{AssPr}</span> values.</p>
<p>First, the higher values of AssRe confirm the robustness of our solution in assigning consistent tracks to individual objects. This is directly reflected into the count precision performance - with an overall value of <span class="math inline">\mathsf{CountPR}</span> 17.6 points higher than the next best method (SORT) - or even more so in the complete disappearance of orange (redundant) counts in the graph. A key aspect is that these improvements are not counteracted by a lower <span class="math inline">\mathsf{CountRe}</span>: on the contrary, our tracker, which is more stable, also captures more object (albeit still missing most of them, with a <span class="math inline">\mathsf{CountRe}</span> below 50%). Note finally, that the strongest improvements are obtained for sequence 2 which is also the part with the strongest motion.</p>
<div id="d2836559" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_summary(results, index_start<span class="op">=</span><span class="dv">0</span>, index_stop<span class="op">=-</span><span class="dv">1</span>):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> results.loc[:,[<span class="ss">f'Correct_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'Redundant_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'False_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'Missing_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'Fused_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>, <span class="ss">f'GT_IDs'</span>,<span class="ss">f'HOTA_TP</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'AssRe</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>]].iloc[index_start:index_stop]</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    results.columns <span class="op">=</span> [<span class="st">'correct'</span>,<span class="st">'redundant'</span>,<span class="st">'false'</span>,<span class="st">'missing'</span>,<span class="st">'mingled'</span>,<span class="st">'gt'</span>,<span class="st">'hota_tp'</span>,<span class="st">'ass_re'</span>]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    ass_re <span class="op">=</span> results[<span class="st">'ass_re'</span>]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    hota_tp <span class="op">=</span> results[<span class="st">'hota_tp'</span>]</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    ass_re_cb <span class="op">=</span> (ass_re <span class="op">*</span> hota_tp).<span class="bu">sum</span>() <span class="op">/</span> hota_tp.<span class="bu">sum</span>()</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> results[<span class="st">'correct'</span>]</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    redundant <span class="op">=</span> results[<span class="st">'redundant'</span>]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    false <span class="op">=</span> results[<span class="st">'false'</span>]</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    missing <span class="op">=</span> results[<span class="st">'missing'</span>]</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    summary <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'ass_re_cb'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>ass_re_cb<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    count_re <span class="op">=</span> correct.<span class="bu">sum</span>() <span class="op">/</span> (correct <span class="op">+</span> missing).<span class="bu">sum</span>()</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'count_re_cb'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>count_re<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    count_re_std <span class="op">=</span> (correct <span class="op">/</span> (correct <span class="op">+</span> missing)).std()</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'count_re_std'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>np<span class="sc">.</span>nan_to_num(count_re_std)<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    count_pr <span class="op">=</span> correct.<span class="bu">sum</span>() <span class="op">/</span> (correct <span class="op">+</span> false <span class="op">+</span> redundant).<span class="bu">sum</span>()</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'count_pr_cb'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>count_pr<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    count_pr_std <span class="op">=</span> (correct <span class="op">/</span> (correct <span class="op">+</span> false <span class="op">+</span> redundant)).std()</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'count_pr_std'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>np<span class="sc">.</span>nan_to_num(count_pr_std)<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> summary </span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_summaries(results, sequence_names):</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    summaries <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (sequence_name, index_start, index_stop) <span class="kw">in</span> <span class="bu">zip</span>(sequence_names, indices[:<span class="op">-</span><span class="dv">1</span>], indices[<span class="dv">1</span>:]):</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        summaries[sequence_name] <span class="op">=</span> get_summary(results, index_start, index_stop)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    summaries[<span class="st">'All'</span>] <span class="op">=</span> get_summary(results)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> summaries</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>summaries <span class="op">=</span> []</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>method_names <span class="op">=</span> [<span class="st">'fairmot_kappa_7_tau_9'</span>, <span class="st">'sort_kappa_7_tau_9'</span>,<span class="st">'ours_EKF_1_kappa_7_tau_8'</span>]</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>pretty_method_names <span class="op">=</span> [<span class="st">'FairMOT'</span>,<span class="st">'SORT'</span>,<span class="st">'Ours'</span>]</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tracker_name <span class="kw">in</span> method_names:</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.read_csv(os.path.join(eval_dir_short,<span class="st">'surfrider-test'</span>,tracker_name,<span class="st">'pedestrian_detailed.csv'</span>))</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>    sequence_names <span class="op">=</span> [<span class="st">'S1'</span>,<span class="st">'S2'</span>,<span class="st">'S3'</span>]</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    summaries.append(pd.DataFrame(get_summaries(results, sequence_names)).T)</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>fairmot_star, sort, ours <span class="op">=</span> summaries</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> [<span class="st">'fairmot_kappa_7_tau_9'</span>,<span class="st">'sort_kappa_7_tau_9'</span>,<span class="st">'ours_EKF_1_kappa_7_tau_8'</span>]</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> [<span class="st">'ass_re'</span>,<span class="st">'count_re'</span>, <span class="st">'count_re_std'</span>, <span class="st">'count_pr'</span>, <span class="st">'count_pr_std'</span>]</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> summary <span class="kw">in</span> summaries:    </span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>    summary.columns <span class="op">=</span> columns</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>results_S1 <span class="op">=</span> pd.DataFrame([summary.iloc[<span class="dv">0</span>] <span class="cf">for</span> summary <span class="kw">in</span> summaries])</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>results_S1.index <span class="op">=</span> method_names</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>results_S2 <span class="op">=</span> pd.DataFrame([summary.iloc[<span class="dv">1</span>] <span class="cf">for</span> summary <span class="kw">in</span> summaries])</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>results_S2.index <span class="op">=</span> method_names</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>results_S3 <span class="op">=</span> pd.DataFrame([summary.iloc[<span class="dv">2</span>] <span class="cf">for</span> summary <span class="kw">in</span> summaries])</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>results_S3.index <span class="op">=</span> method_names</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>results_All <span class="op">=</span> pd.DataFrame([summary.iloc[<span class="dv">3</span>] <span class="cf">for</span> summary <span class="kw">in</span> summaries])</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>results_All.index <span class="op">=</span> method_names</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> [results_S1, results_S2, results_S3, results_All]</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>column_names <span class="op">=</span> [</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>    <span class="st">"AssRe"</span>,</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r"CountRe"</span>,</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r"σ(CountRe)"</span>,</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r"CountPr"</span>,</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r"σ(CountPr)"</span>]</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>row_names <span class="op">=</span> [<span class="st">"FairMOT"</span>, <span class="st">"Sort"</span>, <span class="st">"Ours"</span>]</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> result <span class="kw">in</span> results:</span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>    result.columns <span class="op">=</span> column_names</span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>    result.index <span class="op">=</span> row_names</span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>results_S1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AssRe</th>
<th data-quarto-table-cell-role="th">CountRe</th>
<th data-quarto-table-cell-role="th">σ(CountRe)</th>
<th data-quarto-table-cell-role="th">CountPr</th>
<th data-quarto-table-cell-role="th">σ(CountPr)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">FairMOT</td>
<td>62.0</td>
<td>31.2</td>
<td>25.6</td>
<td>52.6</td>
<td>24.6</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Sort</td>
<td>65.6</td>
<td>43.8</td>
<td>26.4</td>
<td>53.8</td>
<td>20.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Ours</td>
<td>79.5</td>
<td>50.0</td>
<td>27.9</td>
<td>64.0</td>
<td>23.8</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<div id="3be96954" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>results_S2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AssRe</th>
<th data-quarto-table-cell-role="th">CountRe</th>
<th data-quarto-table-cell-role="th">σ(CountRe)</th>
<th data-quarto-table-cell-role="th">CountPr</th>
<th data-quarto-table-cell-role="th">σ(CountPr)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">FairMOT</td>
<td>8.7</td>
<td>12.5</td>
<td>35.4</td>
<td>50.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Sort</td>
<td>20.7</td>
<td>12.5</td>
<td>35.4</td>
<td>33.3</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Ours</td>
<td>72.7</td>
<td>50.0</td>
<td>0.0</td>
<td>100.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<div id="a43663e8" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>results_S3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AssRe</th>
<th data-quarto-table-cell-role="th">CountRe</th>
<th data-quarto-table-cell-role="th">σ(CountRe)</th>
<th data-quarto-table-cell-role="th">CountPr</th>
<th data-quarto-table-cell-role="th">σ(CountPr)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">FairMOT</td>
<td>17.4</td>
<td>25.0</td>
<td>47.1</td>
<td>50.0</td>
<td>50.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Sort</td>
<td>19.6</td>
<td>25.0</td>
<td>47.1</td>
<td>40.0</td>
<td>50.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Ours</td>
<td>24.6</td>
<td>37.5</td>
<td>41.7</td>
<td>60.0</td>
<td>47.9</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<div id="0dd0a6f1" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>results_All</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AssRe</th>
<th data-quarto-table-cell-role="th">CountRe</th>
<th data-quarto-table-cell-role="th">σ(CountRe)</th>
<th data-quarto-table-cell-role="th">CountPr</th>
<th data-quarto-table-cell-role="th">σ(CountPr)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">FairMOT</td>
<td>56.7</td>
<td>27.1</td>
<td>31.6</td>
<td>52.0</td>
<td>30.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Sort</td>
<td>59.8</td>
<td>35.4</td>
<td>32.7</td>
<td>50.0</td>
<td>30.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Ours</td>
<td>76.0</td>
<td>47.9</td>
<td>28.8</td>
<td>67.6</td>
<td>32.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<section id="detailed-results-on-individual-segments" class="level4" data-number="6.2.0.1">
<h4 data-number="6.2.0.1" class="anchored" data-anchor-id="detailed-results-on-individual-segments"><span class="header-section-number">6.2.0.1</span> Detailed results on individual segments</h4>
<div id="aa6e8cd6" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>set_split(<span class="st">'test'</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">30</span>,<span class="dv">10</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, title, tracker_name <span class="kw">in</span> <span class="bu">zip</span>(axes, pretty_method_names,  method_names):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.read_csv(os.path.join(eval_dir_short,<span class="st">'surfrider-test'</span>,tracker_name,<span class="st">'pedestrian_detailed.csv'</span>))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> results.loc[:,[<span class="ss">f'Redundant_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'False_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'Missing_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>]].iloc[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    results.columns <span class="op">=</span> [<span class="st">'redundant'</span>, <span class="st">'false'</span>, <span class="st">'missing'</span>]</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    results.loc[:,<span class="st">'missing'</span>] <span class="op">=</span> <span class="op">-</span> results.loc[:,<span class="st">'missing'</span>]</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    results.columns <span class="op">=</span> [<span class="st">'$\hat{\mathsf</span><span class="sc">{N}</span><span class="st">}_</span><span class="sc">{red}</span><span class="st">$'</span>, <span class="st">'$\hat{\mathsf</span><span class="sc">{N}</span><span class="st">}_</span><span class="sc">{false}</span><span class="st">$'</span>, <span class="st">'$-\hat{\mathsf</span><span class="sc">{N}</span><span class="st">}_</span><span class="sc">{mis}</span><span class="st">$'</span>]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    results.plot(ax <span class="op">=</span> ax, kind<span class="op">=</span><span class="st">'bar'</span>, stacked<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>[<span class="st">'orange'</span>, <span class="st">'red'</span>, <span class="st">'black'</span>], title<span class="op">=</span>title, xlabel<span class="op">=</span><span class="st">'Sequence nb'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-11-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="practical-impact-and-future-goals" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Practical impact and future goals</h1>
<p>We successfully tackled video object counting on river banks, in particular issues which could be addressed independently of detection quality. Moreover the methodology developed to assess count quality enables us to precisely highlight the challenges that pertain to video object counting on river banks. Conducted in coordination with Surfrider Foundation Europe, an NGO specialized on water preservation, our work marks an important milestone in a broader campaign for macrolitter monitoring and is already being used in a production version of a monitoring system. That said, large amounts of litter items are still not detected. Solving this problem is largely a question of augmenting the object detector training dataset through crowdsourced images. A <a href="https://www.trashroulette.com">specific annotation platform</a> is online, thus the amount of annotated images is expected to continuously increase, while training is provided to volunteers collecting data on the field to ensure data quality. Finally, several expeditions on different rivers are already underway and new video footage is expected to be annotated in the near future for better evaluation. All data is made freely available. Future goals include downsizing the algorithm, a possibility given the architectural simplicity of anchor-free detection and the relatively low computational complexity of EKF. In a citizen science perspective, a fully embedded version for portable devices will allow a larger deployment. The resulting field data will help better understand litter origin, allowing to model and predict litter density in non surveyed areas. Correlations between macro litter density and environmental parameters will be studied (e.g., population density, catchment size, land use and hydromorphology). Finally, our work naturally benefits any extension of macrolitter monitoring in other areas (urban, coastal, etc) that may rely on a similar setup of moving cameras.</p>
</section>
<section id="supplements" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Supplements</h1>
<section id="sec-image_dataset_appendix" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="sec-image_dataset_appendix"><span class="header-section-number">8.1</span> Details on the image dataset</h2>
<section id="categories" class="level3" data-number="8.1.1">
<h3 data-number="8.1.1" class="anchored" data-anchor-id="categories"><span class="header-section-number">8.1.1</span> Categories</h3>
<p>In this work, we do not seek to precisely predict the proportions of the different types of counted litter. However, we build our dataset to allow classification tasks. Though litter classifications built by experts already exist, most are based on semantic rather than visual features and do not particularly consider the problem of class imbalance, which makes statistical learning more delicate. In conjunction with water pollution experts, we therefore define a custom macrolitter taxonomy which balances annotation ease and pragmatic decisions for computer vision applications. This classification, depicted in <a href="#fig-trash-categories-image" class="quarto-xref">Figure&nbsp;4</a> can be understood as follows.</p>
<ol type="1">
<li><p>We define a set of frequently observed classes that annotateors can choose from, divided into:</p>
<ul>
<li>Classes for rigid and easily recognisable items which are often observed and have definite shapes</li>
<li>Classes for fragmented objects which are often found along river banks but whose aspects are more varied</li>
</ul></li>
<li><p>We define two supplementary categories used whenever the annotater cannot classify the item they are observing in an image using classes given in 1.</p>
<ul>
<li>A first category is used whenever the item is clearly identifiable but its class is not proposed. This will ensure that our classification can be improved in the future, as images with items in this category will be checked regularly to decide whether a new class needs to be created.</li>
<li>Another category is used whenever the annotater does not understand the item they are seeing. Images containing items denoted as such will not be used for applications involving classification.</li>
</ul></li>
</ol>
<div id="fig-trash-categories-image" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trash-categories-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/trash_categories.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trash-categories-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Trash categories defined to facilitate porting to a counting system that allows trash identification
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-video-dataset-appendix" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="sec-video-dataset-appendix"><span class="header-section-number">8.2</span> Details on the evaluation videos</h2>
<section id="river-segments" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="river-segments"><span class="header-section-number">8.2.1</span> River segments</h3>
<p>In this section, we provide further details on the evaluation material. <a href="#fig-river-sections" class="quarto-xref">Figure&nbsp;5</a> shows the setup and positioning of the three river segments <span class="math inline">S_1</span>, <span class="math inline">S_2</span> and <span class="math inline">S_3</span> used to evaluate the methods. The segments differ in the following aspects.</p>
<ul>
<li>Segment 1: Medium current, high and dense vegetation not obstructing vision of the right riverbank from watercrafts, extra objects installed before the field experiment.</li>
<li>Segment 2: High current, low and dense vegetation obstructing vision of the right riverbank from watercrafts.</li>
<li>Segment 3: Medium current, high and little vegetation not obstructing vision of the left riverbank from watercrafts.</li>
</ul>
<div id="fig-river-sections" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-river-sections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/river_sections.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-river-sections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Aerial view of the three river segments of the evaluation material
</figcaption>
</figure>
</div>
<section id="track-annotation-protocol" class="level4" data-number="8.2.1.1">
<h4 data-number="8.2.1.1" class="anchored" data-anchor-id="track-annotation-protocol"><span class="header-section-number">8.2.1.1</span> Track annotation protocol</h4>
<p>To annotate tracks on the evaluation sequences, we used the online tool “CVAT” which allows to locate bounding boxes on video frames and propagate them in time. The following items provide further details on the exact annotation process.</p>
<ul>
<li>Object tracks start whenever a litter item becomes fully visible and identifiable by the naked eye.</li>
<li>Positions and sizes of objects are given at nearly every second of the video with automatic interpolation for frames in-between: this yields clean tracks with precise positions at 24fps.</li>
<li>We do not provide inferred locations when an object is fully occluded, but tracks restart with the same identity whenever the object becomes visible again.</li>
<li>Tracks stop whenever an object becomes indistinguishable and will not reappear again.</li>
</ul>
</section>
</section>
</section>
<section id="sec-tracking_module_appendix" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="sec-tracking_module_appendix"><span class="header-section-number">8.3</span> Implementation details for the tracking module</h2>
<section id="sec-covariance_matrices" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="sec-covariance_matrices"><span class="header-section-number">8.3.1</span> Covariance matrices for state and observation noises</h3>
<p>In our state space model, <span class="math inline">Q</span> models the noise associated with the movement model we posit in <a href="#sec-bayesian_tracking" class="quarto-xref">Section&nbsp;4.3</a> involving optical flow estimates, while <span class="math inline">R</span> models the noise associated with the observation of the true position via our object detector. An attempt to estimate the diagonal values of these matrices was the following.</p>
<ul>
<li>To estimate <span class="math inline">R</span>, we computed a mean <span class="math inline">L_2</span> error between the known positions of objects and the associated predictions by the object detector, for images in our training dataset.</li>
<li>To estimate <span class="math inline">Q</span>, we built a small synthetic dataset of consecutive frames taken from videos, where positions of objects in two consecutive frames are known. We computed a mean <span class="math inline">L_2</span> error between the known positions in the second frame and the positions estimated by shifting the positions in the first frame with the estimated optical flow values.</li>
</ul>
<p>This led to <span class="math inline">R_{00} = R_{11} = 1.1</span>, <span class="math inline">Q_{00} = 4.7</span> and <span class="math inline">Q_{11} = 0.9</span>, for grids of dimensions <span class="math inline">\lfloor w/p\rfloor \times \lfloor h/p\rfloor = 480 \times 270</span>. All other coefficients were not estimated and supposed to be 0.</p>
<p>An important remark is that though we use these values in practice, we found that tracking results are largely unaffected by small variations of <span class="math inline">R</span> and <span class="math inline">Q</span>. As long as values are meaningful relative to the image dimensions and the size of the objects, most noise levels show relatively similar performance.</p>
</section>
<section id="sec-tau_kappa_appendix" class="level3" data-number="8.3.2">
<h3 data-number="8.3.2" class="anchored" data-anchor-id="sec-tau_kappa_appendix"><span class="header-section-number">8.3.2</span> Influence of <span class="math inline">\tau</span> and <span class="math inline">\kappa</span></h3>
<p>An understanding of <span class="math inline">\kappa</span>, <span class="math inline">\tau</span> and <span class="math inline">\nu</span> can be stated as follows. For any track, given a value for <span class="math inline">\kappa</span> and <span class="math inline">\nu</span>, an observation at time <span class="math inline">n</span> is only kept if there are also <span class="math inline">\nu \cdot \kappa</span> observations in the temporal window of size <span class="math inline">\kappa</span> that surrounds <span class="math inline">n</span> (windows are centered around <span class="math inline">n</span> except at the start and end of the track). The track is only counted if the remaining number of observations is strictly higher than <span class="math inline">\tau</span>. At a given <span class="math inline">\nu &gt; 0.5</span>, <span class="math inline">\kappa</span> and <span class="math inline">\tau</span> should ideally be chosen to jointly decrease <span class="math inline">\mathsf{\hat{N}_{false}}</span> and <span class="math inline">\mathsf{\hat{N}_{red}}</span> as much as possible without increasing <span class="math inline">\mathsf{\hat{N}_{mis}}</span> (true objects become uncounted if tracks are discarded too easily).</p>
<p>In the following code cell, we plot the error decomposition of the counts for several values of <span class="math inline">\kappa</span> and <span class="math inline">\tau</span> with <span class="math inline">\nu=0.6</span> for the outputs of the three different trackers. We choose <span class="math inline">\nu = 0.7</span> and compute the optimal point as the one which minimizes the overall count error <span class="math inline">\mathsf{\hat{N}} (= \mathsf{\hat{N}_{mis}} + \mathsf{\hat{N}_{red}} + \mathsf{\hat{N}_{false}})</span>.</p>
<div id="0e9b80c0" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>set_split(<span class="st">'val'</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {<span class="st">'legend.fontsize'</span>: <span class="st">'x-large'</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>         <span class="st">'axes.labelsize'</span>: <span class="st">'x-large'</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>         <span class="st">'axes.titlesize'</span>:<span class="st">'x-large'</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>         <span class="st">'xtick.labelsize'</span>:<span class="st">'x-large'</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>         <span class="st">'ytick.labelsize'</span>:<span class="st">'x-large'</span>}</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.rcParams.update(params)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hyperparameters(method_name, pretty_method_name):</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    tau_values <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">10</span>)]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    kappa_values <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>]</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    fig, (ax0, ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    pretty_names<span class="op">=</span>[<span class="ss">f'$\kappa=</span><span class="sc">{</span>kappa<span class="sc">}</span><span class="ss">$'</span> <span class="cf">for</span> kappa <span class="kw">in</span> kappa_values]</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    n_count <span class="op">=</span> {}</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> kappa, pretty_name <span class="kw">in</span> <span class="bu">zip</span>(kappa_values, pretty_names):</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        tracker_names <span class="op">=</span> [<span class="ss">f'</span><span class="sc">{</span>method_name<span class="sc">}</span><span class="ss">_kappa_</span><span class="sc">{</span>kappa<span class="sc">}</span><span class="ss">_tau_</span><span class="sc">{</span>tau<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> tau <span class="kw">in</span> tau_values]</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        all_results <span class="op">=</span> {tracker_name: pd.read_csv(os.path.join(eval_dir_short,<span class="st">'surfrider-test'</span>,tracker_name,<span class="st">'pedestrian_detailed.csv'</span>)).iloc[:<span class="op">-</span><span class="dv">1</span>] <span class="cf">for</span> tracker_name <span class="kw">in</span> tracker_names}</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        n_missing <span class="op">=</span> []</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        n_false <span class="op">=</span> []</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        n_redundant <span class="op">=</span> []</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> tracker_name, tracker_results <span class="kw">in</span> all_results.items():</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>            missing <span class="op">=</span> (tracker_results[<span class="st">'GT_IDs'</span>].<span class="bu">sum</span>() <span class="op">-</span> tracker_results[<span class="st">'Correct_IDs___50'</span>].<span class="bu">sum</span>())</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            false <span class="op">=</span> tracker_results[<span class="st">'False_IDs___50'</span>].<span class="bu">sum</span>()</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            redundant <span class="op">=</span> tracker_results[<span class="st">'Redundant_IDs___50'</span>].<span class="bu">sum</span>()</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>            n_missing.append(missing)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            n_false.append(false)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            n_redundant.append(redundant)</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>            n_count[tracker_name] <span class="op">=</span> missing <span class="op">+</span> false <span class="op">+</span> redundant</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        ax0.scatter(tau_values, n_missing)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        ax0.plot(tau_values, n_missing, label<span class="op">=</span>pretty_name, linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ax0.set_xlabel('$\\tau$')</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        ax0.set_ylabel(<span class="st">'$N_</span><span class="sc">{mis}</span><span class="st">$'</span>)</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>        ax1.scatter(tau_values, n_false)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>        ax1.plot(tau_values, n_false, linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ax1.set_xlabel('$\\tau$')</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        ax1.set_ylabel(<span class="st">'$N_</span><span class="sc">{false}</span><span class="st">$'</span>)</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>        ax2.scatter(tau_values, n_redundant)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>        ax2.plot(tau_values, n_redundant, linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>        ax2.set_xlabel(<span class="st">'$</span><span class="ch">\\</span><span class="st">tau$'</span>)</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>        ax2.set_ylabel(<span class="st">'$N_</span><span class="sc">{red}</span><span class="st">$'</span>)</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>    best_value <span class="op">=</span> np.inf</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>    best_key <span class="op">=</span> <span class="st">''</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> n_count.items():</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> v <span class="op">&lt;</span> best_value: best_key <span class="op">=</span> k</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>        best_value <span class="op">=</span> v</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>    best_key <span class="op">=</span> best_key.split(<span class="st">'kappa'</span>)[<span class="dv">1</span>]</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>    best_kappa <span class="op">=</span> <span class="bu">int</span>(best_key.split(<span class="st">'_'</span>)[<span class="dv">1</span>])</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>    best_tau <span class="op">=</span> <span class="bu">int</span>(best_key.split(<span class="st">'_'</span>)[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Best parameters for </span><span class="sc">{</span>pretty_method_name<span class="sc">}</span><span class="ss">: (kappa, tau) = (</span><span class="sc">{</span>best_kappa<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>best_tau<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>    handles, labels <span class="op">=</span> ax0.get_legend_handles_labels()</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>    fig.legend(handles, labels, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>    plt.autoscale(<span class="va">True</span>)</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> method_name, pretty_method_name <span class="kw">in</span> <span class="bu">zip</span>(method_names, pretty_method_names):</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>    hyperparameters(method_name.split(<span class="st">'kappa'</span>)[<span class="dv">0</span>][:<span class="op">-</span><span class="dv">1</span>], pretty_method_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters for FairMOT: (kappa, tau) = (7, 9)
Best parameters for SORT: (kappa, tau) = (7, 9)
Best parameters for Ours: (kappa, tau) = (7, 8)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-12-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-12-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="published-202301-chagneux-macrolitter_files/figure-html/cell-12-output-4.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-bayesian_filtering" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="sec-bayesian_filtering"><span class="header-section-number">8.4</span> Bayesian filtering</h2>
<p>Considering a state space model with <span class="math inline">(X_k, Z_k)_{k \geq 0}</span> the random processes for the states and observations, respectively, the filtering recursions are given by:</p>
<ul>
<li>The predict step: <span class="math inline">p(x_{k+1}|z_{1:k}) = \int p(x_{k+1}|x_k)p(x_k|z_{1:k})\mathrm{d}x_k.</span></li>
<li>The update step: <span class="math inline">p(x_{k+1}|z_{1:k+1}) \propto p(z_{k+1} | x_{k+1})p(x_{k+1}|z_{1:k}).</span></li>
</ul>
<p>The recursions are intractable in most cases, but when the model is linear and Gaussian, i.e.&nbsp;such that:</p>
<p><span class="math display">X_{k} = A_kX_{k-1} + a_k + \eta_k</span> <span class="math display">Z_{k} = B_kX_{k} + b_k + \epsilon_k</span></p>
<p>with <span class="math inline">\eta_k \sim \mathcal{N}(0,Q_k)</span> and <span class="math inline">\epsilon_k \sim \mathcal{N}(0,R_k)</span>, then the distribution of <span class="math inline">X_k</span> given <span class="math inline">Z_{1:k}</span> is a Gaussian <span class="math inline">\mathcal{N}(\mu_k,\Sigma_k)</span> following:</p>
<ul>
<li><span class="math inline">\mu_{k|k-1} = A_k\mu_{k-1} + a_k</span> and <span class="math inline">\Sigma_{k|k-1} = A_k \Sigma_{k-1} A_k^T + Q_k</span> (Kalman predict step),</li>
<li><span class="math inline">\mu_{k} = \mu_{k|k-1} + K_k\left[Z_k - (B_k\mu_{k|k-1} + b_k)\right]</span> and <span class="math inline">\Sigma_{k} = (I - K_kB_k)\Sigma_{k|k-1}</span> (Kalman update step),</li>
</ul>
<p>where <span class="math inline">K_k = \Sigma_{k|k-1}B_k^T(B_k \Sigma_{k|k-1} B_k^T + R_k)^{-1}</span>.</p>
<p>In the case of the linearized model in <a href="#sec-state_space_model" class="quarto-xref">Section&nbsp;4.3.2</a>, EKF consists in applying these updates with:</p>
<p><span class="math display">A_k = (I + \partial_X\Delta_k(\lfloor \mu_{k-1} \rfloor),</span> <span class="math display">a_k = \Delta_k(\lfloor \mu_{k-1} \rfloor) - \partial_X\Delta_k(\lfloor \mu_{k-1} \rfloor)\mu_{k-1},</span> <span class="math display">Q_k = Q, R_k = R,</span> <span class="math display">B_k = I, b_k = 0.</span></p>
</section>
<section id="sec-confidence_regions_appendix" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="sec-confidence_regions_appendix"><span class="header-section-number">8.5</span> Computing the confidence regions</h2>
<p>In words, <span class="math inline">P(i,\ell)</span> is the mass in <span class="math inline">V_\delta(z_n^i) \subset \mathbb{R}^2</span> of the probability distribution of <span class="math inline">Z_n^\ell</span> given <span class="math inline">Z_{1:n-1}^\ell</span>. It is related to the filtering distribution at the previous timestep via</p>
<p><span class="math display">
p(z_n | z_{1:n-1}) = \int \int p(z_n | x_n) p(x_n | x_{n-1}) p(x_{n-1} | z_{1:n-1}) \mathrm{d} x_{n} \mathrm{d} x_{n-1}
</span></p>
<p>When using EKF, this distribution is a multivariate Gaussian whose moments can be analytically obtained from the filtering mean and variance and the parameters of the linear model, i.e.&nbsp;</p>
<p><span class="math display">
\mathbb{E} \left[Z_n^\ell | Z_{1:n-1}^\ell \right] = B_k (A_k \mu_{k-1} + a_k) + b_k
</span></p>
<p>and</p>
<p><span class="math display">
\mathbb{V} \left[Z_n^\ell | Z_{1:n-1}^\ell \right] = B_k (A_k \Sigma_k A_k^T + Q_k) B_k^T + R_k  
</span></p>
<p>following the previously introduced notation. Note that given the values of <span class="math inline">A_k, B_k, a_k, b_k</span> in our model these equations are simplified in practice, e.g.&nbsp;<span class="math inline">B_k = I, b_k = 0</span> and <span class="math inline">A_k \mu_{k-1} + a_k = \mu_{k-1} + \Delta_k(\lfloor \mu_{k-1} \rfloor)</span>.</p>
<p>In <span class="math inline">\mathbb{R}^2</span>, values of the cumulative distribution function (cdf) of a multivariate Gaussian distribution are easy to compute. Denote with <span class="math inline">F_n^\ell</span> the cdf of <span class="math inline">\mathbb{L}_n^\ell</span>. If <span class="math inline">V_\delta(z)</span> is a squared neighborhood of size <span class="math inline">\delta</span> and centered on <span class="math inline">z=(x,y) \in \mathbb{R}^2</span>, then, denoting with <span class="math inline">\mathbb{L}_n^\ell</span> the distribution of <span class="math inline">Z_n^\ell</span> given <span class="math inline">Z_{1:n-1}^\ell</span>:</p>
<p><span class="math display">
\mathbb{L}_n^{\ell}(V_\delta(z)) = F_n^\ell(x+\delta,y+\delta) + F_n^\ell(x-\delta,y-\delta) - \left[F_n^\ell(x+\delta,y-\delta) + F_n^\ell(x-\delta,y+\delta)\right]
</span></p>
<p>This allows easy computation of <span class="math inline">P(i,\ell) = \mathbb{L}_n^\ell(V_\delta(z_n^i))</span>.</p>
<section id="sec-impact-algorithm-appendix" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1" class="anchored" data-anchor-id="sec-impact-algorithm-appendix"><span class="header-section-number">8.5.1</span> Impact of the filtering algorithm</h3>
<p>An advantage of the data association method proposed in <a href="#sec-data_association" class="quarto-xref">Section&nbsp;4.4</a> is that it is very generic and does not constrain the tracking solution to any particular choice of filtering algorithm. As for EKF, UKF implementations are already available to compute the distribution of <span class="math inline">Z_k</span> given <span class="math inline">Z_{1:k-1}</span> and the corresponding confidence regions (see <a href="#sec-tracking_module_appendix" class="quarto-xref">Section&nbsp;8.3</a> above). We propose a solution to compute this distribution when SMC is used, and performance comparisons between the EKF, UKF and SMC versions of our trackers are discussed.</p>
</section>
<section id="smc-based-tracking" class="level3" data-number="8.5.2">
<h3 data-number="8.5.2" class="anchored" data-anchor-id="smc-based-tracking"><span class="header-section-number">8.5.2</span> SMC-based tracking</h3>
<p>Denote <span class="math inline">\mathbb{Q}_k</span> the filtering distribution (ie. that of <span class="math inline">Z_k</span> given <span class="math inline">X_{1:k}</span>) for the HMM <span class="math inline">(X_k,Z_k)_{k \geq 1}</span> (omitting the dependency on the observations for notation ease). Using a set of samples <span class="math inline">\{X_k^i\}_{1 \leq i \leq N}</span> and importance weights <span class="math inline">\{w_k^i\}_{1 \leq i \leq N}</span>, SMC methods build an approximation of the following form:</p>
<p><span class="math display">
\widehat{\mathbb{Q}}^{SMC}_k(\mathrm{d} x_k) = \sum_{i=1}^N w_k^i \delta_{X_k^i}(\mathrm{d} x_k) \,.
</span> Contrary to EKF and UKF, the distribution <span class="math inline">\mathbb{L}_k</span> of <span class="math inline">Z_k</span> given <span class="math inline">Z_{1:k-1}</span> is not directly available but can be obtained via an additional Monte Carlo sampling step. Marginalizing over <span class="math inline">(X_{k-1}</span>, <span class="math inline">X_k)</span> and using the conditional independence properties of HMMs, we decompose <span class="math inline">\mathbb{L}_k</span> using the conditional state transition <span class="math inline">\mathbb{M}_k(x,\mathrm{d} x')</span> and the likelihood of <span class="math inline">Z_k</span> given <span class="math inline">X_k</span>, denoted by <span class="math inline">\mathbb{G}_k(x, \mathrm{d} z)</span>:</p>
<p><span class="math display">
\mathbb{L}_k(\mathrm{d} z_k) = \int \int \mathbb{G}_k(x_k, \mathrm{d} z_k)\mathbb{M}_k(x_{k-1}, \mathrm{d} x_k)\mathbb{Q}_{k-1}(\mathrm{d} x_{k-1}) \,.
</span></p>
<p>Replacing <span class="math inline">\mathbb{Q}_{k-1}</span> with <span class="math inline">\widehat{\mathbb{Q}}^{SMC}_{k-1}</span> into the previous equation yields</p>
<p><span class="math display">
\widehat{\mathbb{L}}^{SMC}_k(\mathrm{d} z_k) = \sum_{k=1}^N w_k^i \int \mathbb{G}_k(x_k,\mathrm{d} z_k) \mathbb{M}_k(X_{k-1}^i, \mathrm{d} x_k)  \,.
</span></p>
<p>In our model, the state transition is Gaussian and therefore easy to sample from. Thus an approximated predictive distribution <span class="math inline">\widehat{\mathbb{L}}_k</span> can be obtained using Monte Carlo estimates built from random samples <span class="math inline">\{X_k^{i,j}\}_{1 \leq i \leq N}^{1 \leq j \leq M}</span> drawn from <span class="math inline">\mathbb{M}_k(X_{k-1}^i, \mathrm{d} x_k)</span>. This leads to</p>
<p><span class="math display">
\widehat{\mathbb{L}}_k(\mathrm{d} z_k) = \sum_{i=1}^N \sum_{j=1}^M w_k^i \mathbb{G}_k(X_k^{i,j},\mathrm{d} z_k) \,.
</span></p>
<p>Since the observation likelihood is also Gaussian, <span class="math inline">\widehat{\mathbb{L}}_k</span> is a Gaussian mixture, thus values of <span class="math inline">\widehat{\mathbb{L}}_k(\mathsf{A})</span> for any <span class="math inline">\mathsf{A} \subset \mathbb{R}^2</span> can be computed by applying the tools from <a href="#sec-confidence_regions_appendix" class="quarto-xref">Section&nbsp;8.5</a> to all mixture components. Similar to EKF and UKF, this approximated predictive distribution is used to recover object identities via <span class="math inline">\widehat{\mathbb{L}}_n^{\ell}(V_\delta(z_n^i))</span> computed for all incoming detections <span class="math inline">\mathcal{D}_n = \{z_n^i\}_{1 \leq i \leq D_n}</span> and each of the <span class="math inline">1 \leq \ell \leq L_n</span> filters, where <span class="math inline">\widehat{\mathbb{L}}_n^{\ell}</span> is the predictive distribution associated with the <span class="math inline">\ell</span>-th filter.</p>
</section>
<section id="performance-comparison" class="level3" data-number="8.5.3">
<h3 data-number="8.5.3" class="anchored" data-anchor-id="performance-comparison"><span class="header-section-number">8.5.3</span> Performance comparison</h3>
<p>In theory, sampling-based methods like UKF and SMC are better suited for nonlinear state space models like the one we propose in <a href="#sec-state_space_model" class="quarto-xref">Section&nbsp;4.3.2</a>. However, we observe very few differences in count results when upgrading from EKF to UKF to SMC. In practise, there is no difference at all between our EKF and UKF implementations, which show strictly identical values for <span class="math inline">\mathsf{\hat{N}_{true}}</span>, <span class="math inline">\mathsf{\hat{N}_{false}}</span> and <span class="math inline">\mathsf{\hat{N}_{red}}</span>. For the SMC version, values for <span class="math inline">\mathsf{\hat{N}_{false}}</span> and <span class="math inline">\mathsf{\hat{N}_{red}}</span> improve by a very small amount (2 and 1, respectively), but <span class="math inline">\mathsf{\hat{N}_{mis}}</span> is slightly worse (one more object missed), and these results depend loosely on the number of samples used to approximate the filtering distributions and the number of samples for the Monte Carlo scheme. Therefore, our motion estimates via the optical flow <span class="math inline">\Delta_n</span> prove very reliable in our application context, so much that EKF, though suboptimal, brings equivalent results. This comforts us into keeping it as a faster and computationally simpler option. That said, this conclusion might not hold in scenarios where camera motion is even stronger, which was our main motivation to develop a flexible tracking solution and to provide implementations of UKF and SMC versions. This allows easier extension of our work to more challenging data.</p>
</section>
</section>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Arteta2016" class="csl-entry" role="listitem">
Arteta, Carlos, Victor Lempitsky, and Andrew Zisserman. 2016. <span>“Counting in the Wild”</span> 9911 (October): 483–98. <a href="https://doi.org/10.1007/978-3-319-46478-7_30">https://doi.org/10.1007/978-3-319-46478-7_30</a>.
</div>
<div id="ref-bergmann2019" class="csl-entry" role="listitem">
Bergmann, P., T. Meinhardt, and L. Leal-Taixe. 2019. <span>“Tracking Without Bells and Whistles.”</span> In <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 941–51. Los Alamitos, CA, USA: IEEE Computer Society. <a href="https://doi.org/10.1109/ICCV.2019.00103">https://doi.org/10.1109/ICCV.2019.00103</a>.
</div>
<div id="ref-bernardin2008" class="csl-entry" role="listitem">
Bernardin, Keni, and Rainer Stiefelhagen. 2008. <span>“Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics.”</span> <em>EURASIP Journal on Image and Video Processing</em> 2008 (January). <a href="https://doi.org/10.1155/2008/246309">https://doi.org/10.1155/2008/246309</a>.
</div>
<div id="ref-Bewley2016" class="csl-entry" role="listitem">
Bewley, Alex, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. 2016. <span>“<span class="nocase">Simple online and realtime tracking</span>.”</span> In <em>Proceedings - International Conference on Image Processing, ICIP</em>, 2016-Augus:3464–68. <a href="https://doi.org/10.1109/ICIP.2016.7533003">https://doi.org/10.1109/ICIP.2016.7533003</a>.
</div>
<div id="ref-Bruge2018" class="csl-entry" role="listitem">
Bruge, Antoine, Cristina Barreau, Jérémy Carlot, Hélène Collin, Clément Moreno, and Philippe Maison. 2018. <span>“<span class="nocase">Monitoring litter inputs from the Adour river (southwest France) to the marine environment</span>.”</span> <em>Journal of Marine Science and Engineering</em> 6 (1). <a href="https://doi.org/10.3390/jmse6010024">https://doi.org/10.3390/jmse6010024</a>.
</div>
<div id="ref-Caesar2020" class="csl-entry" role="listitem">
Caesar, Holger, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. 2020. <span>“<span class="nocase">Nuscenes: A multimodal dataset for autonomous driving</span>.”</span> In <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, 11618–28. <a href="https://doi.org/10.1109/CVPR42600.2020.01164">https://doi.org/10.1109/CVPR42600.2020.01164</a>.
</div>
<div id="ref-Chattopadhyay" class="csl-entry" role="listitem">
Chattopadhyay, Prithvijit, Ramakrishna Vedantam, Ramprasaath R Selvaraju, Dhruv Batra, and Devi Parikh. 2017. <span>“<span class="nocase">Counting everyday objects in everyday scenes</span>.”</span> In <em>Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</em>, 2017-Janua:4428–37. <a href="https://doi.org/10.1109/CVPR.2017.471">https://doi.org/10.1109/CVPR.2017.471</a>.
</div>
<div id="ref-Chu2021" class="csl-entry" role="listitem">
Chu, Peng, Jiang Wang, Quanzeng You, Haibin Ling, and Zicheng Liu. 2021. <span>“<span class="nocase">TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking</span>.”</span> <a href="http://arxiv.org/abs/2104.00194">http://arxiv.org/abs/2104.00194</a>.
</div>
<div id="ref-Ciaparrone2020b" class="csl-entry" role="listitem">
Ciaparrone, Gioele, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and Francisco Herrera. 2020. <span>“<span class="nocase">Deep learning in video multi-object tracking: A survey</span>.”</span> <em>Neurocomputing</em> 381: 61–88. <a href="https://doi.org/10.1016/j.neucom.2019.11.023">https://doi.org/10.1016/j.neucom.2019.11.023</a>.
</div>
<div id="ref-Dendorfer2020" class="csl-entry" role="listitem">
Dendorfer, Patrick, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taixé. 2020. <span>“<span class="nocase">MOT20: A benchmark for multi object tracking in crowded scenes</span>.”</span> <a href="https://arxiv.org/abs/2003.09003">https://arxiv.org/abs/2003.09003</a>.
</div>
<div id="ref-VanEmmerik2020" class="csl-entry" role="listitem">
Emmerik, Tim van, and Anna Schwarz. 2020. <span>“Plastic Debris in Rivers.”</span> <em>WIREs Water</em> 7 (1): e1398. https://doi.org/<a href="https://doi.org/10.1002/wat2.1398">https://doi.org/10.1002/wat2.1398</a>.
</div>
<div id="ref-VanEmmerik2019" class="csl-entry" role="listitem">
Emmerik, Tim van, Romain Tramoy, Caroline van Calcar, Soline Alligant, Robin Treilles, Bruno Tassin, and Johnny Gasperi. 2019. <span>“<span>Seine Plastic Debris Transport Tenfolded During Increased River Discharge</span>.”</span> <em>Frontiers in Marine Science</em> 6 (October): 1–7. <a href="https://doi.org/10.3389/fmars.2019.00642">https://doi.org/10.3389/fmars.2019.00642</a>.
</div>
<div id="ref-farneback2003two" class="csl-entry" role="listitem">
Farnebäck, G. 2003. <span>“Two-Frame Motion Estimation Based on Polynomial Expansion.”</span> In <em>Scandinavian Conference on Image Analysis</em>, 363–70. Springer.
</div>
<div id="ref-Fulton2018" class="csl-entry" role="listitem">
Fulton, Michael, Jungseok Hong, Md Jahidul Islam, and Junaed Sattar. 2019. <span>“Robotic Detection of Marine Litter Using Deep Visual Detection Models.”</span> In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 5752–58. IEEE.
</div>
<div id="ref-gamage2020" class="csl-entry" role="listitem">
Gamage, Thushari, and J. D. M. Senevirathna. 2020. <span>“Plastic Pollution in the Marine Environment.”</span> <em>Heliyon</em> 6 (August): e04709. <a href="https://doi.org/10.1016/j.heliyon.2020.e04709">https://doi.org/10.1016/j.heliyon.2020.e04709</a>.
</div>
<div id="ref-garcia2020" class="csl-entry" role="listitem">
García-Fernández, Ángel F., Abu Sajana Rahmathullah, and Lennart Svensson. 2020. <span>“A Metric on the Space of Finite Sets of Trajectories for Evaluation of Multi-Target Tracking Algorithms.”</span> <em>IEEE Transactions on Signal Processing</em> 68: 3917–28. <a href="https://doi.org/10.1109/TSP.2020.3005309">https://doi.org/10.1109/TSP.2020.3005309</a>.
</div>
<div id="ref-gasperi2014" class="csl-entry" role="listitem">
Gasperi, Johnny, Rachid Dris, Tiffany Bonin, Vincent Rocher, and Bruno Tassin. 2014. <span>“Assessment of Floating Plastic Debris in Surface Water Along the Seine River.”</span> <em>Environmental Pollution</em> 195: 163–66. https://doi.org/<a href="https://doi.org/10.1016/j.envpol.2014.09.001">https://doi.org/10.1016/j.envpol.2014.09.001</a>.
</div>
<div id="ref-geyer2017" class="csl-entry" role="listitem">
Geyer, Roland, Jenna Jambeck, and Kara Law. 2017. <span>“Production, Use, and Fate of All Plastics Ever Made.”</span> <em>Science Advances</em> 3 (July): e1700782. <a href="https://doi.org/10.1126/sciadv.1700782">https://doi.org/10.1126/sciadv.1700782</a>.
</div>
<div id="ref-gonzales2021" class="csl-entry" role="listitem">
González-Fernández, D., A. Cózar, G. Hanke, J. Viejo, C. Morales-Caselles, R. Bakiu, D. Barcelo, et al. 2021. <span>“Floating Macrolitter Leaked from Europe into the Ocean.”</span> <em>Nature Sustainability</em> 4: 474–83.
</div>
<div id="ref-jambeck2015" class="csl-entry" role="listitem">
Jambeck, Jenna, Roland Geyer, Chris Wilcox, Theodore Siegler, Miriam Perryman, Anthony Andrady, Ramani Narayan, and Kara Law. 2015. <span>“Marine Pollution. Plastic Waste Inputs from Land into the Ocean.”</span> <em>Science (New York, N.Y.)</em> 347 (February): 768–71. <a href="https://doi.org/10.1126/science.1260352">https://doi.org/10.1126/science.1260352</a>.
</div>
<div id="ref-kuhn" class="csl-entry" role="listitem">
Kuhn, H. W. 1955. <span>“The Hungarian Method for the Assignment Problem.”</span> <em>Naval Research Logistics Quarterly</em> 2 (1-2): 83–97. https://doi.org/<a href="https://doi.org/10.1002/nav.3800020109">https://doi.org/10.1002/nav.3800020109</a>.
</div>
<div id="ref-Law" class="csl-entry" role="listitem">
Law, Hei, and Jia Deng. 2018. <span>“CornerNet: Detecting Objects as Paired Keypoints.”</span> In <em>Computer Vision - <span>ECCV</span> 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part <span>XIV</span></em>, edited by Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, 11218:765–81. Lecture Notes in Computer Science. Springer. <a href="https://doi.org/10.1007/978-3-030-01264-9\_45">https://doi.org/10.1007/978-3-030-01264-9\_45</a>.
</div>
<div id="ref-vanlieshout2020automated" class="csl-entry" role="listitem">
Lieshout, Colin van, Kees van Oeveren, Tim van Emmerik, and Eric Postma. 2020. <span>“<span class="nocase">Automated River Plastic Monitoring Using Deep Learning and Cameras</span>.”</span> <em>Earth and Space Science</em> 7 (8): e2019EA000960. <a href="https://doi.org/10.1029/2019EA000960">https://doi.org/10.1029/2019EA000960</a>.
</div>
<div id="ref-luiten2020" class="csl-entry" role="listitem">
Luiten, Jonathon, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixé, and Bastian Leibe. 2021. <span>“Hota: A Higher Order Metric for Evaluating Multi-Object Tracking.”</span> <em>International Journal of Computer Vision</em> 129 (2): 548–78.
</div>
<div id="ref-luo2021" class="csl-entry" role="listitem">
Luo, Wenhan, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, and Tae-Kyun Kim. 2021. <span>“Multiple Object Tracking: A Literature Review.”</span> <em>Artificial Intelligence</em> 293: 103448. https://doi.org/<a href="https://doi.org/10.1016/j.artint.2020.103448">https://doi.org/10.1016/j.artint.2020.103448</a>.
</div>
<div id="ref-mahler2003" class="csl-entry" role="listitem">
Mahler, R. P. S. 2003. <span>“Multitarget Bayes Filtering via First-Order Multitarget Moments.”</span> <em>IEEE Transactions on Aerospace and Electronic Systems</em> 39 (4): 1152–78. <a href="https://doi.org/10.1109/TAES.2003.1261119">https://doi.org/10.1109/TAES.2003.1261119</a>.
</div>
<div id="ref-Miao2019" class="csl-entry" role="listitem">
Miao, Yunqi, Jungong Han, Yongsheng Gao, and Baochang Zhang. 2019. <span>“<span class="nocase">ST-CNN: Spatial-Temporal Convolutional Neural Network for crowd counting in videos</span>.”</span> <em>Pattern Recognition Letters</em> 125 (July): 113–18. <a href="https://doi.org/10.1016/j.patrec.2019.04.012">https://doi.org/10.1016/j.patrec.2019.04.012</a>.
</div>
<div id="ref-moritt2014" class="csl-entry" role="listitem">
Morritt, David, Paris V. Stefanoudis, Dave Pearce, Oliver A. Crimmen, and Paul F. Clark. 2014. <span>“Plastic in the Thames: A River Runs Through It.”</span> <em>Marine Pollution Bulletin</em> 78 (1): 196–200. https://doi.org/<a href="https://doi.org/10.1016/j.marpolbul.2013.10.035">https://doi.org/10.1016/j.marpolbul.2013.10.035</a>.
</div>
<div id="ref-paragios2006" class="csl-entry" role="listitem">
Paragios, Nikos, Yunmei Chen, and Olivier D Faugeras. 2006. <em>Handbook of Mathematical Models in Computer Vision</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-Proenca2020" class="csl-entry" role="listitem">
Proença, Pedro F, and Pedro Simões. 2020. <span>“<span class="nocase">TACO: Trash Annotations in Context for Litter Detection</span>.”</span> <a href="http://tacodataset.org/ http://arxiv.org/abs/2003.06975">http://tacodataset.org/ http://arxiv.org/abs/2003.06975</a>.
</div>
<div id="ref-ren2016faster" class="csl-entry" role="listitem">
Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. 2015. <span>“Faster r-CNN: Towards Real-Time Object Detection with Region Proposal Networks.”</span> In <em>Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1</em>, 91–99. NIPS’15. Cambridge, MA, USA: MIT Press.
</div>
<div id="ref-RistaniSZCT16" class="csl-entry" role="listitem">
Ristani, Ergys, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. 2016. <span>“Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking.”</span> In <em>European Conference on Computer Vision</em>, 17–35. Springer.
</div>
<div id="ref-rochman2016" class="csl-entry" role="listitem">
Rochman, Chelsea, Anthony Andrady, Sarah Dudas, Joan Fabres, François Galgani, Denise lead, Valeria Hidalgo-Ruz, et al. 2016. <span>“Sources, Fate and Effects of Microplastics in the Marine Environment: Part 2 of a Global Assessment,”</span> December.
</div>
<div id="ref-sarkka2013bayesian" class="csl-entry" role="listitem">
Särkkä, S. 2013. <em>Bayesian Filtering and Smoothing</em>. New York, NY, USA: Cambridge University Press.
</div>
<div id="ref-Wang2021" class="csl-entry" role="listitem">
Wang, Yongxin, Kris Kitani, and Xinshuo Weng. 2021. <span>“Joint Object Detection and Multi-Object Tracking with Graph Neural Networks,”</span> May.
</div>
<div id="ref-welden2020" class="csl-entry" role="listitem">
Welden, Natalie. 2020. <span>“The Environmental Impacts of Plastic Pollution,”</span> January, 195–222. <a href="https://doi.org/10.1016/B978-0-12-817880-5.00008-6">https://doi.org/10.1016/B978-0-12-817880-5.00008-6</a>.
</div>
<div id="ref-Wojke2018" class="csl-entry" role="listitem">
Wojke, Nicolai, Alex Bewley, and Dietrich Paulus. 2018. <span>“<span class="nocase">Simple online and realtime tracking with a deep association metric</span>.”</span> In <em>Proceedings - International Conference on Image Processing, ICIP</em>, 2017-Septe:3645–49. <a href="https://doi.org/10.1109/ICIP.2017.8296962">https://doi.org/10.1109/ICIP.2017.8296962</a>.
</div>
<div id="ref-Wolf2020" class="csl-entry" role="listitem">
Wolf, Mattis, Katelijn van den Berg, Shungudzemwoyo Pascal Garaba, Nina Gnann, Klaus Sattler, Frederic Theodor Stahl, and Oliver Zielinski. 2020. <span>“<span class="nocase">Machine learning for aquatic plastic litter detection, classification and quantification (APLASTIC–Q)</span>.”</span> <em>Environmental Research Letters</em>. <a href="https://doi.org/10.1088/1748-9326/abbd01">https://doi.org/10.1088/1748-9326/abbd01</a>.
</div>
<div id="ref-wu2020fast" class="csl-entry" role="listitem">
Wu, Xingjiao, Baohan Xu, Yingbin Zheng, Hao Ye, Jing Yang, and Liang He. 2020. <span>“Fast Video Crowd Counting with a Temporal Aware Network.”</span> <em>Neurocomputing</em> 403: 13–20. https://doi.org/<a href="https://doi.org/10.1016/j.neucom.2020.04.071">https://doi.org/10.1016/j.neucom.2020.04.071</a>.
</div>
<div id="ref-Xiong2017" class="csl-entry" role="listitem">
Xiong, Feng, Xingjian Shi, and Dit-Yan Yeung. 2017. <span>“Spatiotemporal Modeling for Crowd Counting in Videos.”</span> In <em>2017 IEEE International Conference on Computer Vision (ICCV)</em>, 5161–69. <a href="https://doi.org/10.1109/ICCV.2017.551">https://doi.org/10.1109/ICCV.2017.551</a>.
</div>
<div id="ref-fisher2017" class="csl-entry" role="listitem">
Yu, Fisher, Dequan Wang, Evan Shelhamer, and Trevor Darrell. 2018. <span>“Deep Layer Aggregation.”</span> In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2403–12. <a href="https://doi.org/10.1109/CVPR.2018.00255">https://doi.org/10.1109/CVPR.2018.00255</a>.
</div>
<div id="ref-Zhanga" class="csl-entry" role="listitem">
Zhang, Yifu, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. 2021. <span>“Fairmot: On the Fairness of Detection and Re-Identification in Multiple Object Tracking.”</span> <em>International Journal of Computer Vision</em>, 1–19.
</div>
<div id="ref-zhou2020" class="csl-entry" role="listitem">
Zhou, Xingyi, Vladlen Koltun, and Philipp Krähenbühl. 2020. <span>“Tracking Objects as Points,”</span> October, 474–90. <a href="https://doi.org/10.1007/978-3-030-58548-8_28">https://doi.org/10.1007/978-3-030-58548-8_28</a>.
</div>
<div id="ref-Zhou2019" class="csl-entry" role="listitem">
Zhou, Xingyi, Dequan Wang, and Philipp Krähenbühl. 2019. <span>“<span class="nocase">Objects as points</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1904.07850">http://arxiv.org/abs/1904.07850</a>.
</div>
</div>
<!-- -->

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{chagneux2023,
  author = {Chagneux, Mathis and Le Corff, Sylvain and Gloaguen, Pierre
    and Ollion, Charles and Lepâtre, Océane and Bruge, Antoine},
  publisher = {French Statistical Society},
  title = {Macrolitter Video Counting on Riverbanks Using State Space
    Models and Moving Cameras},
  journal = {Computo},
  date = {2023-02-16},
  url = {https://computo.sfds.asso.fr/published-202301-chagneux-macrolitter/},
  doi = {10.57750/845m-f805},
  issn = {2824-7795},
  langid = {en},
  abstract = {Litter is a known cause of degradation in marine
    environments and most of it travels in rivers before reaching the
    oceans. In this paper, we present a novel algorithm to assist waste
    monitoring along watercourses. While several attempts have been made
    to quantify litter using neural object detection in photographs of
    floating items, we tackle the more challenging task of counting
    directly in videos using boat-embedded cameras. We rely on
    multi-object tracking (MOT) but focus on the key pitfalls of false
    and redundant counts which arise in typical scenarios of poor
    detection performance. Our system only requires supervision at the
    image level and performs Bayesian filtering via a state space model
    based on optical flow. We present a new open image dataset gathered
    through a crowdsourced campaign and used to train a center-based
    anchor-free object detector. Realistic video footage assembled by
    water monitoring experts is annotated and provided for evaluation.
    Improvements in count quality are demonstrated against systems built
    from state-of-the-art multi-object trackers sharing the same
    detection capabilities. A precise error decomposition allows clear
    analysis and highlights the remaining challenges.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-chagneux2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Chagneux, Mathis, Sylvain Le Corff, Pierre Gloaguen, Charles Ollion,
Océane Lepâtre, and Antoine Bruge. 2023. <span>“Macrolitter Video
Counting on Riverbanks Using State Space Models and Moving Cameras
.”</span> <em>Computo</em>, February. <a href="https://doi.org/10.57750/845m-f805">https://doi.org/10.57750/845m-f805</a>.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb16" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Macrolitter video counting on riverbanks using state space models and moving cameras "</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> ""</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: "Mathis Chagneux"</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">    corresponding: true</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">    email: mathis.chagneux@telecom-paris.fr</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://www.linkedin.com/in/mathis-chagneux-140245158/?originalSubdomain=fr</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: Telecom Paris, LTCI</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation-url: https://www.telecom-paris.fr/fr/recherche/laboratoires/laboratoire-traitement-et-communication-de-linformation-ltci</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: "Sylvain Le Corff"</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">    email: sylvain.le_corff@sorbonne-universite.fr</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://sylvainlc.github.io/</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0001-5211-2328</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: Sorbonne Université, UMR 8001 (LPSM)</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation-url: https://www.lpsm.paris/</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: "Pierre Gloaguen"</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co">    email: pierre.gloaguen@agroparistech.fr</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://papayoun.github.io/</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0003-2239-5413</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: AgroParisTech, UMR MIA 518</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation-url: https://mia-ps.inrae.fr/</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: "Charles Ollion"</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co">    email: charles.ollion@gmail.com</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://charlesollion.github.io/</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-6763-701X</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: Naia Science</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: "Océane Lepâtre"</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co">    email: olepatre@surfrider.eu</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://fr.linkedin.com/in/oc%C3%A9ane-lep%C3%A2tre-675b38116</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: None</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: Surfrider Foundation Europe</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation-url: https://surfrider.eu/</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: "Antoine Bruge"</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a><span class="co">    email: antoine.bruge@outlook.com</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://www.linkedin.com/in/antoinebruge/</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-0548-234X</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: Surfrider Foundation Europe</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation-url: https://surfrider.eu/ </span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2023-02-16</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> &gt;+</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a><span class="co">    Litter is a known cause of degradation in marine environments and most of</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a><span class="co">    it travels in rivers before reaching the oceans. In this paper, we present</span></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a><span class="co">    a novel algorithm to assist waste monitoring along watercourses. While</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a><span class="co">    several attempts have been made to quantify litter using neural object</span></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a><span class="co">    detection in photographs of floating items, we tackle the more challenging</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a><span class="co">    task of counting directly in videos using boat-embedded cameras. We rely</span></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a><span class="co">    on multi-object tracking (MOT) but focus on the key pitfalls of false and</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a><span class="co">    redundant counts which arise in typical scenarios of poor detection</span></span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a><span class="co">    performance. Our system only requires supervision at the image level and</span></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a><span class="co">    performs Bayesian filtering via a state space model based on optical flow.</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a><span class="co">    We present a new open image dataset gathered through a crowdsourced</span></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a><span class="co">    campaign and used to train a center-based anchor-free object detector.</span></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a><span class="co">    Realistic video footage assembled by water monitoring experts is annotated</span></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a><span class="co">    and provided for evaluation. Improvements in count quality are</span></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a><span class="co">    demonstrated against systems built from state-of-the-art multi-object</span></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a><span class="co">    trackers sharing the same detection capabilities. A precise error</span></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a><span class="co">    decomposition allows clear analysis and highlights the remaining</span></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a><span class="co">    challenges.</span></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span></span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a><span class="co">  type: article-journal</span></span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a><span class="co">  container-title: "Computo"</span></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a><span class="co">  doi: "10.57750/845m-f805"</span></span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a><span class="co">  publisher: "French Statistical Society"</span></span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="co">  issn: "2824-7795"</span></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a><span class="co">  pdf-url: "https://computo.sfds.asso.fr/published-202301-chagneux-macrolitter/published-202301-chagneux-macrolitter.pdf"</span></span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a><span class="co">  url: "https://computo.sfds.asso.fr/published-202301-chagneux-macrolitter/"</span></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a><span class="an">google-scholar:</span><span class="co"> true</span></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a><span class="an">github-user:</span><span class="co"> computorg</span></span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a><span class="an">repo:</span><span class="co"> "published-202301-chagneux-macrolitter"</span></span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a><span class="an">published:</span><span class="co"> true</span></span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-html: default</span></span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-pdf: default</span></span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>Litter pollution concerns every part of the globe. Each year, almost ten</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>thousand million tons of plastic waste is generated, among which 80\% ends up</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>in landfills or in nature (@geyer2017), notably threatening all of the</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>world’s oceans, seas and aquatic environments (@welden2020, @gamage2020).</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>Plastic pollution is known to already impact more than 3763 marine species</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>worldwide (see <span class="co">[</span><span class="ot">this</span><span class="co">](https://litterbase.awi.de/)</span> detailed analysis) with risk</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>of proliferation through the whole food chain. This accumulation of waste is</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>the endpoint of the largely misunderstood path of trash, mainly coming from</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>land-based sources (@rochman2016), yet rivers have been identified as a</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>major pathway for the introduction of waste into marine environments</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>(@jambeck2015). Therefore, field data on rivers and monitoring are</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>strongly needed to assess the impact of measures that can be taken. The</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>analysis of such field data over time is pivotal to understand the efficiency</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>of the actions implemented such as choosing zero-waste alternatives to</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>plastic, designing new products to be long-lasting or reusable, introducing</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>policies to reduce over-packing.</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>Different methods have already been tested to monitor waste in rivers: litter</span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>collection and sorting on riverbanks (@Bruge2018), visual counting of</span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>drifting litter from bridges (@gonzales2021), floating booms</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>(@gasperi2014), and nets (@moritt2014). All are helpful to understand</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a>the origin and typology of litter pollution yet hardly compatible with long</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a>term monitoring at country scales. Monitoring tools need to be reliable, easy</span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>to set up on various types of rivers, and should give an overview of plastic</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>pollution during peak discharge to help locate hotspots and provide trends.</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a>Newer studies suggest that plastic debris transport could be better understood</span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>by counting litter trapped on river banks, providing a good indication of the</span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a>local macrolitter pollution especially after increased river discharge</span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a>(@VanEmmerik2019, @VanEmmerik2020). Based on these findings, we propose a</span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a>new method for litter monitoring which relies on videos of river banks</span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a>directly captured from moving boats.</span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a>In this case, object detection with deep neural networks (DNNs) may be used,</span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a>but new challenges arise. First, available data is still scarce. When</span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a>considering entire portions of river banks from many different locations, the</span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a>variety of scenes, viewing angles and/or light conditions is not well covered</span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a>by existing plastic litter datasets like (@Proenca2020), where litter is</span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>usually captured from relatively close distances and many times in urban or</span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a>domestic backgrounds. Therefore, achieving robust object detection across</span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a>multiple conditions is still delicate.</span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a>Second, counting from videos is a different task than counting from</span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a>independent images, because individual objects will typically appear in</span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a>several consecutive frames, yet they must only be counted once. This last</span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a>problem of association has been extensively studied for the multi-object</span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a>tracking (MOT) task, which aims at recovering individual trajectories for</span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a>objects in videos. When successful MOT is achieved, counting objects in videos</span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a>is equivalent to counting the number of estimated trajectories. Deep learning</span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a>has been increasingly used to improve MOT solutions (@Ciaparrone2020b).</span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a>However, newer state-of-the-art techniques require increasingly heavy and</span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a>costly supervision, typically all object positions provided at every frame. In</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a>addition, many successful techniques (@bergmann2019) can hardly be used</span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a>in scenarios with abrupt and nonlinear camera motion. Finally, while research</span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a>is still active to rigorously evaluate performance at multi-object *tracking*</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a>(@luiten2020), most but not all aspects of the latter may affect global</span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a>video counts, which calls for a separate evaluation protocol dedicated to</span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a>multi-object *counting*.</span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a>Our contribution can be summarized as follows.</span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We provide a novel open-source image dataset of macro litter, which includes various objects seen from different rivers and different contexts.</span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a>This dataset was produced with a new open-sourced platform for data gathering and annotation developed in conjunction with Surfrider Foundation Europe, continuously growing with more data.</span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We propose a new algorithm specifically tailored to count in videos with fast camera movements.</span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a>In a nutshell, DNN-based object detection is paired with a robust state space movement model which uses optical flow to perform Bayesian filtering, while confidence regions built on posterior predictive distributions are used for data association.</span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a>This framework does not require video annotations at training time: the multi-object tracking module does not require supervision, only the DNN-based object detection does require annotated images.</span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a>It also fully leverages optical flow estimates and the uncertainty provided by Bayesian predictions to recover object identities even when detection recall is low.</span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>Contrary to existing MOT solutions, this method ensures that tracks are stable enough to avoid repeated counting of the same object.</span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We provide a set of video sequences where litter counts are known and depicted in real conditions.</span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a>For these videos only, litter positions are manually annotated at every frame in order to carefully analyze performance.</span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a>This allows us to build new informative count metrics.</span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a>We compare the count performance of our method against other MOT-based alternatives.</span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a>A first visual illustration of the second claim is presented via the following code chunks: on three selected frames, we present a typical scenario where our strategy can avoid overcounting the same object (we depict internal workings of our solution against the end result of the competitors).</span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo</span></span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "*Our method*: one object (red dot) is correctly detected at every frame and given a consistent identity throughout the sequence with low location uncertainty (red ellipse). Next to it, a false positive detection is generated at the first frame (brown dot) but immediatly lost in the following frames: the associated uncertainty grows fast (brown ellipse). In our solution, this type of track will not be counted. A third correctly detected object (pink) appears in the third frame and begins a new track."</span></span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.prepare_data <span class="im">import</span> download_data</span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.track <span class="im">import</span> default_args <span class="im">as</span> args</span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {<span class="st">'legend.fontsize'</span>: <span class="st">'xx-large'</span>,</span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a>         <span class="st">'axes.labelsize'</span>: <span class="st">'xx-large'</span>,</span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a>         <span class="st">'axes.titlesize'</span>:<span class="st">'xx-large'</span>,</span>
<span id="cb16-181"><a href="#cb16-181" aria-hidden="true" tabindex="-1"></a>         <span class="st">'xtick.labelsize'</span>:<span class="st">'xx-large'</span>,</span>
<span id="cb16-182"><a href="#cb16-182" aria-hidden="true" tabindex="-1"></a>         <span class="st">'ytick.labelsize'</span>:<span class="st">'xx-large'</span>}</span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a>plt.rcParams.update(params)</span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a><span class="co"># download frames and detections from a given deep detector model</span></span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a>download_data()</span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare arguments</span></span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a>args.external_detections <span class="op">=</span> <span class="va">True</span></span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a>args.data_dir <span class="op">=</span> <span class="st">'data/external_detections/part_1_segment_0'</span></span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a>args.output_dir <span class="op">=</span> <span class="st">'surfnet/results'</span></span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a>args.noise_covariances_path <span class="op">=</span> <span class="st">'surfnet/data/tracking_parameters'</span></span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a>args.confidence_threshold <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb16-194"><a href="#cb16-194" aria-hidden="true" tabindex="-1"></a>args.algorithm <span class="op">=</span> <span class="st">'EKF'</span></span>
<span id="cb16-195"><a href="#cb16-195" aria-hidden="true" tabindex="-1"></a>args.ratio <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb16-196"><a href="#cb16-196" aria-hidden="true" tabindex="-1"></a>args.display <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-197"><a href="#cb16-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-198"><a href="#cb16-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-199"><a href="#cb16-199" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.tracking.utils <span class="im">import</span> resize_external_detections, write_tracking_results_to_file</span>
<span id="cb16-200"><a href="#cb16-200" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.tools.video_readers <span class="im">import</span> FramesWithInfo</span>
<span id="cb16-201"><a href="#cb16-201" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.tracking.trackers <span class="im">import</span> get_tracker</span>
<span id="cb16-202"><a href="#cb16-202" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.track <span class="im">import</span> track_video</span>
<span id="cb16-203"><a href="#cb16-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-204"><a href="#cb16-204" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize variances</span></span>
<span id="cb16-205"><a href="#cb16-205" aria-hidden="true" tabindex="-1"></a>transition_variance <span class="op">=</span> np.load(os.path.join(args.noise_covariances_path, <span class="st">'transition_variance.npy'</span>))</span>
<span id="cb16-206"><a href="#cb16-206" aria-hidden="true" tabindex="-1"></a>observation_variance <span class="op">=</span> np.load(os.path.join(args.noise_covariances_path, <span class="st">'observation_variance.npy'</span>))</span>
<span id="cb16-207"><a href="#cb16-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-208"><a href="#cb16-208" aria-hidden="true" tabindex="-1"></a><span class="co"># Get tracker algorithm</span></span>
<span id="cb16-209"><a href="#cb16-209" aria-hidden="true" tabindex="-1"></a>engine <span class="op">=</span> get_tracker(args.algorithm)</span>
<span id="cb16-210"><a href="#cb16-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-211"><a href="#cb16-211" aria-hidden="true" tabindex="-1"></a><span class="co"># Open data: detections and frames</span></span>
<span id="cb16-212"><a href="#cb16-212" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(os.path.join(args.data_dir, <span class="st">'saved_detections.pickle'</span>),<span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb16-213"><a href="#cb16-213" aria-hidden="true" tabindex="-1"></a>    detections <span class="op">=</span> pickle.load(f)</span>
<span id="cb16-214"><a href="#cb16-214" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(os.path.join(args.data_dir, <span class="st">'saved_frames.pickle'</span>),<span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb16-215"><a href="#cb16-215" aria-hidden="true" tabindex="-1"></a>    frames <span class="op">=</span> pickle.load(f)</span>
<span id="cb16-216"><a href="#cb16-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-217"><a href="#cb16-217" aria-hidden="true" tabindex="-1"></a><span class="co"># Create frame reader and resize detections</span></span>
<span id="cb16-218"><a href="#cb16-218" aria-hidden="true" tabindex="-1"></a>reader <span class="op">=</span> FramesWithInfo(frames)</span>
<span id="cb16-219"><a href="#cb16-219" aria-hidden="true" tabindex="-1"></a>detections <span class="op">=</span> resize_external_detections(detections, args.ratio)</span>
<span id="cb16-220"><a href="#cb16-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-221"><a href="#cb16-221" aria-hidden="true" tabindex="-1"></a><span class="co"># Start tracking, storing intermediate tracklets</span></span>
<span id="cb16-222"><a href="#cb16-222" aria-hidden="true" tabindex="-1"></a>results, frame_to_trackers <span class="op">=</span> track_video(reader, detections, args, engine, </span>
<span id="cb16-223"><a href="#cb16-223" aria-hidden="true" tabindex="-1"></a>                                         transition_variance, observation_variance, return_trackers<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-224"><a href="#cb16-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-225"><a href="#cb16-225" aria-hidden="true" tabindex="-1"></a><span class="co"># Write final results</span></span>
<span id="cb16-226"><a href="#cb16-226" aria-hidden="true" tabindex="-1"></a>write_tracking_results_to_file(results, ratio_x<span class="op">=</span>args.ratio, ratio_y<span class="op">=</span>args.ratio, output_filename<span class="op">=</span>args.output_dir)</span>
<span id="cb16-227"><a href="#cb16-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-228"><a href="#cb16-228" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> surfnet.track <span class="im">import</span> build_image_trackers</span>
<span id="cb16-229"><a href="#cb16-229" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose a few indices to display (same for our algorithm and SORT)</span></span>
<span id="cb16-230"><a href="#cb16-230" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> [<span class="dv">108</span>, <span class="dv">112</span>, <span class="dv">117</span>]</span>
<span id="cb16-231"><a href="#cb16-231" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-232"><a href="#cb16-232" aria-hidden="true" tabindex="-1"></a>considered_frames <span class="op">=</span> [frames[i] <span class="cf">for</span> i <span class="kw">in</span> idxs]</span>
<span id="cb16-233"><a href="#cb16-233" aria-hidden="true" tabindex="-1"></a>considered_trackers <span class="op">=</span> [frame_to_trackers[i] <span class="cf">for</span> i <span class="kw">in</span> idxs]</span>
<span id="cb16-234"><a href="#cb16-234" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> build_image_trackers(considered_frames, considered_trackers, args, reader)</span>
<span id="cb16-235"><a href="#cb16-235" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-236"><a href="#cb16-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-237"><a href="#cb16-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-240"><a href="#cb16-240" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-241"><a href="#cb16-241" aria-hidden="true" tabindex="-1"></a><span class="co">## Tracker with SORT </span></span>
<span id="cb16-242"><a href="#cb16-242" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb16-243"><a href="#cb16-243" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb16-244"><a href="#cb16-244" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sort.sort <span class="im">import</span> track <span class="im">as</span> sort_tracker</span>
<span id="cb16-245"><a href="#cb16-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-246"><a href="#cb16-246" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Tracking with SORT...'</span>)</span>
<span id="cb16-247"><a href="#cb16-247" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'--- Begin SORT internal logs'</span>)</span>
<span id="cb16-248"><a href="#cb16-248" aria-hidden="true" tabindex="-1"></a>sort_tracker(detections_dir<span class="op">=</span><span class="st">'data/external_detections'</span>, output_dir<span class="op">=</span><span class="st">'sort/results'</span>)</span>
<span id="cb16-249"><a href="#cb16-249" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'--- End'</span>)</span>
<span id="cb16-250"><a href="#cb16-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-251"><a href="#cb16-251" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> read_sort_output(filename):</span>
<span id="cb16-252"><a href="#cb16-252" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Reads the output .txt of Sort (or other tracking algorithm)</span></span>
<span id="cb16-253"><a href="#cb16-253" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-254"><a href="#cb16-254" aria-hidden="true" tabindex="-1"></a>    dict_frames <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb16-255"><a href="#cb16-255" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename) <span class="im">as</span> f:</span>
<span id="cb16-256"><a href="#cb16-256" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb16-257"><a href="#cb16-257" aria-hidden="true" tabindex="-1"></a>            items <span class="op">=</span> line[:<span class="op">-</span><span class="dv">1</span>].split(<span class="st">","</span>)</span>
<span id="cb16-258"><a href="#cb16-258" aria-hidden="true" tabindex="-1"></a>            frame <span class="op">=</span> <span class="bu">int</span>(items[<span class="dv">0</span>])</span>
<span id="cb16-259"><a href="#cb16-259" aria-hidden="true" tabindex="-1"></a>            objnum <span class="op">=</span> <span class="bu">int</span>(items[<span class="dv">1</span>])</span>
<span id="cb16-260"><a href="#cb16-260" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="bu">float</span>(items[<span class="dv">2</span>])</span>
<span id="cb16-261"><a href="#cb16-261" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> <span class="bu">float</span>(items[<span class="dv">3</span>])</span>
<span id="cb16-262"><a href="#cb16-262" aria-hidden="true" tabindex="-1"></a>            dict_frames[<span class="bu">int</span>(items[<span class="dv">0</span>])].append((objnum, x, y))</span>
<span id="cb16-263"><a href="#cb16-263" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>  dict_frames</span>
<span id="cb16-264"><a href="#cb16-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-265"><a href="#cb16-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-266"><a href="#cb16-266" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_image(frames, trackers, image_shape<span class="op">=</span>(<span class="dv">135</span>,<span class="dv">240</span>), downsampling<span class="op">=</span><span class="dv">2</span><span class="op">*</span><span class="dv">4</span>):</span>
<span id="cb16-267"><a href="#cb16-267" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Builds a full image with consecutive frames and their displayed trackers</span></span>
<span id="cb16-268"><a href="#cb16-268" aria-hidden="true" tabindex="-1"></a><span class="co">    frames: a list of K np.array</span></span>
<span id="cb16-269"><a href="#cb16-269" aria-hidden="true" tabindex="-1"></a><span class="co">    trackers: a list of K trackers. Each tracker is a per frame list of tracked objects</span></span>
<span id="cb16-270"><a href="#cb16-270" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-271"><a href="#cb16-271" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> <span class="bu">len</span>(frames)</span>
<span id="cb16-272"><a href="#cb16-272" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(trackers) <span class="op">==</span> K</span>
<span id="cb16-273"><a href="#cb16-273" aria-hidden="true" tabindex="-1"></a>    font <span class="op">=</span> cv2.FONT_HERSHEY_COMPLEX</span>
<span id="cb16-274"><a href="#cb16-274" aria-hidden="true" tabindex="-1"></a>    output_img<span class="op">=</span>np.zeros((image_shape[<span class="dv">0</span>], image_shape[<span class="dv">1</span>]<span class="op">*</span>K, <span class="dv">3</span>), dtype<span class="op">=</span>np.uint8)</span>
<span id="cb16-275"><a href="#cb16-275" aria-hidden="true" tabindex="-1"></a>    object_ids <span class="op">=</span> []</span>
<span id="cb16-276"><a href="#cb16-276" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tracker <span class="kw">in</span> trackers:</span>
<span id="cb16-277"><a href="#cb16-277" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> detection <span class="kw">in</span> tracker:</span>
<span id="cb16-278"><a href="#cb16-278" aria-hidden="true" tabindex="-1"></a>            object_ids.append(detection[<span class="dv">0</span>])</span>
<span id="cb16-279"><a href="#cb16-279" aria-hidden="true" tabindex="-1"></a>    min_object_id <span class="op">=</span> <span class="bu">min</span>(object_ids)</span>
<span id="cb16-280"><a href="#cb16-280" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb16-281"><a href="#cb16-281" aria-hidden="true" tabindex="-1"></a>        frame <span class="op">=</span> cv2.cvtColor(cv2.resize(frames[i], image_shape[::<span class="op">-</span><span class="dv">1</span>]), cv2.COLOR_BGR2RGB)</span>
<span id="cb16-282"><a href="#cb16-282" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> detection <span class="kw">in</span> trackers[i]:</span>
<span id="cb16-283"><a href="#cb16-283" aria-hidden="true" tabindex="-1"></a>            cv2.putText(frame, <span class="ss">f'</span><span class="sc">{</span>detection[<span class="dv">0</span>]<span class="op">-</span>min_object_id <span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, (<span class="bu">int</span>(detection[<span class="dv">1</span>]<span class="op">/</span>downsampling)<span class="op">+</span><span class="dv">10</span>, <span class="bu">int</span>(detection[<span class="dv">2</span>]<span class="op">/</span>downsampling)<span class="op">+</span><span class="dv">10</span>), font, <span class="fl">0.5</span>, (<span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dv">1</span>, cv2.LINE_AA)</span>
<span id="cb16-284"><a href="#cb16-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-285"><a href="#cb16-285" aria-hidden="true" tabindex="-1"></a>        output_img[:,i<span class="op">*</span>image_shape[<span class="dv">1</span>]:(i<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>image_shape[<span class="dv">1</span>],:] <span class="op">=</span> frame</span>
<span id="cb16-286"><a href="#cb16-286" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output_img</span>
<span id="cb16-287"><a href="#cb16-287" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-288"><a href="#cb16-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-291"><a href="#cb16-291" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-292"><a href="#cb16-292" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-sort</span></span>
<span id="cb16-293"><a href="#cb16-293" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "*SORT*: the resulting count is also 2, but both counts arise from tracks generated by the same object, the latter not re-associated at all in the second frame. Additionally, the third object is discarded (in post-processing) by their strategy."</span></span>
<span id="cb16-294"><a href="#cb16-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-295"><a href="#cb16-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-296"><a href="#cb16-296" aria-hidden="true" tabindex="-1"></a><span class="co"># open sort output</span></span>
<span id="cb16-297"><a href="#cb16-297" aria-hidden="true" tabindex="-1"></a>tracker_file <span class="op">=</span> <span class="st">"sort/results/part_1_segment_0.txt"</span></span>
<span id="cb16-298"><a href="#cb16-298" aria-hidden="true" tabindex="-1"></a>frame_to_track <span class="op">=</span> read_sort_output(tracker_file)</span>
<span id="cb16-299"><a href="#cb16-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-300"><a href="#cb16-300" aria-hidden="true" tabindex="-1"></a>condisered_frames <span class="op">=</span> [frames[idx] <span class="cf">for</span> idx <span class="kw">in</span> idxs]</span>
<span id="cb16-301"><a href="#cb16-301" aria-hidden="true" tabindex="-1"></a>considered_tracks <span class="op">=</span> [frame_to_track[i] <span class="cf">for</span> i <span class="kw">in</span> idxs]</span>
<span id="cb16-302"><a href="#cb16-302" aria-hidden="true" tabindex="-1"></a>out_img <span class="op">=</span> build_image(condisered_frames, considered_tracks)</span>
<span id="cb16-303"><a href="#cb16-303" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">6</span>))</span>
<span id="cb16-304"><a href="#cb16-304" aria-hidden="true" tabindex="-1"></a>plt.imshow(out_img)</span>
<span id="cb16-305"><a href="#cb16-305" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)<span class="op">;</span></span>
<span id="cb16-306"><a href="#cb16-306" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-307"><a href="#cb16-307" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-308"><a href="#cb16-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-309"><a href="#cb16-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-310"><a href="#cb16-310" aria-hidden="true" tabindex="-1"></a><span class="fu"># Related works</span></span>
<span id="cb16-311"><a href="#cb16-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-312"><a href="#cb16-312" aria-hidden="true" tabindex="-1"></a><span class="fu">## AI-automated counting</span></span>
<span id="cb16-313"><a href="#cb16-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-314"><a href="#cb16-314" aria-hidden="true" tabindex="-1"></a>Counting from images has been an ongoing challenge in computer vision. Most</span>
<span id="cb16-315"><a href="#cb16-315" aria-hidden="true" tabindex="-1"></a>works can be divided into (i) detection-based methods where objects are</span>
<span id="cb16-316"><a href="#cb16-316" aria-hidden="true" tabindex="-1"></a>individually located for counting, (ii) density-based methods where counts are</span>
<span id="cb16-317"><a href="#cb16-317" aria-hidden="true" tabindex="-1"></a>obtained by summing a predicted density map, and (iii) regression-based</span>
<span id="cb16-318"><a href="#cb16-318" aria-hidden="true" tabindex="-1"></a>methods where counts are directly regressed from input images</span>
<span id="cb16-319"><a href="#cb16-319" aria-hidden="true" tabindex="-1"></a>(@Chattopadhyay). While some of these works tackled the problem of</span>
<span id="cb16-320"><a href="#cb16-320" aria-hidden="true" tabindex="-1"></a>counting in wild scenes (@Arteta2016), most are focused on pedestrian and</span>
<span id="cb16-321"><a href="#cb16-321" aria-hidden="true" tabindex="-1"></a>crowd counting. Though several works (@wu2020fast, @Xiong2017, @Miao2019)</span>
<span id="cb16-322"><a href="#cb16-322" aria-hidden="true" tabindex="-1"></a>showed the relevance of leveraging sequential inter-frame information to</span>
<span id="cb16-323"><a href="#cb16-323" aria-hidden="true" tabindex="-1"></a>achieve better counts at every frame, none of these methods actually attempt</span>
<span id="cb16-324"><a href="#cb16-324" aria-hidden="true" tabindex="-1"></a>to produce global video counts.</span>
<span id="cb16-325"><a href="#cb16-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-326"><a href="#cb16-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-327"><a href="#cb16-327" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computer vision for macro litter monitoring</span></span>
<span id="cb16-328"><a href="#cb16-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-329"><a href="#cb16-329" aria-hidden="true" tabindex="-1"></a>Automatic macro litter monitoring in rivers is still a relatively nascent</span>
<span id="cb16-330"><a href="#cb16-330" aria-hidden="true" tabindex="-1"></a>initiative, yet there have already been several attempts at using DNN-based</span>
<span id="cb16-331"><a href="#cb16-331" aria-hidden="true" tabindex="-1"></a>object recognition tools to count plastic trash. Recently, (@Proenca2020)</span>
<span id="cb16-332"><a href="#cb16-332" aria-hidden="true" tabindex="-1"></a>used a combination of two Convolutional Neural Networks (CNNs) to detect and</span>
<span id="cb16-333"><a href="#cb16-333" aria-hidden="true" tabindex="-1"></a>quantify plastic litter using geospatial images from Cambodia. In</span>
<span id="cb16-334"><a href="#cb16-334" aria-hidden="true" tabindex="-1"></a>(@Wolf2020), reliable estimates of plastic density were obtained using</span>
<span id="cb16-335"><a href="#cb16-335" aria-hidden="true" tabindex="-1"></a>Faster R-CNN (@ren2016faster) on images extracted from bridge-mounted</span>
<span id="cb16-336"><a href="#cb16-336" aria-hidden="true" tabindex="-1"></a>cameras. For underwater waste monitoring, (@vanlieshout2020automated)</span>
<span id="cb16-337"><a href="#cb16-337" aria-hidden="true" tabindex="-1"></a>assembled a dataset with bounding box annotations, and showed promising</span>
<span id="cb16-338"><a href="#cb16-338" aria-hidden="true" tabindex="-1"></a>performance with several object detectors. They later turned to generative</span>
<span id="cb16-339"><a href="#cb16-339" aria-hidden="true" tabindex="-1"></a>models to obtain more synthetic data from a small dataset @(Hong2020).</span>
<span id="cb16-340"><a href="#cb16-340" aria-hidden="true" tabindex="-1"></a>While proving the practicality of deep learning for automatic waste detection</span>
<span id="cb16-341"><a href="#cb16-341" aria-hidden="true" tabindex="-1"></a>in various contexts, these works only provide counts for separate images of</span>
<span id="cb16-342"><a href="#cb16-342" aria-hidden="true" tabindex="-1"></a>photographed litter. To the best of our knowledge, no solution has been</span>
<span id="cb16-343"><a href="#cb16-343" aria-hidden="true" tabindex="-1"></a>proposed to count litter directly in videos.</span>
<span id="cb16-344"><a href="#cb16-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-345"><a href="#cb16-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-346"><a href="#cb16-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-347"><a href="#cb16-347" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multi-object tracking</span></span>
<span id="cb16-348"><a href="#cb16-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-349"><a href="#cb16-349" aria-hidden="true" tabindex="-1"></a>Multi-object tracking usually involves object detection, data association and</span>
<span id="cb16-350"><a href="#cb16-350" aria-hidden="true" tabindex="-1"></a>track management, with a very large number of methods already existing before</span>
<span id="cb16-351"><a href="#cb16-351" aria-hidden="true" tabindex="-1"></a>DNNs (@luo2021). MOT approaches now mostly differ in the level of</span>
<span id="cb16-352"><a href="#cb16-352" aria-hidden="true" tabindex="-1"></a>supervision they require for each step: until recently, most successful</span>
<span id="cb16-353"><a href="#cb16-353" aria-hidden="true" tabindex="-1"></a>methods (like @Bewley2016 have been detection-based, i.e. involving</span>
<span id="cb16-354"><a href="#cb16-354" aria-hidden="true" tabindex="-1"></a>only a DNN-based object detector trained at the image level and coupled with</span>
<span id="cb16-355"><a href="#cb16-355" aria-hidden="true" tabindex="-1"></a>an unsupervised data association step. In specific fields such as pedestrian</span>
<span id="cb16-356"><a href="#cb16-356" aria-hidden="true" tabindex="-1"></a>tracking or autonomous driving, vast datasets now provide precise object</span>
<span id="cb16-357"><a href="#cb16-357" aria-hidden="true" tabindex="-1"></a>localisation and identities throughout entire videos (@Caesar2020)</span>
<span id="cb16-358"><a href="#cb16-358" aria-hidden="true" tabindex="-1"></a>@Dendorfer2020). Current state-of-the-art methods leverage this supervision via</span>
<span id="cb16-359"><a href="#cb16-359" aria-hidden="true" tabindex="-1"></a>deep visual feature extraction (@Wojke2018, @Zhanga) or even self-attention</span>
<span id="cb16-360"><a href="#cb16-360" aria-hidden="true" tabindex="-1"></a>(@Chu2021) and graph neural networks (@Wang2021). For these</span>
<span id="cb16-361"><a href="#cb16-361" aria-hidden="true" tabindex="-1"></a>applications, motion prediction may be required, yet well-trained appearance</span>
<span id="cb16-362"><a href="#cb16-362" aria-hidden="true" tabindex="-1"></a>models are usually enough to deal with detection failures under simple motion,</span>
<span id="cb16-363"><a href="#cb16-363" aria-hidden="true" tabindex="-1"></a>therefore the linear constant-velocity assumption often prevails</span>
<span id="cb16-364"><a href="#cb16-364" aria-hidden="true" tabindex="-1"></a>(@Ciaparrone2020b).</span>
<span id="cb16-365"><a href="#cb16-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-366"><a href="#cb16-366" aria-hidden="true" tabindex="-1"></a>In the case of macrolitter monitoring, however, available image datasets are</span>
<span id="cb16-367"><a href="#cb16-367" aria-hidden="true" tabindex="-1"></a>still orders of magnitude smaller, and annotated video datasets do not exist</span>
<span id="cb16-368"><a href="#cb16-368" aria-hidden="true" tabindex="-1"></a>at all. Even more so, real shooting conditions induce chaotic movements on the</span>
<span id="cb16-369"><a href="#cb16-369" aria-hidden="true" tabindex="-1"></a>boat-embedded cameras. A close work of ours is that of (@Fulton2018), who</span>
<span id="cb16-370"><a href="#cb16-370" aria-hidden="true" tabindex="-1"></a>paired Kalman filtering with optical flow to yield fruit count estimates on</span>
<span id="cb16-371"><a href="#cb16-371" aria-hidden="true" tabindex="-1"></a>entire video sequences captured by moving robots. However, their video footage</span>
<span id="cb16-372"><a href="#cb16-372" aria-hidden="true" tabindex="-1"></a>is captured at night with consistent lighting conditions, backgrounds are</span>
<span id="cb16-373"><a href="#cb16-373" aria-hidden="true" tabindex="-1"></a>largely similar across sequences, and camera movements are less challenging.</span>
<span id="cb16-374"><a href="#cb16-374" aria-hidden="true" tabindex="-1"></a>In our application context, we find that using MOT for the task of counting</span>
<span id="cb16-375"><a href="#cb16-375" aria-hidden="true" tabindex="-1"></a>objects requires a new movement model, to take into account missing detections</span>
<span id="cb16-376"><a href="#cb16-376" aria-hidden="true" tabindex="-1"></a>and large camera movements.</span>
<span id="cb16-377"><a href="#cb16-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-378"><a href="#cb16-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-379"><a href="#cb16-379" aria-hidden="true" tabindex="-1"></a><span class="fu"># Datasets for training and evaluation</span></span>
<span id="cb16-380"><a href="#cb16-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-381"><a href="#cb16-381" aria-hidden="true" tabindex="-1"></a>Our main dataset of annotated images is used to train the object detector.</span>
<span id="cb16-382"><a href="#cb16-382" aria-hidden="true" tabindex="-1"></a>Then, only for evaluation purposes, we provide videos with annotated object</span>
<span id="cb16-383"><a href="#cb16-383" aria-hidden="true" tabindex="-1"></a>positions and known global counts. Our motivation is to avoid relying on</span>
<span id="cb16-384"><a href="#cb16-384" aria-hidden="true" tabindex="-1"></a>training data that requires this resource-consuming process.</span>
<span id="cb16-385"><a href="#cb16-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-386"><a href="#cb16-386" aria-hidden="true" tabindex="-1"></a><span class="fu">## Images</span></span>
<span id="cb16-387"><a href="#cb16-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-388"><a href="#cb16-388" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data collection</span></span>
<span id="cb16-389"><a href="#cb16-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-390"><a href="#cb16-390" aria-hidden="true" tabindex="-1"></a>With help from volunteers, we compile photographs of litter stranded on river</span>
<span id="cb16-391"><a href="#cb16-391" aria-hidden="true" tabindex="-1"></a>banks after increased river discharge, shot directly from kayaks navigating at</span>
<span id="cb16-392"><a href="#cb16-392" aria-hidden="true" tabindex="-1"></a>varying distances from the shore. Images span multiple rivers with various</span>
<span id="cb16-393"><a href="#cb16-393" aria-hidden="true" tabindex="-1"></a>levels of water current, on different seasons, mostly in southwestern France.</span>
<span id="cb16-394"><a href="#cb16-394" aria-hidden="true" tabindex="-1"></a>The resulting pictures depict trash items under the same conditions as the</span>
<span id="cb16-395"><a href="#cb16-395" aria-hidden="true" tabindex="-1"></a>video footage we wish to count on, while spanning a wide variety of</span>
<span id="cb16-396"><a href="#cb16-396" aria-hidden="true" tabindex="-1"></a>backgrounds, light conditions, viewing angles and picture quality.</span>
<span id="cb16-397"><a href="#cb16-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-398"><a href="#cb16-398" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bounding box annotation</span></span>
<span id="cb16-399"><a href="#cb16-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-400"><a href="#cb16-400" aria-hidden="true" tabindex="-1"></a>For object detection applications, the images are annotated using a custom</span>
<span id="cb16-401"><a href="#cb16-401" aria-hidden="true" tabindex="-1"></a>online platform where each object is located using a bounding box. In this</span>
<span id="cb16-402"><a href="#cb16-402" aria-hidden="true" tabindex="-1"></a>work, we focus only on litter counting without classification, however the</span>
<span id="cb16-403"><a href="#cb16-403" aria-hidden="true" tabindex="-1"></a>annotated objects are already classified into specific categories which are</span>
<span id="cb16-404"><a href="#cb16-404" aria-hidden="true" tabindex="-1"></a>described in @fig-trash-categories-image.</span>
<span id="cb16-405"><a href="#cb16-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-406"><a href="#cb16-406" aria-hidden="true" tabindex="-1"></a>A few samples are depicted below:</span>
<span id="cb16-407"><a href="#cb16-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-410"><a href="#cb16-410" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-411"><a href="#cb16-411" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ExifTags</span>
<span id="cb16-412"><a href="#cb16-412" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pycocotools.coco <span class="im">import</span> COCO</span>
<span id="cb16-413"><a href="#cb16-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-414"><a href="#cb16-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-415"><a href="#cb16-415" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_bbox(image, anns, ratio):</span>
<span id="cb16-416"><a href="#cb16-416" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb16-417"><a href="#cb16-417" aria-hidden="true" tabindex="-1"></a><span class="co">    Display the specified annotations.</span></span>
<span id="cb16-418"><a href="#cb16-418" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-419"><a href="#cb16-419" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ann <span class="kw">in</span> anns:</span>
<span id="cb16-420"><a href="#cb16-420" aria-hidden="true" tabindex="-1"></a>        [bbox_x, bbox_y, bbox_w, bbox_h] <span class="op">=</span> (ratio<span class="op">*</span>np.array(ann[<span class="st">'bbox'</span>])).astype(<span class="bu">int</span>)</span>
<span id="cb16-421"><a href="#cb16-421" aria-hidden="true" tabindex="-1"></a>        cv2.rectangle(image, (bbox_x,bbox_y),(bbox_x<span class="op">+</span>bbox_w,bbox_y<span class="op">+</span>bbox_h), color<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">255</span>),thickness<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb16-422"><a href="#cb16-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-423"><a href="#cb16-423" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image</span>
<span id="cb16-424"><a href="#cb16-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-425"><a href="#cb16-425" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span> <span class="op">=</span> <span class="st">'surfnet/data/images'</span></span>
<span id="cb16-426"><a href="#cb16-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-427"><a href="#cb16-427" aria-hidden="true" tabindex="-1"></a>ann_dir <span class="op">=</span> os.path.join(<span class="bu">dir</span>,<span class="st">'annotations'</span>)</span>
<span id="cb16-428"><a href="#cb16-428" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> os.path.join(<span class="bu">dir</span>,<span class="st">'images'</span>)</span>
<span id="cb16-429"><a href="#cb16-429" aria-hidden="true" tabindex="-1"></a>ann_file <span class="op">=</span> os.path.join(ann_dir, <span class="st">'subset_of_annotations.json'</span>)</span>
<span id="cb16-430"><a href="#cb16-430" aria-hidden="true" tabindex="-1"></a>coco <span class="op">=</span> COCO(ann_file)</span>
<span id="cb16-431"><a href="#cb16-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-432"><a href="#cb16-432" aria-hidden="true" tabindex="-1"></a>imgIds <span class="op">=</span> np.array(coco.getImgIds())</span>
<span id="cb16-433"><a href="#cb16-433" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="sc">{}</span><span class="st"> images loaded'</span>.<span class="bu">format</span>(<span class="bu">len</span>(imgIds)))</span>
<span id="cb16-434"><a href="#cb16-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-435"><a href="#cb16-435" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> imgId <span class="kw">in</span> imgIds:</span>
<span id="cb16-436"><a href="#cb16-436" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb16-437"><a href="#cb16-437" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> coco.loadImgs(ids<span class="op">=</span>[imgId])[<span class="dv">0</span>]</span>
<span id="cb16-438"><a href="#cb16-438" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb16-439"><a href="#cb16-439" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(os.path.join(data_dir,image[<span class="st">'file_name'</span>]))</span>
<span id="cb16-440"><a href="#cb16-440" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rotation of the picture in the Exif tags</span></span>
<span id="cb16-441"><a href="#cb16-441" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> orientation <span class="kw">in</span> ExifTags.TAGS.keys():</span>
<span id="cb16-442"><a href="#cb16-442" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ExifTags.TAGS[orientation]<span class="op">==</span><span class="st">'Orientation'</span>:</span>
<span id="cb16-443"><a href="#cb16-443" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb16-444"><a href="#cb16-444" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-445"><a href="#cb16-445" aria-hidden="true" tabindex="-1"></a>        exif <span class="op">=</span> image._getexif()</span>
<span id="cb16-446"><a href="#cb16-446" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exif <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb16-447"><a href="#cb16-447" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> exif[orientation] <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb16-448"><a href="#cb16-448" aria-hidden="true" tabindex="-1"></a>                image<span class="op">=</span>image.rotate(<span class="dv">180</span>, expand<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-449"><a href="#cb16-449" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> exif[orientation] <span class="op">==</span> <span class="dv">6</span>:</span>
<span id="cb16-450"><a href="#cb16-450" aria-hidden="true" tabindex="-1"></a>                image<span class="op">=</span>image.rotate(<span class="dv">270</span>, expand<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-451"><a href="#cb16-451" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> exif[orientation] <span class="op">==</span> <span class="dv">8</span>:</span>
<span id="cb16-452"><a href="#cb16-452" aria-hidden="true" tabindex="-1"></a>                image<span class="op">=</span>image.rotate(<span class="dv">90</span>, expand<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-453"><a href="#cb16-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-454"><a href="#cb16-454" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> (<span class="pp">AttributeError</span>, <span class="pp">KeyError</span>, <span class="pp">IndexError</span>):</span>
<span id="cb16-455"><a href="#cb16-455" aria-hidden="true" tabindex="-1"></a>        <span class="co"># cases: image don't have getexif</span></span>
<span id="cb16-456"><a href="#cb16-456" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb16-457"><a href="#cb16-457" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> cv2.cvtColor(np.array(image.convert(<span class="st">'RGB'</span>)),  cv2.COLOR_RGB2BGR)</span>
<span id="cb16-458"><a href="#cb16-458" aria-hidden="true" tabindex="-1"></a>    annIds <span class="op">=</span> coco.getAnnIds(imgIds<span class="op">=</span>[imgId])</span>
<span id="cb16-459"><a href="#cb16-459" aria-hidden="true" tabindex="-1"></a>    anns <span class="op">=</span> coco.loadAnns(ids<span class="op">=</span>annIds)</span>
<span id="cb16-460"><a href="#cb16-460" aria-hidden="true" tabindex="-1"></a>    h,w <span class="op">=</span> image.shape[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb16-461"><a href="#cb16-461" aria-hidden="true" tabindex="-1"></a>    target_h <span class="op">=</span> <span class="dv">1080</span></span>
<span id="cb16-462"><a href="#cb16-462" aria-hidden="true" tabindex="-1"></a>    ratio <span class="op">=</span> target_h<span class="op">/</span>h</span>
<span id="cb16-463"><a href="#cb16-463" aria-hidden="true" tabindex="-1"></a>    target_w <span class="op">=</span> <span class="bu">int</span>(ratio<span class="op">*</span>w) </span>
<span id="cb16-464"><a href="#cb16-464" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> cv2.resize(image,(target_w,target_h))</span>
<span id="cb16-465"><a href="#cb16-465" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> draw_bbox(image,anns,ratio)</span>
<span id="cb16-466"><a href="#cb16-466" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> cv2.cvtColor(image,cv2.COLOR_BGR2RGB)</span>
<span id="cb16-467"><a href="#cb16-467" aria-hidden="true" tabindex="-1"></a>    plt.imshow(image)</span>
<span id="cb16-468"><a href="#cb16-468" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb16-469"><a href="#cb16-469" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-470"><a href="#cb16-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-471"><a href="#cb16-471" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video sequences</span></span>
<span id="cb16-472"><a href="#cb16-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-473"><a href="#cb16-473" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data collection</span></span>
<span id="cb16-474"><a href="#cb16-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-475"><a href="#cb16-475" aria-hidden="true" tabindex="-1"></a>For evaluation, an on-field study was conducted with 20 volunteers to manually</span>
<span id="cb16-476"><a href="#cb16-476" aria-hidden="true" tabindex="-1"></a>count litter along three different riverbank sections in April 2021, on the</span>
<span id="cb16-477"><a href="#cb16-477" aria-hidden="true" tabindex="-1"></a>Gave d'Oloron near Auterrive (Pyrénées-Atlantiques, France), using kayaks. The</span>
<span id="cb16-478"><a href="#cb16-478" aria-hidden="true" tabindex="-1"></a>river sections, each 500 meters long, were precisely defined for their</span>
<span id="cb16-479"><a href="#cb16-479" aria-hidden="true" tabindex="-1"></a>differences in background, vegetation, river current, light conditions and</span>
<span id="cb16-480"><a href="#cb16-480" aria-hidden="true" tabindex="-1"></a>accessibility (see @sec-video-dataset-appendix for aerial views of</span>
<span id="cb16-481"><a href="#cb16-481" aria-hidden="true" tabindex="-1"></a>the shooting site and details on the river sections). In total, the three</span>
<span id="cb16-482"><a href="#cb16-482" aria-hidden="true" tabindex="-1"></a>videos amount to 20 minutes of footage at 24 frames per second (fps) and a</span>
<span id="cb16-483"><a href="#cb16-483" aria-hidden="true" tabindex="-1"></a>resolution of 1920x1080 pixels.</span>
<span id="cb16-484"><a href="#cb16-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-485"><a href="#cb16-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-486"><a href="#cb16-486" aria-hidden="true" tabindex="-1"></a><span class="fu">### Track annotation</span></span>
<span id="cb16-487"><a href="#cb16-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-488"><a href="#cb16-488" aria-hidden="true" tabindex="-1"></a>On video footage, we manually recovered all visible object trajectories on</span>
<span id="cb16-489"><a href="#cb16-489" aria-hidden="true" tabindex="-1"></a>each river section using an online video annotation tool (more details</span>
<span id="cb16-490"><a href="#cb16-490" aria-hidden="true" tabindex="-1"></a>in @sec-video-dataset-appendix for the precise methodology). From that, we</span>
<span id="cb16-491"><a href="#cb16-491" aria-hidden="true" tabindex="-1"></a>obtained a collection of distinct object tracks spanning the entire footage.</span>
<span id="cb16-492"><a href="#cb16-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-493"><a href="#cb16-493" aria-hidden="true" tabindex="-1"></a><span class="fu"># Optical flow-based counting via Bayesian filtering and confidence regions</span></span>
<span id="cb16-494"><a href="#cb16-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-495"><a href="#cb16-495" aria-hidden="true" tabindex="-1"></a>Our counting method is divided into several interacting blocks. First, a</span>
<span id="cb16-496"><a href="#cb16-496" aria-hidden="true" tabindex="-1"></a>detector outputs a set of predicted positions for objects in the current</span>
<span id="cb16-497"><a href="#cb16-497" aria-hidden="true" tabindex="-1"></a>frame. The second block is a tracking module designing consistent trajectories</span>
<span id="cb16-498"><a href="#cb16-498" aria-hidden="true" tabindex="-1"></a>of potential objects within the video. At each frame, a third block links the</span>
<span id="cb16-499"><a href="#cb16-499" aria-hidden="true" tabindex="-1"></a>successive detections together using confidence regions provided by the</span>
<span id="cb16-500"><a href="#cb16-500" aria-hidden="true" tabindex="-1"></a>tracking module, proposing distinct tracks for each object. A final</span>
<span id="cb16-501"><a href="#cb16-501" aria-hidden="true" tabindex="-1"></a>postprocessing step only keeps the best tracks which are enumerated to yield</span>
<span id="cb16-502"><a href="#cb16-502" aria-hidden="true" tabindex="-1"></a>the final count.</span>
<span id="cb16-503"><a href="#cb16-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-504"><a href="#cb16-504" aria-hidden="true" tabindex="-1"></a><span class="fu">## Detector</span></span>
<span id="cb16-505"><a href="#cb16-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-506"><a href="#cb16-506" aria-hidden="true" tabindex="-1"></a><span class="fu">### Center-based anchor-free detection</span></span>
<span id="cb16-507"><a href="#cb16-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-508"><a href="#cb16-508" aria-hidden="true" tabindex="-1"></a>In most benchmarks, the prediction quality of object attributes like bounding</span>
<span id="cb16-509"><a href="#cb16-509" aria-hidden="true" tabindex="-1"></a>boxes is often used to  improve tracking. For counting, however, point</span>
<span id="cb16-510"><a href="#cb16-510" aria-hidden="true" tabindex="-1"></a>detection is theoretically enough and advantageous in many ways. First, to</span>
<span id="cb16-511"><a href="#cb16-511" aria-hidden="true" tabindex="-1"></a>build large datasets, a method which only requires the lightest annotation</span>
<span id="cb16-512"><a href="#cb16-512" aria-hidden="true" tabindex="-1"></a>format may benefit from more data due to annotation ease. Second, contrary to</span>
<span id="cb16-513"><a href="#cb16-513" aria-hidden="true" tabindex="-1"></a>previous popular methods (@ren2016faster) involving intricate mechanisms</span>
<span id="cb16-514"><a href="#cb16-514" aria-hidden="true" tabindex="-1"></a>for bounding box prediction, center-based and anchor-free detectors</span>
<span id="cb16-515"><a href="#cb16-515" aria-hidden="true" tabindex="-1"></a>(@Zhou2019, @Law) only use additional regression heads which can simply be</span>
<span id="cb16-516"><a href="#cb16-516" aria-hidden="true" tabindex="-1"></a>removed for point detection. Adding to all this, (@Zhanga) highlight</span>
<span id="cb16-517"><a href="#cb16-517" aria-hidden="true" tabindex="-1"></a>conceptual and experimental reasons to favor anchor-free detection in</span>
<span id="cb16-518"><a href="#cb16-518" aria-hidden="true" tabindex="-1"></a>tracking-related tasks.</span>
<span id="cb16-519"><a href="#cb16-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-520"><a href="#cb16-520" aria-hidden="true" tabindex="-1"></a>For these reasons, we use a stripped version of CenterNet (@Zhou2019)</span>
<span id="cb16-521"><a href="#cb16-521" aria-hidden="true" tabindex="-1"></a>where offset and bounding box regression heads are discarded to output bare</span>
<span id="cb16-522"><a href="#cb16-522" aria-hidden="true" tabindex="-1"></a>estimates of center positions on a coarse grid. An encoder-decoder network</span>
<span id="cb16-523"><a href="#cb16-523" aria-hidden="true" tabindex="-1"></a>takes an input image $I \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>^{w \times h \times 3}$ (an RGB image of</span>
<span id="cb16-524"><a href="#cb16-524" aria-hidden="true" tabindex="-1"></a>width $w$ and height $h$), and produces a heatmap $\hat{Y} \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>^{\lfloor</span>
<span id="cb16-525"><a href="#cb16-525" aria-hidden="true" tabindex="-1"></a>w/p\rfloor \times \lfloor h/p\rfloor}$ such that  $\hat{Y}_{xy}$ is the</span>
<span id="cb16-526"><a href="#cb16-526" aria-hidden="true" tabindex="-1"></a>probability that $(x,y)$ is the center of an object ($p$ being a stride</span>
<span id="cb16-527"><a href="#cb16-527" aria-hidden="true" tabindex="-1"></a>coefficient). At inference, peak detection and thresholding are applied to</span>
<span id="cb16-528"><a href="#cb16-528" aria-hidden="true" tabindex="-1"></a>$\hat{Y}$, yielding the set of detections. The bulk of this detector relies on</span>
<span id="cb16-529"><a href="#cb16-529" aria-hidden="true" tabindex="-1"></a>the DLA34 architecture (@fisher2017). In a video, for each frame $I_n \in</span>
<span id="cb16-530"><a href="#cb16-530" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>^{w \times h \times 3}$ (where $n$ indexes the frame number), the</span>
<span id="cb16-531"><a href="#cb16-531" aria-hidden="true" tabindex="-1"></a>detector outputs a set $\mathcal{D}_n = \{z_n^i\}_{1 \leq i \leq D_n}$ where</span>
<span id="cb16-532"><a href="#cb16-532" aria-hidden="true" tabindex="-1"></a>each $z_n^i = (x_n^i,y_n^i)$ specifies the coordinates of one of the $D_n$</span>
<span id="cb16-533"><a href="#cb16-533" aria-hidden="true" tabindex="-1"></a>detected objects.</span>
<span id="cb16-534"><a href="#cb16-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-535"><a href="#cb16-535" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training {#sec-detector_training}</span></span>
<span id="cb16-536"><a href="#cb16-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-537"><a href="#cb16-537" aria-hidden="true" tabindex="-1"></a>Training the detector is done similarly as in @Proenca2020.</span>
<span id="cb16-538"><a href="#cb16-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-539"><a href="#cb16-539" aria-hidden="true" tabindex="-1"></a> For every image, the corresponding set $\mathcal{B} =</span>
<span id="cb16-540"><a href="#cb16-540" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>(c^w_i,c^h_i,w_i,h_i)<span class="sc">\}</span>_{1 \leq i\leq B}$ of $B$ annotated bounding boxes --</span>
<span id="cb16-541"><a href="#cb16-541" aria-hidden="true" tabindex="-1"></a>*i.e.* a center $(c^w_i,c^h_i)$, a width $w_i$ and a height $h_i$-- is</span>
<span id="cb16-542"><a href="#cb16-542" aria-hidden="true" tabindex="-1"></a>rendered into a ground truth heatmap $Y \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>^{{\lfloor w/p\rfloor \times</span>
<span id="cb16-543"><a href="#cb16-543" aria-hidden="true" tabindex="-1"></a>\lfloor h/p\rfloor}}$ by applying kernels at the bounding box centers and</span>
<span id="cb16-544"><a href="#cb16-544" aria-hidden="true" tabindex="-1"></a>taking element-wise maximum. For all $1 \leq x \leq w/p$, $1 \leq y \leq h/p$,</span>
<span id="cb16-545"><a href="#cb16-545" aria-hidden="true" tabindex="-1"></a>the ground truth at $(x,y)$ is</span>
<span id="cb16-546"><a href="#cb16-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-547"><a href="#cb16-547" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-548"><a href="#cb16-548" aria-hidden="true" tabindex="-1"></a>  Y_{xy} =  \max\limits_{1\leq i\leq B}\left(\exp\left<span class="sc">\{</span>-\frac{(x-c_i^w)^2+(y-c_i^h)^2}{2\sigma^2_i}\right<span class="sc">\}</span>\right),</span>
<span id="cb16-549"><a href="#cb16-549" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-550"><a href="#cb16-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-551"><a href="#cb16-551" aria-hidden="true" tabindex="-1"></a>where $\sigma_i$ is a parameter depending on the size of the object.</span>
<span id="cb16-552"><a href="#cb16-552" aria-hidden="true" tabindex="-1"></a>Training the detector is done by minimizing a penalty-reduced weighted focal loss</span>
<span id="cb16-553"><a href="#cb16-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-554"><a href="#cb16-554" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-555"><a href="#cb16-555" aria-hidden="true" tabindex="-1"></a>\mathcal{L}(\hat{Y},Y) = -\sum_{x,y} \gamma_{xy}^\beta\left(1-\hat{p}_{xy}\right)^\alpha \log{\left(\hat{p}_{xy}\right)},</span>
<span id="cb16-556"><a href="#cb16-556" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-557"><a href="#cb16-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-558"><a href="#cb16-558" aria-hidden="true" tabindex="-1"></a>where $\alpha$, $\beta$ are hyperparameters and</span>
<span id="cb16-559"><a href="#cb16-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-560"><a href="#cb16-560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-561"><a href="#cb16-561" aria-hidden="true" tabindex="-1"></a>(\hat{p}_{xy},\gamma_{xy}) = \left<span class="sc">\{</span></span>
<span id="cb16-562"><a href="#cb16-562" aria-hidden="true" tabindex="-1"></a>    \begin{array}{ll}</span>
<span id="cb16-563"><a href="#cb16-563" aria-hidden="true" tabindex="-1"></a>        (\hat{Y}_{xy},1) &amp; \mbox{if } Y_{xy} = 1, <span class="sc">\\</span></span>
<span id="cb16-564"><a href="#cb16-564" aria-hidden="true" tabindex="-1"></a>        (1 - \hat{Y}_{xy},1 - Y_{xy}) &amp; \mbox{otherwise.}</span>
<span id="cb16-565"><a href="#cb16-565" aria-hidden="true" tabindex="-1"></a>    \end{array}</span>
<span id="cb16-566"><a href="#cb16-566" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb16-567"><a href="#cb16-567" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-568"><a href="#cb16-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-569"><a href="#cb16-569" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian tracking with optical flow {#sec-bayesian_tracking} </span></span>
<span id="cb16-570"><a href="#cb16-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-571"><a href="#cb16-571" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optical flow</span></span>
<span id="cb16-572"><a href="#cb16-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-573"><a href="#cb16-573" aria-hidden="true" tabindex="-1"></a>Between two timesteps $n-1$ and $n$, the optical flow $\Delta_n$ is a mapping</span>
<span id="cb16-574"><a href="#cb16-574" aria-hidden="true" tabindex="-1"></a>satisfying the following consistency constraint (@paragios2006):</span>
<span id="cb16-575"><a href="#cb16-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-576"><a href="#cb16-576" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-577"><a href="#cb16-577" aria-hidden="true" tabindex="-1"></a>\widetilde{I}_n[u] = \widetilde{I}_{n-1}<span class="co">[</span><span class="ot">u+\Delta_n(u)</span><span class="co">]</span>,</span>
<span id="cb16-578"><a href="#cb16-578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-579"><a href="#cb16-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-580"><a href="#cb16-580" aria-hidden="true" tabindex="-1"></a>where, in our case, $\widetilde{I}_n$ denotes the frame $n$ downsampled to</span>
<span id="cb16-581"><a href="#cb16-581" aria-hidden="true" tabindex="-1"></a>dimensions $\lfloor w/p\rfloor \times \lfloor h/p\rfloor$ and $u = (x,y)$ is a</span>
<span id="cb16-582"><a href="#cb16-582" aria-hidden="true" tabindex="-1"></a>coordinate on that grid. To estimate $\Delta_n$, we choose a simple</span>
<span id="cb16-583"><a href="#cb16-583" aria-hidden="true" tabindex="-1"></a>unsupervised Gunner-Farneback algorithm which does not require further</span>
<span id="cb16-584"><a href="#cb16-584" aria-hidden="true" tabindex="-1"></a>annotations, see @farneback2003two for details.</span>
<span id="cb16-585"><a href="#cb16-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-586"><a href="#cb16-586" aria-hidden="true" tabindex="-1"></a><span class="fu">### State space model {#sec-state_space_model}</span></span>
<span id="cb16-587"><a href="#cb16-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-588"><a href="#cb16-588" aria-hidden="true" tabindex="-1"></a>Using optical flow as a building block, we posit a state space model where</span>
<span id="cb16-589"><a href="#cb16-589" aria-hidden="true" tabindex="-1"></a>estimates of $\Delta_n$ are used as a time and state-dependent offset for the</span>
<span id="cb16-590"><a href="#cb16-590" aria-hidden="true" tabindex="-1"></a>state transition.</span>
<span id="cb16-591"><a href="#cb16-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-592"><a href="#cb16-592" aria-hidden="true" tabindex="-1"></a>Let $(X_k)_{k \geq 1}$ and $(Z_k)_{k \geq 1}$ be the true (but hidden) and</span>
<span id="cb16-593"><a href="#cb16-593" aria-hidden="true" tabindex="-1"></a>observed (detected) positions of a target object in $\mathbb{R}^2$, respectively.</span>
<span id="cb16-594"><a href="#cb16-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-595"><a href="#cb16-595" aria-hidden="true" tabindex="-1"></a>Considering the optical flow value associated with $X_{k-1}$ on the discrete</span>
<span id="cb16-596"><a href="#cb16-596" aria-hidden="true" tabindex="-1"></a>grid of dimensions $\lfloor w/p\rfloor \times \lfloor h/p\rfloor$, write</span>
<span id="cb16-597"><a href="#cb16-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-598"><a href="#cb16-598" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-599"><a href="#cb16-599" aria-hidden="true" tabindex="-1"></a>X_k = X_{k-1} + \Delta_k(\lfloor X_{k-1} \rfloor) + \eta_k</span>
<span id="cb16-600"><a href="#cb16-600" aria-hidden="true" tabindex="-1"></a>$$ {#eq-state-transition}</span>
<span id="cb16-601"><a href="#cb16-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-602"><a href="#cb16-602" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb16-603"><a href="#cb16-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-604"><a href="#cb16-604" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-605"><a href="#cb16-605" aria-hidden="true" tabindex="-1"></a>Z_k = X_k + \varepsilon_k,</span>
<span id="cb16-606"><a href="#cb16-606" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-607"><a href="#cb16-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-608"><a href="#cb16-608" aria-hidden="true" tabindex="-1"></a>where $(\eta_k)_{k\geq 1}$ are i.i.d. centered Gaussian random variables with</span>
<span id="cb16-609"><a href="#cb16-609" aria-hidden="true" tabindex="-1"></a>covariance matrix $Q$ independent of $(\varepsilon_k)_{k\geq 1}$ i.i.d.</span>
<span id="cb16-610"><a href="#cb16-610" aria-hidden="true" tabindex="-1"></a>centered Gaussian random variables with covariance matrix $R$.</span>
<span id="cb16-611"><a href="#cb16-611" aria-hidden="true" tabindex="-1"></a>In the following, $Q$ and $R$ are assumed to be diagonal, and are</span>
<span id="cb16-612"><a href="#cb16-612" aria-hidden="true" tabindex="-1"></a>hyperparameters set to values given in @sec-covariance_matrices.</span>
<span id="cb16-613"><a href="#cb16-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-614"><a href="#cb16-614" aria-hidden="true" tabindex="-1"></a><span class="fu">### Approximations of the filtering distributions</span></span>
<span id="cb16-615"><a href="#cb16-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-616"><a href="#cb16-616" aria-hidden="true" tabindex="-1"></a>Denoting $u_{1:k} = (u_1,\ldots,u_k)$ for any $k$ and sequence $(u_i)_{i \geq</span>
<span id="cb16-617"><a href="#cb16-617" aria-hidden="true" tabindex="-1"></a>0}$, Bayesian filtering aims at computing the conditional distribution of</span>
<span id="cb16-618"><a href="#cb16-618" aria-hidden="true" tabindex="-1"></a>$X_k$ given $Z_{1:k}$, referred to as the filtering distribution. In the case</span>
<span id="cb16-619"><a href="#cb16-619" aria-hidden="true" tabindex="-1"></a>of linear and Gaussian state space models, this distribution is known to be</span>
<span id="cb16-620"><a href="#cb16-620" aria-hidden="true" tabindex="-1"></a>Gaussian, and Kalman filtering allows to update exactly the posterior mean</span>
<span id="cb16-621"><a href="#cb16-621" aria-hidden="true" tabindex="-1"></a>$\mu_k = \mathbb{E}<span class="co">[</span><span class="ot">X_k|Z_{1:k}</span><span class="co">]</span>$ and posterior variance matrix $\Sigma_k =</span>
<span id="cb16-622"><a href="#cb16-622" aria-hidden="true" tabindex="-1"></a>\mathbb{V}<span class="co">[</span><span class="ot">X_k|Z_{1:k}</span><span class="co">]</span>$. This algorithm and its extensions are prevalent and used</span>
<span id="cb16-623"><a href="#cb16-623" aria-hidden="true" tabindex="-1"></a>extensively in time-series and sequential-data analysis. As the transition</span>
<span id="cb16-624"><a href="#cb16-624" aria-hidden="true" tabindex="-1"></a>model proposed in @eq-state-transition is nonlinear, Kalman updates cannot</span>
<span id="cb16-625"><a href="#cb16-625" aria-hidden="true" tabindex="-1"></a>be implemented and solving the target tracking task requires resorting to</span>
<span id="cb16-626"><a href="#cb16-626" aria-hidden="true" tabindex="-1"></a>alternatives. Many solutions have been proposed to deal with strong</span>
<span id="cb16-627"><a href="#cb16-627" aria-hidden="true" tabindex="-1"></a>nonlinearities in the literature, such as unscented Kalman filters (UKF) or</span>
<span id="cb16-628"><a href="#cb16-628" aria-hidden="true" tabindex="-1"></a>Sequential Monte Carlo (SMC) methods (see @sarkka2013bayesian and</span>
<span id="cb16-629"><a href="#cb16-629" aria-hidden="true" tabindex="-1"></a>references therein). Most SMC methods have been widely studied and shown to be</span>
<span id="cb16-630"><a href="#cb16-630" aria-hidden="true" tabindex="-1"></a>very effective even in presence of strongly nonlinear dynamics and/or</span>
<span id="cb16-631"><a href="#cb16-631" aria-hidden="true" tabindex="-1"></a>non-Gaussian noise, however such sample-based solutions are computationally</span>
<span id="cb16-632"><a href="#cb16-632" aria-hidden="true" tabindex="-1"></a>intensive, especially in settings where many objects have to be tracked and</span>
<span id="cb16-633"><a href="#cb16-633" aria-hidden="true" tabindex="-1"></a>false positive detections involve unnecessary sampling steps. On the other</span>
<span id="cb16-634"><a href="#cb16-634" aria-hidden="true" tabindex="-1"></a>hand, UKF requires fewer samples and provides an intermediary solution in</span>
<span id="cb16-635"><a href="#cb16-635" aria-hidden="true" tabindex="-1"></a>presence of mild nonlinearities. In our setting, we find that a linearisation</span>
<span id="cb16-636"><a href="#cb16-636" aria-hidden="true" tabindex="-1"></a>of the model @eq-state-transition yields approximation which is</span>
<span id="cb16-637"><a href="#cb16-637" aria-hidden="true" tabindex="-1"></a>computationally cheap and as robust on our data:</span>
<span id="cb16-638"><a href="#cb16-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-639"><a href="#cb16-639" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-640"><a href="#cb16-640" aria-hidden="true" tabindex="-1"></a>X_k = X_{k-1} + \Delta_k(\lfloor \mu_{k-1} \rfloor) + \partial_X\Delta_k(\lfloor \mu_{k-1} \rfloor)(X_{k-1}-\mu_{k-1}) + \eta_k .</span>
<span id="cb16-641"><a href="#cb16-641" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-642"><a href="#cb16-642" aria-hidden="true" tabindex="-1"></a>where $\partial_X$ is the derivative operator with respect to the 2-dimensional spatial input $X$. </span>
<span id="cb16-643"><a href="#cb16-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-644"><a href="#cb16-644" aria-hidden="true" tabindex="-1"></a>This allows the implementation of Kalman updates on the linearised model, a</span>
<span id="cb16-645"><a href="#cb16-645" aria-hidden="true" tabindex="-1"></a>technique named extended Kalman filtering (EKF). For a more complete</span>
<span id="cb16-646"><a href="#cb16-646" aria-hidden="true" tabindex="-1"></a>presentation of Bayesian and Kalman filtering, please refer to</span>
<span id="cb16-647"><a href="#cb16-647" aria-hidden="true" tabindex="-1"></a>@sec-bayesian_filtering. On the currently available data, we find that</span>
<span id="cb16-648"><a href="#cb16-648" aria-hidden="true" tabindex="-1"></a>the optical flow estimates are very informative and accurate, making this</span>
<span id="cb16-649"><a href="#cb16-649" aria-hidden="true" tabindex="-1"></a>approximation sufficient. For completeness, we present</span>
<span id="cb16-650"><a href="#cb16-650" aria-hidden="true" tabindex="-1"></a>@sec-impact-algorithm-appendix an SMC-based solution and discuss the</span>
<span id="cb16-651"><a href="#cb16-651" aria-hidden="true" tabindex="-1"></a>empirical differences and use-cases where the latter might be a more relevant</span>
<span id="cb16-652"><a href="#cb16-652" aria-hidden="true" tabindex="-1"></a>choice. </span>
<span id="cb16-653"><a href="#cb16-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-654"><a href="#cb16-654" aria-hidden="true" tabindex="-1"></a>In any case, the state space model naturally accounts for missing</span>
<span id="cb16-655"><a href="#cb16-655" aria-hidden="true" tabindex="-1"></a>observations, as the contribution of $\Delta_k$ in every transition ensures</span>
<span id="cb16-656"><a href="#cb16-656" aria-hidden="true" tabindex="-1"></a>that each filter can cope with arbitrary inter-frame motion to keep track of</span>
<span id="cb16-657"><a href="#cb16-657" aria-hidden="true" tabindex="-1"></a>its target. </span>
<span id="cb16-658"><a href="#cb16-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-659"><a href="#cb16-659" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generating potential object tracks</span></span>
<span id="cb16-660"><a href="#cb16-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-661"><a href="#cb16-661" aria-hidden="true" tabindex="-1"></a>The full MOT algorithm consists of a set of single-object trackers following</span>
<span id="cb16-662"><a href="#cb16-662" aria-hidden="true" tabindex="-1"></a>the previous model, but each provided with distinct observations at every</span>
<span id="cb16-663"><a href="#cb16-663" aria-hidden="true" tabindex="-1"></a>frame. These separate filters provide track proposals for every object</span>
<span id="cb16-664"><a href="#cb16-664" aria-hidden="true" tabindex="-1"></a>detected in the video.</span>
<span id="cb16-665"><a href="#cb16-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-666"><a href="#cb16-666" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data association using confidence regions {#sec-data_association}</span></span>
<span id="cb16-667"><a href="#cb16-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-668"><a href="#cb16-668" aria-hidden="true" tabindex="-1"></a>Throughout the video, depending on various conditions on the incoming</span>
<span id="cb16-669"><a href="#cb16-669" aria-hidden="true" tabindex="-1"></a>detections, existing trackers must be updated (with or without a new</span>
<span id="cb16-670"><a href="#cb16-670" aria-hidden="true" tabindex="-1"></a>observation) and others might need to be created. This setup requires a third</span>
<span id="cb16-671"><a href="#cb16-671" aria-hidden="true" tabindex="-1"></a>party data association block to link the incoming detections with the correct</span>
<span id="cb16-672"><a href="#cb16-672" aria-hidden="true" tabindex="-1"></a>filters.</span>
<span id="cb16-673"><a href="#cb16-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-674"><a href="#cb16-674" aria-hidden="true" tabindex="-1"></a>At the frame $n$, a set of $L_n$ Bayesian filters track previously seen</span>
<span id="cb16-675"><a href="#cb16-675" aria-hidden="true" tabindex="-1"></a>objects and a new set of detections $\mathcal{D}_n$ is provided by the</span>
<span id="cb16-676"><a href="#cb16-676" aria-hidden="true" tabindex="-1"></a>detector. Denote by $1 \leq \ell \leq L_n$ the index of each filter at time</span>
<span id="cb16-677"><a href="#cb16-677" aria-hidden="true" tabindex="-1"></a>$n$, and by convention write $Z^\ell_{1:n-1}$  the previous observed positions</span>
<span id="cb16-678"><a href="#cb16-678" aria-hidden="true" tabindex="-1"></a>associated with index $\ell$ (even if no observation is available at some past</span>
<span id="cb16-679"><a href="#cb16-679" aria-hidden="true" tabindex="-1"></a>times for that object). Let $\rho \in (0,1)$ be a confidence level.</span>
<span id="cb16-680"><a href="#cb16-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-681"><a href="#cb16-681" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>For every detected object $z_n^i \in \mathcal{D}_n$ and every filter</span>
<span id="cb16-682"><a href="#cb16-682" aria-hidden="true" tabindex="-1"></a>$\ell$, compute $P(i,\ell) = \mathbb{P}(Z_n^\ell \in V_\delta(z_n^i)\mid</span>
<span id="cb16-683"><a href="#cb16-683" aria-hidden="true" tabindex="-1"></a>Z^\ell_{1:n-1})$ where $V_\delta(z)$ is the neighborhood of $z$ defined as the</span>
<span id="cb16-684"><a href="#cb16-684" aria-hidden="true" tabindex="-1"></a>squared area of width $2\delta$ centered on $z$ (see @sec-confidence_regions_appendix for exact computations).</span>
<span id="cb16-685"><a href="#cb16-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-686"><a href="#cb16-686" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Using the Hungarian algorithm (@kuhn), compute the assignment between</span>
<span id="cb16-687"><a href="#cb16-687" aria-hidden="true" tabindex="-1"></a>detections and filters with $P$ as cost function, but discarding associations</span>
<span id="cb16-688"><a href="#cb16-688" aria-hidden="true" tabindex="-1"></a>$(i,\ell)$ having $P(i,\ell) &lt; \rho$. Formally, $\rho$ represents the level of</span>
<span id="cb16-689"><a href="#cb16-689" aria-hidden="true" tabindex="-1"></a>a confidence region centered on detections and we use $\rho = 0.5$. Denote</span>
<span id="cb16-690"><a href="#cb16-690" aria-hidden="true" tabindex="-1"></a>$a_{\rho}$ the resulting assignment map defined as $a_{\rho}(i) = \ell$ if</span>
<span id="cb16-691"><a href="#cb16-691" aria-hidden="true" tabindex="-1"></a>$z_n^i$ was associated with the $\ell$-th filter, and $a_{\rho}(i) = 0$ if</span>
<span id="cb16-692"><a href="#cb16-692" aria-hidden="true" tabindex="-1"></a>$z_n^i$ was not associated with any filter.</span>
<span id="cb16-693"><a href="#cb16-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-694"><a href="#cb16-694" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>For $1 \leq i \leq D_n$, if $a_{\rho}(i) = \ell$, use $z_n^i$ as a new observation to update the $\ell$-th filter.</span>
<span id="cb16-695"><a href="#cb16-695" aria-hidden="true" tabindex="-1"></a>If $a_{\rho}(i) = 0$, create a new filter initialized from the prior distribution, i.e.</span>
<span id="cb16-696"><a href="#cb16-696" aria-hidden="true" tabindex="-1"></a>sample the true location as a Gaussian random variable with mean $z_n^i$ and variance $R$.</span>
<span id="cb16-697"><a href="#cb16-697" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>For all filters $\ell'$ which were not provided a new observation, update only the predictive law of $X^{\ell'}_{n}$ given $Z^{\ell'}_{1:n-1}$.</span>
<span id="cb16-698"><a href="#cb16-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-699"><a href="#cb16-699" aria-hidden="true" tabindex="-1"></a>In other words, we seek to associate filters and detections by maximising a global cost built from the predictive distributions of the available filters, but an association is only valid if its corresponding predictive probability is high enough.</span>
<span id="cb16-700"><a href="#cb16-700" aria-hidden="true" tabindex="-1"></a>Though the Hungarian algorithm is a very popular algorithm in MOT, it is often used with the Euclidean distance or an Intersection-over-Union (IoU) criterion.</span>
<span id="cb16-701"><a href="#cb16-701" aria-hidden="true" tabindex="-1"></a>Using confidence regions for the distributions of $Z_n$ given $Z_{1:(n - 1)}$ instead allows to naturally include uncertainty in the decision process.</span>
<span id="cb16-702"><a href="#cb16-702" aria-hidden="true" tabindex="-1"></a>Note that we deactivate filters whose posterior mean estimates lie outside the image subspace in $\mathbb{R}^2$.</span>
<span id="cb16-703"><a href="#cb16-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-704"><a href="#cb16-704" aria-hidden="true" tabindex="-1"></a>A visual depiction of the entire pipeline (from detection to final</span>
<span id="cb16-705"><a href="#cb16-705" aria-hidden="true" tabindex="-1"></a>association) is provided below. This way of combining a set of Bayesian</span>
<span id="cb16-706"><a href="#cb16-706" aria-hidden="true" tabindex="-1"></a>filters with a data association step that resorts on the most likely</span>
<span id="cb16-707"><a href="#cb16-707" aria-hidden="true" tabindex="-1"></a>hypothesis is a form of Global Nearest Neighbor (GNN) tracking. Another</span>
<span id="cb16-708"><a href="#cb16-708" aria-hidden="true" tabindex="-1"></a>possibility is to perform multi-target filtering by including the data</span>
<span id="cb16-709"><a href="#cb16-709" aria-hidden="true" tabindex="-1"></a>association step directly into the probabilistic model, as in</span>
<span id="cb16-710"><a href="#cb16-710" aria-hidden="true" tabindex="-1"></a>@mahler2003. A generalisation of single-target recursive Bayesian</span>
<span id="cb16-711"><a href="#cb16-711" aria-hidden="true" tabindex="-1"></a>filtering, this class of methods is grounded in the point process literature</span>
<span id="cb16-712"><a href="#cb16-712" aria-hidden="true" tabindex="-1"></a>and well motivated theoretically. In case of strong false positive detection</span>
<span id="cb16-713"><a href="#cb16-713" aria-hidden="true" tabindex="-1"></a>rates, close and/or reappearing objects, practical benefits may be obtained</span>
<span id="cb16-714"><a href="#cb16-714" aria-hidden="true" tabindex="-1"></a>from these solutions. Finally, note that another well-motivated choice for</span>
<span id="cb16-715"><a href="#cb16-715" aria-hidden="true" tabindex="-1"></a>$P(i,\ell)$ could be to use the marginal likelihood $\mathbb{P}(Z_n^\ell \in</span>
<span id="cb16-716"><a href="#cb16-716" aria-hidden="true" tabindex="-1"></a>V_\delta(z_n^i))$, which is standard in modern MOT. </span>
<span id="cb16-717"><a href="#cb16-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-718"><a href="#cb16-718" aria-hidden="true" tabindex="-1"></a>::: {#fig-diagram}  </span>
<span id="cb16-719"><a href="#cb16-719" aria-hidden="true" tabindex="-1"></a><span class="al">![](figures/diagram.png)</span></span>
<span id="cb16-720"><a href="#cb16-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-721"><a href="#cb16-721" aria-hidden="true" tabindex="-1"></a>Visual representation of the tracking pipeline.</span>
<span id="cb16-722"><a href="#cb16-722" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-723"><a href="#cb16-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-724"><a href="#cb16-724" aria-hidden="true" tabindex="-1"></a><span class="fu">## Counting</span></span>
<span id="cb16-725"><a href="#cb16-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-726"><a href="#cb16-726" aria-hidden="true" tabindex="-1"></a>At the end of the video, the previous process returns a set of candidate</span>
<span id="cb16-727"><a href="#cb16-727" aria-hidden="true" tabindex="-1"></a>tracks. For counting purposes, we find that simple heuristics can be further</span>
<span id="cb16-728"><a href="#cb16-728" aria-hidden="true" tabindex="-1"></a>applied to filter out tracks that do not follow actual objects. More</span>
<span id="cb16-729"><a href="#cb16-729" aria-hidden="true" tabindex="-1"></a>precisely, we observe that tracks of real objects usually contain more (i)</span>
<span id="cb16-730"><a href="#cb16-730" aria-hidden="true" tabindex="-1"></a>observations and (ii) streams of uninterrupted observations. Denote by $T_\ell</span>
<span id="cb16-731"><a href="#cb16-731" aria-hidden="true" tabindex="-1"></a>= \left<span class="sc">\{</span>n \in \mathbb{N} \mid \exists  z \in \mathcal{D}_n,  Z_n^{\ell} =</span>
<span id="cb16-732"><a href="#cb16-732" aria-hidden="true" tabindex="-1"></a>z\right<span class="sc">\}</span>$ all timesteps where the $\ell$-th object is observed. To discard</span>
<span id="cb16-733"><a href="#cb16-733" aria-hidden="true" tabindex="-1"></a>false counts according to (i) and (ii), we compute the moving average</span>
<span id="cb16-734"><a href="#cb16-734" aria-hidden="true" tabindex="-1"></a>$M_\ell^\kappa$ of $1_{T_\ell}$ using windows of size $\kappa$, i.e. the</span>
<span id="cb16-735"><a href="#cb16-735" aria-hidden="true" tabindex="-1"></a>sequence defined by $M_\ell^\kappa<span class="co">[</span><span class="ot">n</span><span class="co">]</span> = \frac{1}{\kappa} \sum_{k \in [<span class="sc">\!</span>[n -</span>
<span id="cb16-736"><a href="#cb16-736" aria-hidden="true" tabindex="-1"></a>\kappa, n + \kappa]<span class="sc">\!</span>]} 1_{T_\ell}<span class="co">[</span><span class="ot">k</span><span class="co">]</span>$. We then build $T_\ell^\kappa =</span>
<span id="cb16-737"><a href="#cb16-737" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span>n \in T_\ell \mid M_\ell^\kappa<span class="co">[</span><span class="ot">n</span><span class="co">]</span> &gt; \nu\right<span class="sc">\}</span>$, and defining</span>
<span id="cb16-738"><a href="#cb16-738" aria-hidden="true" tabindex="-1"></a>$\mathcal{N} = \left<span class="sc">\{</span>\ell \mid |T_\ell^\kappa| &gt; \tau\right<span class="sc">\}</span>$, the final</span>
<span id="cb16-739"><a href="#cb16-739" aria-hidden="true" tabindex="-1"></a>object count is $|\mathcal{N}|$. We choose $\nu = 0.6$ while $\kappa,\tau$ are</span>
<span id="cb16-740"><a href="#cb16-740" aria-hidden="true" tabindex="-1"></a>optimized for best count performance (see @sec-tau_kappa_appendix for a</span>
<span id="cb16-741"><a href="#cb16-741" aria-hidden="true" tabindex="-1"></a>more comprehensive study).</span>
<span id="cb16-742"><a href="#cb16-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-743"><a href="#cb16-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-744"><a href="#cb16-744" aria-hidden="true" tabindex="-1"></a><span class="fu"># Metrics for MOT-based counting</span></span>
<span id="cb16-745"><a href="#cb16-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-746"><a href="#cb16-746" aria-hidden="true" tabindex="-1"></a>Counting in videos using embedded moving cameras is not a common task, and as</span>
<span id="cb16-747"><a href="#cb16-747" aria-hidden="true" tabindex="-1"></a>such it requires a specific evaluation protocol to understand and compare the</span>
<span id="cb16-748"><a href="#cb16-748" aria-hidden="true" tabindex="-1"></a>performance of competing methods. First, not all MOT metrics are relevant,</span>
<span id="cb16-749"><a href="#cb16-749" aria-hidden="true" tabindex="-1"></a>even if some do provide insights to assist evaluation of count performance.</span>
<span id="cb16-750"><a href="#cb16-750" aria-hidden="true" tabindex="-1"></a>Second, considering only raw counts on long videos gives little information on</span>
<span id="cb16-751"><a href="#cb16-751" aria-hidden="true" tabindex="-1"></a>which of the final counts effectively arise from well detected objects.</span>
<span id="cb16-752"><a href="#cb16-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-753"><a href="#cb16-753" aria-hidden="true" tabindex="-1"></a><span class="fu">## Count-related MOT metrics</span></span>
<span id="cb16-754"><a href="#cb16-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-755"><a href="#cb16-755" aria-hidden="true" tabindex="-1"></a>Popular MOT benchmarks usually report several sets of metrics such as ClearMOT</span>
<span id="cb16-756"><a href="#cb16-756" aria-hidden="true" tabindex="-1"></a>(@bernardin2008) or IDF1 (@RistaniSZCT16) which can account for</span>
<span id="cb16-757"><a href="#cb16-757" aria-hidden="true" tabindex="-1"></a>different components of tracking performance. Recently, (@luiten2020)</span>
<span id="cb16-758"><a href="#cb16-758" aria-hidden="true" tabindex="-1"></a>built the so-called HOTA metrics that allow separate evaluation of detection</span>
<span id="cb16-759"><a href="#cb16-759" aria-hidden="true" tabindex="-1"></a>and association using the Jaccard index. The following components of their</span>
<span id="cb16-760"><a href="#cb16-760" aria-hidden="true" tabindex="-1"></a>work are relevant to our task (we provide equation numbers in the original</span>
<span id="cb16-761"><a href="#cb16-761" aria-hidden="true" tabindex="-1"></a>paper for formal definitions).</span>
<span id="cb16-762"><a href="#cb16-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-763"><a href="#cb16-763" aria-hidden="true" tabindex="-1"></a><span class="fu">### Detection</span></span>
<span id="cb16-764"><a href="#cb16-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-765"><a href="#cb16-765" aria-hidden="true" tabindex="-1"></a>First, when considering all frames independently, traditional detection recall</span>
<span id="cb16-766"><a href="#cb16-766" aria-hidden="true" tabindex="-1"></a>($\mathsf{DetRe}$) and precision ($\mathsf{DetPr}$) can be computed to assess the capabilities</span>
<span id="cb16-767"><a href="#cb16-767" aria-hidden="true" tabindex="-1"></a>of the object detector. Denoting with $\mathsf{TP}_n$, $\mathsf{FP}_n$, $\mathsf{FN}_n$ the number of</span>
<span id="cb16-768"><a href="#cb16-768" aria-hidden="true" tabindex="-1"></a>true positive, false positive and false negative detections at frame $n$,</span>
<span id="cb16-769"><a href="#cb16-769" aria-hidden="true" tabindex="-1"></a>respectively, we define $\mathsf{TP} = \sum_n \mathsf{TP}_n$, $\mathsf{FP} = \sum_n \mathsf{FP}_n$ and $\mathsf{FN} =</span>
<span id="cb16-770"><a href="#cb16-770" aria-hidden="true" tabindex="-1"></a>\sum_n \mathsf{FN}_n$, then:</span>
<span id="cb16-771"><a href="#cb16-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-772"><a href="#cb16-772" aria-hidden="true" tabindex="-1"></a>$$\mathsf{DetRe} = \frac{\mathsf{TP}}{\mathsf{TP} + \mathsf{FN}},$$</span>
<span id="cb16-773"><a href="#cb16-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-774"><a href="#cb16-774" aria-hidden="true" tabindex="-1"></a>$$\mathsf{DetPr} = \frac{\mathsf{TP}}{\mathsf{TP} + \mathsf{FP}}.$$</span>
<span id="cb16-775"><a href="#cb16-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-776"><a href="#cb16-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-777"><a href="#cb16-777" aria-hidden="true" tabindex="-1"></a>In classical object detection, those metrics are the main target.</span>
<span id="cb16-778"><a href="#cb16-778" aria-hidden="true" tabindex="-1"></a>In our context, as the first step of the system, this framewise performance impacts the difficulty of counting.</span>
<span id="cb16-779"><a href="#cb16-779" aria-hidden="true" tabindex="-1"></a>However, we must keep in mind that these metrics are computed framewise and might not guarantee anything at a video scale.</span>
<span id="cb16-780"><a href="#cb16-780" aria-hidden="true" tabindex="-1"></a>The next points illustrate that remark.</span>
<span id="cb16-781"><a href="#cb16-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-782"><a href="#cb16-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-783"><a href="#cb16-783" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>If both $\mathsf{DetRe}$ and $\mathsf{DetPr}$ are very high, objects are detected at nearly all frames and most detections come from actual objects.</span>
<span id="cb16-784"><a href="#cb16-784" aria-hidden="true" tabindex="-1"></a>Therefore, robustness to missing observations is high, but even in this context computing associations may fail if camera movements are nontrivial.</span>
<span id="cb16-785"><a href="#cb16-785" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For an ideal tracking algorithm which never counts individual objects twice and does not confuse separate objects in a video, a detector capturing each object for only one frame could theoretically be used.</span>
<span id="cb16-786"><a href="#cb16-786" aria-hidden="true" tabindex="-1"></a>Thus, low $\mathsf{DetRe}$ could theoretically be compensated with robust tracking.</span>
<span id="cb16-787"><a href="#cb16-787" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>If our approach can rule out faulty tracks which do not follow actual objects, then good counts can still be obtained using a detector generating many false positives.</span>
<span id="cb16-788"><a href="#cb16-788" aria-hidden="true" tabindex="-1"></a>Again, this suggests that low $\mathsf{DetPr}$ may allow decent counting performance.</span>
<span id="cb16-789"><a href="#cb16-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-790"><a href="#cb16-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-791"><a href="#cb16-791" aria-hidden="true" tabindex="-1"></a><span class="fu">### Association</span></span>
<span id="cb16-792"><a href="#cb16-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-793"><a href="#cb16-793" aria-hidden="true" tabindex="-1"></a>HOTA association metrics are built to measure tracking performance</span>
<span id="cb16-794"><a href="#cb16-794" aria-hidden="true" tabindex="-1"></a>irrespective of the detection capabilities, by comparing predicted tracks</span>
<span id="cb16-795"><a href="#cb16-795" aria-hidden="true" tabindex="-1"></a>against true object trajectories. In our experiments, we compute the</span>
<span id="cb16-796"><a href="#cb16-796" aria-hidden="true" tabindex="-1"></a>Association Recall ($\mathsf{AssRe}$) and the Association Precision ($\mathsf{AssPr}$).</span>
<span id="cb16-797"><a href="#cb16-797" aria-hidden="true" tabindex="-1"></a>Several intermediate quantities are necessary to introduce these final</span>
<span id="cb16-798"><a href="#cb16-798" aria-hidden="true" tabindex="-1"></a>metrics. Following @luiten2020, we denote with $\mathsf{prID}$ the ID of a predicted</span>
<span id="cb16-799"><a href="#cb16-799" aria-hidden="true" tabindex="-1"></a>track and $\mathsf{gtID}$ the ID of a ground truth track. Given $C$ all couples of</span>
<span id="cb16-800"><a href="#cb16-800" aria-hidden="true" tabindex="-1"></a>$\mathsf{prID}-\mathsf{gtID}$ found among the true positive detections, and $c \in C$ one of</span>
<span id="cb16-801"><a href="#cb16-801" aria-hidden="true" tabindex="-1"></a>these couples, $\mathsf{TPA}(c)$ is the number of frames where $\mathsf{prID}$ is also</span>
<span id="cb16-802"><a href="#cb16-802" aria-hidden="true" tabindex="-1"></a>associated with $\mathsf{gtID}$, $\mathsf{FPA}(c)$ is the number of frames where $\mathsf{prID}$ is</span>
<span id="cb16-803"><a href="#cb16-803" aria-hidden="true" tabindex="-1"></a>associated with another ground truth ID or with no ground truth ID, and</span>
<span id="cb16-804"><a href="#cb16-804" aria-hidden="true" tabindex="-1"></a>$\mathsf{FNA}(c)$ is the number of frames where $\mathsf{gtID}$ is associated with another</span>
<span id="cb16-805"><a href="#cb16-805" aria-hidden="true" tabindex="-1"></a>predicted ID or with no predicted ID. Then:</span>
<span id="cb16-806"><a href="#cb16-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-807"><a href="#cb16-807" aria-hidden="true" tabindex="-1"></a>$$\mathsf{AssPr} = \frac{1}{\mathsf{TP}} \sum_{c \in C} \frac{\mathsf{TPA}(c)}{\mathsf{TPA}(c) + \mathsf{FPA}(c)},$$</span>
<span id="cb16-808"><a href="#cb16-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-809"><a href="#cb16-809" aria-hidden="true" tabindex="-1"></a>$$\mathsf{AssRe} = \frac{1}{\mathsf{TP}} \sum_{c \in C} \frac{\mathsf{TPA}(c)}{\mathsf{TPA}(c) + \mathsf{FNA}(c)}.$$</span>
<span id="cb16-810"><a href="#cb16-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-811"><a href="#cb16-811" aria-hidden="true" tabindex="-1"></a>See @luiten2020 (fig. 2) for a clear illustration of these quantities. </span>
<span id="cb16-812"><a href="#cb16-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-813"><a href="#cb16-813" aria-hidden="true" tabindex="-1"></a>In brief, a low $\mathsf{AssPr}$ implies that several objects are often mingled into only one track, resulting in undercount.</span>
<span id="cb16-814"><a href="#cb16-814" aria-hidden="true" tabindex="-1"></a>A low $\mathsf{AssRe}$ implies that single objects are often associated with multiple tracks.</span>
<span id="cb16-815"><a href="#cb16-815" aria-hidden="true" tabindex="-1"></a>If no method is used to discard redundant tracks this results in overcount.</span>
<span id="cb16-816"><a href="#cb16-816" aria-hidden="true" tabindex="-1"></a>Conversely, association precision ($\mathsf{AssPr}$) measures how exclusive tracks are to each object (it decreases whenever a track covers multiple objects).</span>
<span id="cb16-817"><a href="#cb16-817" aria-hidden="true" tabindex="-1"></a>Again, it is useful to reconsider and illustrate the meaning of these metrics in the context of MOT-based counting.</span>
<span id="cb16-818"><a href="#cb16-818" aria-hidden="true" tabindex="-1"></a>Litter items are typically well separated on river banks, thus predicted tracks are not expected to interfere much.</span>
<span id="cb16-819"><a href="#cb16-819" aria-hidden="true" tabindex="-1"></a>This suggests that reaching high $\mathsf{AssPr}$ on our footage is not challenging.</span>
<span id="cb16-820"><a href="#cb16-820" aria-hidden="true" tabindex="-1"></a>Contrarily, $\mathsf{AssRe}$ is a direct measurement of the capability of the tracker to avoid producing multiple tracks despite missing detections and challenging motion.</span>
<span id="cb16-821"><a href="#cb16-821" aria-hidden="true" tabindex="-1"></a>A high $\mathsf{AssRe}$ therefore typically avoids multiple counts for the same object, which is a key aspect of our work.</span>
<span id="cb16-822"><a href="#cb16-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-823"><a href="#cb16-823" aria-hidden="true" tabindex="-1"></a>Nonetheless, association metrics are only computed for predicted tracks which can effectively be matched with ground truth tracks.</span>
<span id="cb16-824"><a href="#cb16-824" aria-hidden="true" tabindex="-1"></a>Consequently, $\mathsf{AssRe}$ does not account for tracks predicted from streams of false positive detections generated by the detector (e.g.</span>
<span id="cb16-825"><a href="#cb16-825" aria-hidden="true" tabindex="-1"></a>arising from rocks, water reflections, etc).</span>
<span id="cb16-826"><a href="#cb16-826" aria-hidden="true" tabindex="-1"></a>Since such tracks induce false counts, a tracker which produces the fewest is better, but MOT metrics do not measure it.</span>
<span id="cb16-827"><a href="#cb16-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-828"><a href="#cb16-828" aria-hidden="true" tabindex="-1"></a><span class="fu">## Count metrics</span></span>
<span id="cb16-829"><a href="#cb16-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-830"><a href="#cb16-830" aria-hidden="true" tabindex="-1"></a>Denoting by $\mathsf{\hat{N}}$ and $\mathsf{N}$ the respective predicted and ground truth</span>
<span id="cb16-831"><a href="#cb16-831" aria-hidden="true" tabindex="-1"></a>counts for the validation material, the error $\mathsf{\hat{N}} - \mathsf{N}$ is misleading as</span>
<span id="cb16-832"><a href="#cb16-832" aria-hidden="true" tabindex="-1"></a>no information is provided on the quality of the predicted counts.</span>
<span id="cb16-833"><a href="#cb16-833" aria-hidden="true" tabindex="-1"></a>Additionally, results on the original validation footage do not measure the</span>
<span id="cb16-834"><a href="#cb16-834" aria-hidden="true" tabindex="-1"></a>statistical variability of the proposed estimators.</span>
<span id="cb16-835"><a href="#cb16-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-836"><a href="#cb16-836" aria-hidden="true" tabindex="-1"></a><span class="fu">### Count decomposition</span></span>
<span id="cb16-837"><a href="#cb16-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-838"><a href="#cb16-838" aria-hidden="true" tabindex="-1"></a>Define $i \in <span class="co">[</span><span class="ot">\![1, \mathsf{N}]\!</span><span class="co">]</span>$ and $j \in <span class="co">[</span><span class="ot">\![1, \mathsf{\hat{N}}]\!</span><span class="co">]</span>$ the labels of the</span>
<span id="cb16-839"><a href="#cb16-839" aria-hidden="true" tabindex="-1"></a>annotated ground truth tracks and the predicted tracks, respectively. At</span>
<span id="cb16-840"><a href="#cb16-840" aria-hidden="true" tabindex="-1"></a>evaluation, we assign each predicted track to either none or at most one</span>
<span id="cb16-841"><a href="#cb16-841" aria-hidden="true" tabindex="-1"></a>ground truth track, writing $j \rightarrow \emptyset$ or $j \rightarrow i$ for</span>
<span id="cb16-842"><a href="#cb16-842" aria-hidden="true" tabindex="-1"></a>the corresponding assignments. The association is made whenever a predicted</span>
<span id="cb16-843"><a href="#cb16-843" aria-hidden="true" tabindex="-1"></a>track $i$ overlaps with a ground truth track $j$ at any frame, i.e. for a</span>
<span id="cb16-844"><a href="#cb16-844" aria-hidden="true" tabindex="-1"></a>given frame a detection in $i$ is within a threshold $\alpha$ of an object in</span>
<span id="cb16-845"><a href="#cb16-845" aria-hidden="true" tabindex="-1"></a>$j$. We compute metrics for 20 values of $\alpha \in [0.05 \alpha_{max}, 0.95</span>
<span id="cb16-846"><a href="#cb16-846" aria-hidden="true" tabindex="-1"></a>\alpha_{max}]$, with $\alpha_{max} = 0.1 \sqrt{w^2 + h^2}$, then average the</span>
<span id="cb16-847"><a href="#cb16-847" aria-hidden="true" tabindex="-1"></a>results, which is the default method in HOTA to combine results at different</span>
<span id="cb16-848"><a href="#cb16-848" aria-hidden="true" tabindex="-1"></a>thresholds. We keep this default solution, in particular because our results</span>
<span id="cb16-849"><a href="#cb16-849" aria-hidden="true" tabindex="-1"></a>are very consistent accross different thresholds in that range (we only</span>
<span id="cb16-850"><a href="#cb16-850" aria-hidden="true" tabindex="-1"></a>observe a slight decrease in performance for $\alpha = \alpha_{max}$, where</span>
<span id="cb16-851"><a href="#cb16-851" aria-hidden="true" tabindex="-1"></a>occasional false detections probably start to lie below the threshold). </span>
<span id="cb16-852"><a href="#cb16-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-853"><a href="#cb16-853" aria-hidden="true" tabindex="-1"></a>Denote $A_i = <span class="sc">\{</span>j \in <span class="co">[</span><span class="ot">\![1, \mathsf{\hat{N}}]\!</span><span class="co">]</span> \mid j \rightarrow i<span class="sc">\}</span>$ the set of predicted tracks assigned to the $i$-th ground truth track.</span>
<span id="cb16-854"><a href="#cb16-854" aria-hidden="true" tabindex="-1"></a>We define:</span>
<span id="cb16-855"><a href="#cb16-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-856"><a href="#cb16-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-857"><a href="#cb16-857" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathsf{\hat{N}_{true}} = \sum_{i=1}^{\mathsf{N}} 1_{|A_i| &gt; 0}$ the number of ground truth objects successfully counted.</span>
<span id="cb16-858"><a href="#cb16-858" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\mathsf{\hat{N}_{red}} = \sum_{i=1}^{\mathsf{N}} |A_i| - \mathsf{\hat{N}_{true}}$ the number of redundant counts per ground truth object.</span>
<span id="cb16-859"><a href="#cb16-859" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\mathsf{\hat{N}_{mis}} = \mathsf{N} - \mathsf{\hat{N}_{true}}$ the number of ground truth objects that are never effectively counted.</span>
<span id="cb16-860"><a href="#cb16-860" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>$\mathsf{\hat{N}_{false}} = \sum_{j=1}^{\mathsf{\hat{N}}} 1_{j \rightarrow \emptyset}$ the number of counts which cannot be associated with any ground truth object and are therefore considered as false counts.</span>
<span id="cb16-861"><a href="#cb16-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-862"><a href="#cb16-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-863"><a href="#cb16-863" aria-hidden="true" tabindex="-1"></a>Using these metrics provides a much better understanding of $\mathsf{\hat{N}}$ as </span>
<span id="cb16-864"><a href="#cb16-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-865"><a href="#cb16-865" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-866"><a href="#cb16-866" aria-hidden="true" tabindex="-1"></a>\mathsf{\hat{N}} = \mathsf{\hat{N}_{true}} + \mathsf{\hat{N}_{red}} + \mathsf{\hat{N}_{false}},</span>
<span id="cb16-867"><a href="#cb16-867" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-868"><a href="#cb16-868" aria-hidden="true" tabindex="-1"></a>while $\mathsf{\hat{N}_{mis}}$ completely summarises the number of undetected objects.</span>
<span id="cb16-869"><a href="#cb16-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-870"><a href="#cb16-870" aria-hidden="true" tabindex="-1"></a>Conveniently, the quantities can be used to define the count precision ($\mathsf{CountPR}$) and count recall ($\mathsf{CountRe}$) as follows: </span>
<span id="cb16-871"><a href="#cb16-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-872"><a href="#cb16-872" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-873"><a href="#cb16-873" aria-hidden="true" tabindex="-1"></a>\mathsf{CountPR} = \frac{\mathsf{\hat{N}_{true}}}{\mathsf{\hat{N}_{true}} + \mathsf{\hat{N}_{red}} + \mathsf{\hat{N}_{false}}},</span>
<span id="cb16-874"><a href="#cb16-874" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-875"><a href="#cb16-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-876"><a href="#cb16-876" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-877"><a href="#cb16-877" aria-hidden="true" tabindex="-1"></a>\mathsf{CountRe} = \frac{\mathsf{\hat{N}_{true}}}{\mathsf{\hat{N}_{true}} + \mathsf{\hat{N}_{mis}}},</span>
<span id="cb16-878"><a href="#cb16-878" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-879"><a href="#cb16-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-880"><a href="#cb16-880" aria-hidden="true" tabindex="-1"></a>which provide good summaries for the overall count quality, letting aside the tracking performance. </span>
<span id="cb16-881"><a href="#cb16-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-882"><a href="#cb16-882" aria-hidden="true" tabindex="-1"></a>Note that these metrics and the associated decomposition are only defined if</span>
<span id="cb16-883"><a href="#cb16-883" aria-hidden="true" tabindex="-1"></a>the previous assignment between predicted and ground truth tracks can be</span>
<span id="cb16-884"><a href="#cb16-884" aria-hidden="true" tabindex="-1"></a>obtained. In our case, predicted tracks never overlap with several ground</span>
<span id="cb16-885"><a href="#cb16-885" aria-hidden="true" tabindex="-1"></a>truth tracks (because true objects are well separated), and therefore this</span>
<span id="cb16-886"><a href="#cb16-886" aria-hidden="true" tabindex="-1"></a>assignment is straightforward. More involved metrics have been studied at the</span>
<span id="cb16-887"><a href="#cb16-887" aria-hidden="true" tabindex="-1"></a>trajectory level (see for example @garcia2020 and the references</span>
<span id="cb16-888"><a href="#cb16-888" aria-hidden="true" tabindex="-1"></a>therein), though not specifically tailored to the restricted task of counting.</span>
<span id="cb16-889"><a href="#cb16-889" aria-hidden="true" tabindex="-1"></a>For more complicated data, an adaptation of such contributions into proper</span>
<span id="cb16-890"><a href="#cb16-890" aria-hidden="true" tabindex="-1"></a>counting metrics could be valuable. </span>
<span id="cb16-891"><a href="#cb16-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-892"><a href="#cb16-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-893"><a href="#cb16-893" aria-hidden="true" tabindex="-1"></a><span class="fu">### Statistics</span></span>
<span id="cb16-894"><a href="#cb16-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-895"><a href="#cb16-895" aria-hidden="true" tabindex="-1"></a>Since the original validation set comprises only a few unequally long videos,</span>
<span id="cb16-896"><a href="#cb16-896" aria-hidden="true" tabindex="-1"></a>only absolute results are available. Splitting the original sequences into</span>
<span id="cb16-897"><a href="#cb16-897" aria-hidden="true" tabindex="-1"></a>shorter independent sequences of equal length allows to compute basic</span>
<span id="cb16-898"><a href="#cb16-898" aria-hidden="true" tabindex="-1"></a>statistics. For any quantity $\mathsf{\hat{N}}_\bullet$ defined above, we provide</span>
<span id="cb16-899"><a href="#cb16-899" aria-hidden="true" tabindex="-1"></a>$\hat{\sigma}_{\mathsf{\hat{N}}_\bullet}$ the associated empirical standard deviations</span>
<span id="cb16-900"><a href="#cb16-900" aria-hidden="true" tabindex="-1"></a>computed on the set of short sequences.</span>
<span id="cb16-901"><a href="#cb16-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-902"><a href="#cb16-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-903"><a href="#cb16-903" aria-hidden="true" tabindex="-1"></a><span class="fu"># Experiments</span></span>
<span id="cb16-904"><a href="#cb16-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-905"><a href="#cb16-905" aria-hidden="true" tabindex="-1"></a>We denote by $S_1$, $S_2$ and $S_3$ the three river sections of the evaluation material and split the associated footage into independent segments of 30 seconds. We further divide this material into two distinct validation (6min30) and test (7min) splits. </span>
<span id="cb16-906"><a href="#cb16-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-907"><a href="#cb16-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-908"><a href="#cb16-908" aria-hidden="true" tabindex="-1"></a>To demonstrate the benefits of our work, we select two multi-object trackers</span>
<span id="cb16-909"><a href="#cb16-909" aria-hidden="true" tabindex="-1"></a>and build competing counting systems from them. Our first choice is SORT</span>
<span id="cb16-910"><a href="#cb16-910" aria-hidden="true" tabindex="-1"></a>@Bewley2016, which relies on Kalman filtering with velocity updated using the</span>
<span id="cb16-911"><a href="#cb16-911" aria-hidden="true" tabindex="-1"></a>latest past estimates of object positions. Similar to our system, it only</span>
<span id="cb16-912"><a href="#cb16-912" aria-hidden="true" tabindex="-1"></a>relies on image supervision for training, and though DeepSORT</span>
<span id="cb16-913"><a href="#cb16-913" aria-hidden="true" tabindex="-1"></a>(@Wojke2018) is a more recent alternative with better performance, the</span>
<span id="cb16-914"><a href="#cb16-914" aria-hidden="true" tabindex="-1"></a>associated deep appearance network cannot be used without additional video</span>
<span id="cb16-915"><a href="#cb16-915" aria-hidden="true" tabindex="-1"></a>annotations. FairMOT (@Zhanga), a more recent alternative, is similarly</span>
<span id="cb16-916"><a href="#cb16-916" aria-hidden="true" tabindex="-1"></a>intended for use with video supervision but allows self-supervised training</span>
<span id="cb16-917"><a href="#cb16-917" aria-hidden="true" tabindex="-1"></a>using only an image dataset. Built as a new baseline for MOT, it combines</span>
<span id="cb16-918"><a href="#cb16-918" aria-hidden="true" tabindex="-1"></a>linear constant-velocity Kalman filtering with visual features computed by an</span>
<span id="cb16-919"><a href="#cb16-919" aria-hidden="true" tabindex="-1"></a>additional network branch and extracted at the position of the estimated</span>
<span id="cb16-920"><a href="#cb16-920" aria-hidden="true" tabindex="-1"></a>object centers, as introduced in CenterTrack (@zhou2020). We choose</span>
<span id="cb16-921"><a href="#cb16-921" aria-hidden="true" tabindex="-1"></a>FairMOT to compare our method to a solution based on deep visual feature</span>
<span id="cb16-922"><a href="#cb16-922" aria-hidden="true" tabindex="-1"></a>extraction.</span>
<span id="cb16-923"><a href="#cb16-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-924"><a href="#cb16-924" aria-hidden="true" tabindex="-1"></a>Similar to our work, FairMOT uses CenterNet for the detection part and the</span>
<span id="cb16-925"><a href="#cb16-925" aria-hidden="true" tabindex="-1"></a>latter is therefore trained as in @sec-detector_training. We train it using</span>
<span id="cb16-926"><a href="#cb16-926" aria-hidden="true" tabindex="-1"></a>hyperparameters from the original paper. The detection outputs are then shared</span>
<span id="cb16-927"><a href="#cb16-927" aria-hidden="true" tabindex="-1"></a>between all counting methods, allowing fair comparison of counting performance</span>
<span id="cb16-928"><a href="#cb16-928" aria-hidden="true" tabindex="-1"></a>given a fixed object detector. We run all experiments at 12fps, an</span>
<span id="cb16-929"><a href="#cb16-929" aria-hidden="true" tabindex="-1"></a>intermediate framerate to capture all objects while reducing the computational</span>
<span id="cb16-930"><a href="#cb16-930" aria-hidden="true" tabindex="-1"></a>burden.</span>
<span id="cb16-931"><a href="#cb16-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-932"><a href="#cb16-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-933"><a href="#cb16-933" aria-hidden="true" tabindex="-1"></a><span class="fu">## Detection</span></span>
<span id="cb16-934"><a href="#cb16-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-935"><a href="#cb16-935" aria-hidden="true" tabindex="-1"></a>In the following section, we present the performance of the trained detector.</span>
<span id="cb16-936"><a href="#cb16-936" aria-hidden="true" tabindex="-1"></a>Having annotated all frames of the evaluation videos, we directly compute</span>
<span id="cb16-937"><a href="#cb16-937" aria-hidden="true" tabindex="-1"></a>$\mathsf{DetRe}$ and $\mathsf{DetPr}$ on those instead of a test split of the image dataset</span>
<span id="cb16-938"><a href="#cb16-938" aria-hidden="true" tabindex="-1"></a>used for training. This allows realistic assessment of the detection quality</span>
<span id="cb16-939"><a href="#cb16-939" aria-hidden="true" tabindex="-1"></a>of our system on true videos that may include blurry frames or artifacts</span>
<span id="cb16-940"><a href="#cb16-940" aria-hidden="true" tabindex="-1"></a>caused by strong motion. We observe low $\mathsf{DetRe}$, suggesting that objects are</span>
<span id="cb16-941"><a href="#cb16-941" aria-hidden="true" tabindex="-1"></a>only captured on a fraction of the frames they appear on. To better focus on</span>
<span id="cb16-942"><a href="#cb16-942" aria-hidden="true" tabindex="-1"></a>count performance in the next sections, we remove segments that do not</span>
<span id="cb16-943"><a href="#cb16-943" aria-hidden="true" tabindex="-1"></a>generate any correct detection: performance on the remaining footage is</span>
<span id="cb16-944"><a href="#cb16-944" aria-hidden="true" tabindex="-1"></a>increased and given by $\mathsf{DetRe}^{*}$ and $\mathsf{DetPr}^{*}$.</span>
<span id="cb16-945"><a href="#cb16-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-948"><a href="#cb16-948" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-949"><a href="#cb16-949" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb16-950"><a href="#cb16-950" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-951"><a href="#cb16-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-952"><a href="#cb16-952" aria-hidden="true" tabindex="-1"></a>fps <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb16-953"><a href="#cb16-953" aria-hidden="true" tabindex="-1"></a>fps <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>fps<span class="sc">}</span><span class="ss">fps'</span></span>
<span id="cb16-954"><a href="#cb16-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-955"><a href="#cb16-955" aria-hidden="true" tabindex="-1"></a>split <span class="op">=</span> <span class="st">'test'</span></span>
<span id="cb16-956"><a href="#cb16-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-957"><a href="#cb16-957" aria-hidden="true" tabindex="-1"></a>long_segments_names <span class="op">=</span> [<span class="st">'part_1_1'</span>,</span>
<span id="cb16-958"><a href="#cb16-958" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'part_1_2'</span>,</span>
<span id="cb16-959"><a href="#cb16-959" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'part_2'</span>,</span>
<span id="cb16-960"><a href="#cb16-960" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'part_3'</span>]</span>
<span id="cb16-961"><a href="#cb16-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-962"><a href="#cb16-962" aria-hidden="true" tabindex="-1"></a>indices_test <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">7</span>,<span class="dv">9</span>,<span class="dv">13</span>]</span>
<span id="cb16-963"><a href="#cb16-963" aria-hidden="true" tabindex="-1"></a>indices_val <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">14</span>]</span>
<span id="cb16-964"><a href="#cb16-964" aria-hidden="true" tabindex="-1"></a>indices_det <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">17</span>,<span class="dv">24</span>,<span class="dv">38</span>]</span>
<span id="cb16-965"><a href="#cb16-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-966"><a href="#cb16-966" aria-hidden="true" tabindex="-1"></a>alpha_type <span class="op">=</span> <span class="st">'___50'</span></span>
<span id="cb16-967"><a href="#cb16-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-968"><a href="#cb16-968" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_split(split):</span>
<span id="cb16-969"><a href="#cb16-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-970"><a href="#cb16-970" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> split <span class="op">==</span> <span class="st">'val'</span>:</span>
<span id="cb16-971"><a href="#cb16-971" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> indices_val</span>
<span id="cb16-972"><a href="#cb16-972" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> split <span class="op">==</span> <span class="st">'test'</span>:</span>
<span id="cb16-973"><a href="#cb16-973" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> indices_test</span>
<span id="cb16-974"><a href="#cb16-974" aria-hidden="true" tabindex="-1"></a>    gt_dir_short <span class="op">=</span> <span class="ss">f'TrackEval/data/gt/surfrider_short_segments_</span><span class="sc">{</span>fps<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb16-975"><a href="#cb16-975" aria-hidden="true" tabindex="-1"></a>    eval_dir_short <span class="op">=</span> <span class="ss">f'TrackEval/data/trackers/surfrider_short_segments_</span><span class="sc">{</span>fps<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb16-976"><a href="#cb16-976" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> split <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: </span>
<span id="cb16-977"><a href="#cb16-977" aria-hidden="true" tabindex="-1"></a>        gt_dir_short <span class="op">+=</span> <span class="ss">f'_</span><span class="sc">{</span>split<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb16-978"><a href="#cb16-978" aria-hidden="true" tabindex="-1"></a>        eval_dir_short <span class="op">+=</span> <span class="ss">f'_</span><span class="sc">{</span>split<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb16-979"><a href="#cb16-979" aria-hidden="true" tabindex="-1"></a>    gt_dir_short <span class="op">+=</span> <span class="st">'/surfrider-test'</span></span>
<span id="cb16-980"><a href="#cb16-980" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> indices, eval_dir_short, gt_dir_short</span>
<span id="cb16-981"><a href="#cb16-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-982"><a href="#cb16-982" aria-hidden="true" tabindex="-1"></a>indices, eval_dir_short, gt_dir_short <span class="op">=</span> set_split(split)</span>
<span id="cb16-983"><a href="#cb16-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-984"><a href="#cb16-984" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_det_values(index_start<span class="op">=</span><span class="dv">0</span>, index_stop<span class="op">=-</span><span class="dv">1</span>):</span>
<span id="cb16-985"><a href="#cb16-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-986"><a href="#cb16-986" aria-hidden="true" tabindex="-1"></a>    results_for_det <span class="op">=</span> pd.read_csv(os.path.join(<span class="ss">f'TrackEval/data/trackers/surfrider_short_segments_</span><span class="sc">{</span>fps<span class="sc">}</span><span class="ss">'</span>,<span class="st">'surfrider-test'</span>,<span class="st">'ours_EKF_1_kappa_1_tau_0'</span>,<span class="st">'pedestrian_detailed.csv'</span>))</span>
<span id="cb16-987"><a href="#cb16-987" aria-hidden="true" tabindex="-1"></a>    results_det <span class="op">=</span> results_for_det.loc[:,[<span class="ss">f'DetRe</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'DetPr</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>, <span class="ss">f'HOTA_TP</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'HOTA_FN</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'HOTA_FP</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>]].iloc[index_start:index_stop]</span>
<span id="cb16-988"><a href="#cb16-988" aria-hidden="true" tabindex="-1"></a>    results_det.columns <span class="op">=</span> [<span class="st">'hota_det_re'</span>,<span class="st">'hota_det_pr'</span>,<span class="st">'hota_det_tp'</span>,<span class="st">'hota_det_fn'</span>,<span class="st">'hota_det_fp'</span>]</span>
<span id="cb16-989"><a href="#cb16-989" aria-hidden="true" tabindex="-1"></a>    hota_det_re <span class="op">=</span> results_det[<span class="st">'hota_det_re'</span>]</span>
<span id="cb16-990"><a href="#cb16-990" aria-hidden="true" tabindex="-1"></a>    hota_det_pr <span class="op">=</span> results_det[<span class="st">'hota_det_pr'</span>]</span>
<span id="cb16-991"><a href="#cb16-991" aria-hidden="true" tabindex="-1"></a>    hota_det_tp <span class="op">=</span> results_det[<span class="st">'hota_det_tp'</span>]</span>
<span id="cb16-992"><a href="#cb16-992" aria-hidden="true" tabindex="-1"></a>    hota_det_fn <span class="op">=</span> results_det[<span class="st">'hota_det_fn'</span>]</span>
<span id="cb16-993"><a href="#cb16-993" aria-hidden="true" tabindex="-1"></a>    hota_det_fp <span class="op">=</span> results_det[<span class="st">'hota_det_fp'</span>]</span>
<span id="cb16-994"><a href="#cb16-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-995"><a href="#cb16-995" aria-hidden="true" tabindex="-1"></a>    denom_hota_det_re <span class="op">=</span> hota_det_tp <span class="op">+</span> hota_det_fn </span>
<span id="cb16-996"><a href="#cb16-996" aria-hidden="true" tabindex="-1"></a>    denom_hota_det_pr <span class="op">=</span> hota_det_tp <span class="op">+</span> hota_det_fp </span>
<span id="cb16-997"><a href="#cb16-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-998"><a href="#cb16-998" aria-hidden="true" tabindex="-1"></a>    hota_det_re_cb <span class="op">=</span> (hota_det_re <span class="op">*</span> denom_hota_det_re).<span class="bu">sum</span>() <span class="op">/</span> denom_hota_det_re.<span class="bu">sum</span>()</span>
<span id="cb16-999"><a href="#cb16-999" aria-hidden="true" tabindex="-1"></a>    hota_det_pr_cb <span class="op">=</span> (hota_det_pr <span class="op">*</span> denom_hota_det_pr).<span class="bu">sum</span>() <span class="op">/</span> denom_hota_det_pr.<span class="bu">sum</span>()</span>
<span id="cb16-1000"><a href="#cb16-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1001"><a href="#cb16-1001" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [<span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>hota_det_re_cb<span class="sc">:.1f}</span><span class="ss">'</span>, <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>hota_det_pr_cb<span class="sc">:.1f}</span><span class="ss">'</span>]</span>
<span id="cb16-1002"><a href="#cb16-1002" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-1003"><a href="#cb16-1003" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_table_det():</span>
<span id="cb16-1004"><a href="#cb16-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1005"><a href="#cb16-1005" aria-hidden="true" tabindex="-1"></a>    table_values <span class="op">=</span> [get_det_values(index_start, index_stop) <span class="cf">for</span> (index_start, index_stop) <span class="kw">in</span> <span class="bu">zip</span>(indices_det[:<span class="op">-</span><span class="dv">1</span>],indices_det[<span class="dv">1</span>:])]</span>
<span id="cb16-1006"><a href="#cb16-1006" aria-hidden="true" tabindex="-1"></a>    table_values.append(get_det_values())</span>
<span id="cb16-1007"><a href="#cb16-1007" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(table_values)</span>
<span id="cb16-1008"><a href="#cb16-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1009"><a href="#cb16-1009" aria-hidden="true" tabindex="-1"></a>table_det <span class="op">=</span> get_table_det()</span>
<span id="cb16-1010"><a href="#cb16-1010" aria-hidden="true" tabindex="-1"></a>table_det.columns <span class="op">=</span> [<span class="st">'DetRe*'</span>,<span class="st">'DetPr*'</span>]</span>
<span id="cb16-1011"><a href="#cb16-1011" aria-hidden="true" tabindex="-1"></a>table_det.index <span class="op">=</span> [<span class="st">'S1'</span>,<span class="st">'S2'</span>,<span class="st">'S3'</span>,<span class="st">'All'</span>]</span>
<span id="cb16-1012"><a href="#cb16-1012" aria-hidden="true" tabindex="-1"></a>display(table_det)</span>
<span id="cb16-1013"><a href="#cb16-1013" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1014"><a href="#cb16-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1015"><a href="#cb16-1015" aria-hidden="true" tabindex="-1"></a><span class="fu">## Counts</span></span>
<span id="cb16-1016"><a href="#cb16-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1017"><a href="#cb16-1017" aria-hidden="true" tabindex="-1"></a>To fairly compare the three solutions, we calibrate the hyperparameters of our</span>
<span id="cb16-1018"><a href="#cb16-1018" aria-hidden="true" tabindex="-1"></a>postprocessing block on the validation split and keep the values that minimize</span>
<span id="cb16-1019"><a href="#cb16-1019" aria-hidden="true" tabindex="-1"></a>the overall count error $\mathsf{\hat{N}}$ for each of them separately (see</span>
<span id="cb16-1020"><a href="#cb16-1020" aria-hidden="true" tabindex="-1"></a>@sec-tau_kappa_appendix for more information). All methods are found to work optimally at $\kappa = 7$, but our solution requires $\tau = 8$ instead of $\tau = 9$ for other solutions: this lower level of thresholding suggests that raw output of our tracking system is more reliable.</span>
<span id="cb16-1021"><a href="#cb16-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1022"><a href="#cb16-1022" aria-hidden="true" tabindex="-1"></a>We report results using the count-related tracking metrics and count decompositions defined in the previous section. To provide a clear but thorough summary of the performance, we report $\mathsf{AssRe}$, $\mathsf{CountRe}$ and $\mathsf{CountPR}$ as tabled values (the first gives a simple overview of the quality of the predicted tacks while the latter two concisely summarise the count performance). For a more detailed visualisation of the different types of errors, we plot the count error decomposition for all sequences in a separate graph. Note that across all videos and all methods, we find $\mathsf{AssPr}$ between 98.6 and 99.2 which shows that this application context is unconcerned with tracks spanning multiple ground truth objects, therefore we do not conduct a more detailed interpretation of $\mathsf{AssPr}$ values.</span>
<span id="cb16-1023"><a href="#cb16-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1024"><a href="#cb16-1024" aria-hidden="true" tabindex="-1"></a>First, the higher values of AssRe confirm the robustness of our solution in assigning consistent tracks to individual objects. This is directly reflected into the count precision performance - with an overall value of $\mathsf{CountPR}$ 17.6 points higher than the next best method (SORT) - or even more so in the complete disappearance of orange (redundant) counts in the graph. A key aspect is that these improvements are not counteracted by a lower $\mathsf{CountRe}$: on the contrary, our tracker, which is more stable, also captures more object (albeit still missing most of them, with a $\mathsf{CountRe}$ below 50%). Note finally, that the strongest improvements are obtained for sequence 2 which is also the part with the strongest motion.</span>
<span id="cb16-1025"><a href="#cb16-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1028"><a href="#cb16-1028" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1029"><a href="#cb16-1029" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_summary(results, index_start<span class="op">=</span><span class="dv">0</span>, index_stop<span class="op">=-</span><span class="dv">1</span>):</span>
<span id="cb16-1030"><a href="#cb16-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1031"><a href="#cb16-1031" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> results.loc[:,[<span class="ss">f'Correct_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'Redundant_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'False_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'Missing_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'Fused_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>, <span class="ss">f'GT_IDs'</span>,<span class="ss">f'HOTA_TP</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'AssRe</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>]].iloc[index_start:index_stop]</span>
<span id="cb16-1032"><a href="#cb16-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1033"><a href="#cb16-1033" aria-hidden="true" tabindex="-1"></a>    results.columns <span class="op">=</span> [<span class="st">'correct'</span>,<span class="st">'redundant'</span>,<span class="st">'false'</span>,<span class="st">'missing'</span>,<span class="st">'mingled'</span>,<span class="st">'gt'</span>,<span class="st">'hota_tp'</span>,<span class="st">'ass_re'</span>]</span>
<span id="cb16-1034"><a href="#cb16-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1035"><a href="#cb16-1035" aria-hidden="true" tabindex="-1"></a>    ass_re <span class="op">=</span> results[<span class="st">'ass_re'</span>]</span>
<span id="cb16-1036"><a href="#cb16-1036" aria-hidden="true" tabindex="-1"></a>    hota_tp <span class="op">=</span> results[<span class="st">'hota_tp'</span>]</span>
<span id="cb16-1037"><a href="#cb16-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1038"><a href="#cb16-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1039"><a href="#cb16-1039" aria-hidden="true" tabindex="-1"></a>    ass_re_cb <span class="op">=</span> (ass_re <span class="op">*</span> hota_tp).<span class="bu">sum</span>() <span class="op">/</span> hota_tp.<span class="bu">sum</span>()</span>
<span id="cb16-1040"><a href="#cb16-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1041"><a href="#cb16-1041" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> results[<span class="st">'correct'</span>]</span>
<span id="cb16-1042"><a href="#cb16-1042" aria-hidden="true" tabindex="-1"></a>    redundant <span class="op">=</span> results[<span class="st">'redundant'</span>]</span>
<span id="cb16-1043"><a href="#cb16-1043" aria-hidden="true" tabindex="-1"></a>    false <span class="op">=</span> results[<span class="st">'false'</span>]</span>
<span id="cb16-1044"><a href="#cb16-1044" aria-hidden="true" tabindex="-1"></a>    missing <span class="op">=</span> results[<span class="st">'missing'</span>]</span>
<span id="cb16-1045"><a href="#cb16-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1046"><a href="#cb16-1046" aria-hidden="true" tabindex="-1"></a>    summary <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb16-1047"><a href="#cb16-1047" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'ass_re_cb'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>ass_re_cb<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb16-1048"><a href="#cb16-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1049"><a href="#cb16-1049" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-1050"><a href="#cb16-1050" aria-hidden="true" tabindex="-1"></a>    count_re <span class="op">=</span> correct.<span class="bu">sum</span>() <span class="op">/</span> (correct <span class="op">+</span> missing).<span class="bu">sum</span>()</span>
<span id="cb16-1051"><a href="#cb16-1051" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'count_re_cb'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>count_re<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb16-1052"><a href="#cb16-1052" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-1053"><a href="#cb16-1053" aria-hidden="true" tabindex="-1"></a>    count_re_std <span class="op">=</span> (correct <span class="op">/</span> (correct <span class="op">+</span> missing)).std()</span>
<span id="cb16-1054"><a href="#cb16-1054" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'count_re_std'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>np<span class="sc">.</span>nan_to_num(count_re_std)<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb16-1055"><a href="#cb16-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1056"><a href="#cb16-1056" aria-hidden="true" tabindex="-1"></a>    count_pr <span class="op">=</span> correct.<span class="bu">sum</span>() <span class="op">/</span> (correct <span class="op">+</span> false <span class="op">+</span> redundant).<span class="bu">sum</span>()</span>
<span id="cb16-1057"><a href="#cb16-1057" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'count_pr_cb'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>count_pr<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb16-1058"><a href="#cb16-1058" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-1059"><a href="#cb16-1059" aria-hidden="true" tabindex="-1"></a>    count_pr_std <span class="op">=</span> (correct <span class="op">/</span> (correct <span class="op">+</span> false <span class="op">+</span> redundant)).std()</span>
<span id="cb16-1060"><a href="#cb16-1060" aria-hidden="true" tabindex="-1"></a>    summary[<span class="st">'count_pr_std'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>np<span class="sc">.</span>nan_to_num(count_pr_std)<span class="sc">:.1f}</span><span class="ss">'</span></span>
<span id="cb16-1061"><a href="#cb16-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1062"><a href="#cb16-1062" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> summary </span>
<span id="cb16-1063"><a href="#cb16-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1064"><a href="#cb16-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1065"><a href="#cb16-1065" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_summaries(results, sequence_names):</span>
<span id="cb16-1066"><a href="#cb16-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1067"><a href="#cb16-1067" aria-hidden="true" tabindex="-1"></a>    summaries <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb16-1068"><a href="#cb16-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1069"><a href="#cb16-1069" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (sequence_name, index_start, index_stop) <span class="kw">in</span> <span class="bu">zip</span>(sequence_names, indices[:<span class="op">-</span><span class="dv">1</span>], indices[<span class="dv">1</span>:]):</span>
<span id="cb16-1070"><a href="#cb16-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1071"><a href="#cb16-1071" aria-hidden="true" tabindex="-1"></a>        summaries[sequence_name] <span class="op">=</span> get_summary(results, index_start, index_stop)</span>
<span id="cb16-1072"><a href="#cb16-1072" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-1073"><a href="#cb16-1073" aria-hidden="true" tabindex="-1"></a>    summaries[<span class="st">'All'</span>] <span class="op">=</span> get_summary(results)</span>
<span id="cb16-1074"><a href="#cb16-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1075"><a href="#cb16-1075" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> summaries</span>
<span id="cb16-1076"><a href="#cb16-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1077"><a href="#cb16-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1078"><a href="#cb16-1078" aria-hidden="true" tabindex="-1"></a>summaries <span class="op">=</span> []</span>
<span id="cb16-1079"><a href="#cb16-1079" aria-hidden="true" tabindex="-1"></a>method_names <span class="op">=</span> [<span class="st">'fairmot_kappa_7_tau_9'</span>, <span class="st">'sort_kappa_7_tau_9'</span>,<span class="st">'ours_EKF_1_kappa_7_tau_8'</span>]</span>
<span id="cb16-1080"><a href="#cb16-1080" aria-hidden="true" tabindex="-1"></a>pretty_method_names <span class="op">=</span> [<span class="st">'FairMOT'</span>,<span class="st">'SORT'</span>,<span class="st">'Ours'</span>]</span>
<span id="cb16-1081"><a href="#cb16-1081" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tracker_name <span class="kw">in</span> method_names:</span>
<span id="cb16-1082"><a href="#cb16-1082" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.read_csv(os.path.join(eval_dir_short,<span class="st">'surfrider-test'</span>,tracker_name,<span class="st">'pedestrian_detailed.csv'</span>))</span>
<span id="cb16-1083"><a href="#cb16-1083" aria-hidden="true" tabindex="-1"></a>    sequence_names <span class="op">=</span> [<span class="st">'S1'</span>,<span class="st">'S2'</span>,<span class="st">'S3'</span>]</span>
<span id="cb16-1084"><a href="#cb16-1084" aria-hidden="true" tabindex="-1"></a>    summaries.append(pd.DataFrame(get_summaries(results, sequence_names)).T)</span>
<span id="cb16-1085"><a href="#cb16-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1086"><a href="#cb16-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1087"><a href="#cb16-1087" aria-hidden="true" tabindex="-1"></a>fairmot_star, sort, ours <span class="op">=</span> summaries</span>
<span id="cb16-1088"><a href="#cb16-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1089"><a href="#cb16-1089" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> [<span class="st">'fairmot_kappa_7_tau_9'</span>,<span class="st">'sort_kappa_7_tau_9'</span>,<span class="st">'ours_EKF_1_kappa_7_tau_8'</span>]</span>
<span id="cb16-1090"><a href="#cb16-1090" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> [<span class="st">'ass_re'</span>,<span class="st">'count_re'</span>, <span class="st">'count_re_std'</span>, <span class="st">'count_pr'</span>, <span class="st">'count_pr_std'</span>]</span>
<span id="cb16-1091"><a href="#cb16-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1092"><a href="#cb16-1092" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> summary <span class="kw">in</span> summaries:    </span>
<span id="cb16-1093"><a href="#cb16-1093" aria-hidden="true" tabindex="-1"></a>    summary.columns <span class="op">=</span> columns</span>
<span id="cb16-1094"><a href="#cb16-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1095"><a href="#cb16-1095" aria-hidden="true" tabindex="-1"></a>results_S1 <span class="op">=</span> pd.DataFrame([summary.iloc[<span class="dv">0</span>] <span class="cf">for</span> summary <span class="kw">in</span> summaries])</span>
<span id="cb16-1096"><a href="#cb16-1096" aria-hidden="true" tabindex="-1"></a>results_S1.index <span class="op">=</span> method_names</span>
<span id="cb16-1097"><a href="#cb16-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1098"><a href="#cb16-1098" aria-hidden="true" tabindex="-1"></a>results_S2 <span class="op">=</span> pd.DataFrame([summary.iloc[<span class="dv">1</span>] <span class="cf">for</span> summary <span class="kw">in</span> summaries])</span>
<span id="cb16-1099"><a href="#cb16-1099" aria-hidden="true" tabindex="-1"></a>results_S2.index <span class="op">=</span> method_names</span>
<span id="cb16-1100"><a href="#cb16-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1101"><a href="#cb16-1101" aria-hidden="true" tabindex="-1"></a>results_S3 <span class="op">=</span> pd.DataFrame([summary.iloc[<span class="dv">2</span>] <span class="cf">for</span> summary <span class="kw">in</span> summaries])</span>
<span id="cb16-1102"><a href="#cb16-1102" aria-hidden="true" tabindex="-1"></a>results_S3.index <span class="op">=</span> method_names</span>
<span id="cb16-1103"><a href="#cb16-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1104"><a href="#cb16-1104" aria-hidden="true" tabindex="-1"></a>results_All <span class="op">=</span> pd.DataFrame([summary.iloc[<span class="dv">3</span>] <span class="cf">for</span> summary <span class="kw">in</span> summaries])</span>
<span id="cb16-1105"><a href="#cb16-1105" aria-hidden="true" tabindex="-1"></a>results_All.index <span class="op">=</span> method_names</span>
<span id="cb16-1106"><a href="#cb16-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1107"><a href="#cb16-1107" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> [results_S1, results_S2, results_S3, results_All]</span>
<span id="cb16-1108"><a href="#cb16-1108" aria-hidden="true" tabindex="-1"></a>column_names <span class="op">=</span> [</span>
<span id="cb16-1109"><a href="#cb16-1109" aria-hidden="true" tabindex="-1"></a>    <span class="st">"AssRe"</span>,</span>
<span id="cb16-1110"><a href="#cb16-1110" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r"CountRe"</span>,</span>
<span id="cb16-1111"><a href="#cb16-1111" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r"σ(CountRe)"</span>,</span>
<span id="cb16-1112"><a href="#cb16-1112" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r"CountPr"</span>,</span>
<span id="cb16-1113"><a href="#cb16-1113" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r"σ(CountPr)"</span>]</span>
<span id="cb16-1114"><a href="#cb16-1114" aria-hidden="true" tabindex="-1"></a>row_names <span class="op">=</span> [<span class="st">"FairMOT"</span>, <span class="st">"Sort"</span>, <span class="st">"Ours"</span>]</span>
<span id="cb16-1115"><a href="#cb16-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1116"><a href="#cb16-1116" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> result <span class="kw">in</span> results:</span>
<span id="cb16-1117"><a href="#cb16-1117" aria-hidden="true" tabindex="-1"></a>    result.columns <span class="op">=</span> column_names</span>
<span id="cb16-1118"><a href="#cb16-1118" aria-hidden="true" tabindex="-1"></a>    result.index <span class="op">=</span> row_names</span>
<span id="cb16-1119"><a href="#cb16-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1120"><a href="#cb16-1120" aria-hidden="true" tabindex="-1"></a>results_S1</span>
<span id="cb16-1121"><a href="#cb16-1121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1122"><a href="#cb16-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1125"><a href="#cb16-1125" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1126"><a href="#cb16-1126" aria-hidden="true" tabindex="-1"></a>results_S2</span>
<span id="cb16-1127"><a href="#cb16-1127" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1128"><a href="#cb16-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1131"><a href="#cb16-1131" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1132"><a href="#cb16-1132" aria-hidden="true" tabindex="-1"></a>results_S3</span>
<span id="cb16-1133"><a href="#cb16-1133" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1134"><a href="#cb16-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1137"><a href="#cb16-1137" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1138"><a href="#cb16-1138" aria-hidden="true" tabindex="-1"></a>results_All</span>
<span id="cb16-1139"><a href="#cb16-1139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1140"><a href="#cb16-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1141"><a href="#cb16-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1142"><a href="#cb16-1142" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Detailed results on individual segments</span></span>
<span id="cb16-1143"><a href="#cb16-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1146"><a href="#cb16-1146" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1147"><a href="#cb16-1147" aria-hidden="true" tabindex="-1"></a>set_split(<span class="st">'test'</span>)</span>
<span id="cb16-1148"><a href="#cb16-1148" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">30</span>,<span class="dv">10</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-1149"><a href="#cb16-1149" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, title, tracker_name <span class="kw">in</span> <span class="bu">zip</span>(axes, pretty_method_names,  method_names):</span>
<span id="cb16-1150"><a href="#cb16-1150" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.read_csv(os.path.join(eval_dir_short,<span class="st">'surfrider-test'</span>,tracker_name,<span class="st">'pedestrian_detailed.csv'</span>))</span>
<span id="cb16-1151"><a href="#cb16-1151" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> results.loc[:,[<span class="ss">f'Redundant_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'False_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>,<span class="ss">f'Missing_IDs</span><span class="sc">{</span>alpha_type<span class="sc">}</span><span class="ss">'</span>]].iloc[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb16-1152"><a href="#cb16-1152" aria-hidden="true" tabindex="-1"></a>    results.columns <span class="op">=</span> [<span class="st">'redundant'</span>, <span class="st">'false'</span>, <span class="st">'missing'</span>]</span>
<span id="cb16-1153"><a href="#cb16-1153" aria-hidden="true" tabindex="-1"></a>    results.loc[:,<span class="st">'missing'</span>] <span class="op">=</span> <span class="op">-</span> results.loc[:,<span class="st">'missing'</span>]</span>
<span id="cb16-1154"><a href="#cb16-1154" aria-hidden="true" tabindex="-1"></a>    results.columns <span class="op">=</span> [<span class="st">'$\hat{\mathsf</span><span class="sc">{N}</span><span class="st">}_</span><span class="sc">{red}</span><span class="st">$'</span>, <span class="st">'$\hat{\mathsf</span><span class="sc">{N}</span><span class="st">}_</span><span class="sc">{false}</span><span class="st">$'</span>, <span class="st">'$-\hat{\mathsf</span><span class="sc">{N}</span><span class="st">}_</span><span class="sc">{mis}</span><span class="st">$'</span>]</span>
<span id="cb16-1155"><a href="#cb16-1155" aria-hidden="true" tabindex="-1"></a>    results.plot(ax <span class="op">=</span> ax, kind<span class="op">=</span><span class="st">'bar'</span>, stacked<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>[<span class="st">'orange'</span>, <span class="st">'red'</span>, <span class="st">'black'</span>], title<span class="op">=</span>title, xlabel<span class="op">=</span><span class="st">'Sequence nb'</span>)</span>
<span id="cb16-1156"><a href="#cb16-1156" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1157"><a href="#cb16-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1158"><a href="#cb16-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1159"><a href="#cb16-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1160"><a href="#cb16-1160" aria-hidden="true" tabindex="-1"></a><span class="fu"># Practical impact and future goals</span></span>
<span id="cb16-1161"><a href="#cb16-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1162"><a href="#cb16-1162" aria-hidden="true" tabindex="-1"></a>We successfully tackled video object counting on river banks, in particular issues which could be addressed independently of detection quality.</span>
<span id="cb16-1163"><a href="#cb16-1163" aria-hidden="true" tabindex="-1"></a>Moreover the methodology developed to assess count quality enables us to precisely highlight the challenges that pertain to video object counting on river banks.</span>
<span id="cb16-1164"><a href="#cb16-1164" aria-hidden="true" tabindex="-1"></a>Conducted in coordination with Surfrider Foundation Europe, an NGO specialized on water preservation, our work marks an important milestone in a broader campaign for macrolitter monitoring and is already being used in a production version of a monitoring system.</span>
<span id="cb16-1165"><a href="#cb16-1165" aria-hidden="true" tabindex="-1"></a>That said, large amounts of litter items are still not detected.</span>
<span id="cb16-1166"><a href="#cb16-1166" aria-hidden="true" tabindex="-1"></a>Solving this problem is largely a question of augmenting the object detector training dataset through crowdsourced images.</span>
<span id="cb16-1167"><a href="#cb16-1167" aria-hidden="true" tabindex="-1"></a>A <span class="co">[</span><span class="ot">specific annotation platform</span><span class="co">](https://www.trashroulette.com)</span> is online, thus the amount of annotated images is expected to continuously increase, while training is provided to volunteers collecting data on the field to ensure data quality.</span>
<span id="cb16-1168"><a href="#cb16-1168" aria-hidden="true" tabindex="-1"></a>Finally, several expeditions on different rivers are already underway and new video footage is expected to be annotated in the near future for better evaluation.</span>
<span id="cb16-1169"><a href="#cb16-1169" aria-hidden="true" tabindex="-1"></a>All data is made freely available.</span>
<span id="cb16-1170"><a href="#cb16-1170" aria-hidden="true" tabindex="-1"></a>Future goals include downsizing the algorithm, a possibility given the architectural simplicity of anchor-free detection and the relatively low computational complexity of EKF.</span>
<span id="cb16-1171"><a href="#cb16-1171" aria-hidden="true" tabindex="-1"></a>In a citizen science perspective, a fully embedded version for portable devices will allow a larger deployment.</span>
<span id="cb16-1172"><a href="#cb16-1172" aria-hidden="true" tabindex="-1"></a>The resulting field data will help better understand litter origin, allowing to model and predict litter density in non surveyed areas.</span>
<span id="cb16-1173"><a href="#cb16-1173" aria-hidden="true" tabindex="-1"></a>Correlations between macro litter density and environmental parameters will be studied (e.g., population density, catchment size, land use and hydromorphology).</span>
<span id="cb16-1174"><a href="#cb16-1174" aria-hidden="true" tabindex="-1"></a>Finally, our work naturally benefits any extension of macrolitter monitoring in other areas (urban, coastal, etc) that may rely on a similar setup of moving cameras.</span>
<span id="cb16-1175"><a href="#cb16-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1176"><a href="#cb16-1176" aria-hidden="true" tabindex="-1"></a><span class="fu"># Supplements</span></span>
<span id="cb16-1177"><a href="#cb16-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1178"><a href="#cb16-1178" aria-hidden="true" tabindex="-1"></a><span class="fu">## Details on the image dataset {#sec-image_dataset_appendix}</span></span>
<span id="cb16-1179"><a href="#cb16-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1180"><a href="#cb16-1180" aria-hidden="true" tabindex="-1"></a><span class="fu">### Categories</span></span>
<span id="cb16-1181"><a href="#cb16-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1182"><a href="#cb16-1182" aria-hidden="true" tabindex="-1"></a>In this work, we do not seek to precisely predict the proportions of the</span>
<span id="cb16-1183"><a href="#cb16-1183" aria-hidden="true" tabindex="-1"></a>different types of counted litter. However, we build our dataset to allow</span>
<span id="cb16-1184"><a href="#cb16-1184" aria-hidden="true" tabindex="-1"></a>classification tasks. Though litter classifications built by experts already</span>
<span id="cb16-1185"><a href="#cb16-1185" aria-hidden="true" tabindex="-1"></a>exist, most are based on semantic rather than visual features and do not</span>
<span id="cb16-1186"><a href="#cb16-1186" aria-hidden="true" tabindex="-1"></a>particularly consider the problem of class imbalance, which makes statistical</span>
<span id="cb16-1187"><a href="#cb16-1187" aria-hidden="true" tabindex="-1"></a>learning more delicate. In conjunction with water pollution experts, we</span>
<span id="cb16-1188"><a href="#cb16-1188" aria-hidden="true" tabindex="-1"></a>therefore define a custom macrolitter taxonomy which balances annotation ease</span>
<span id="cb16-1189"><a href="#cb16-1189" aria-hidden="true" tabindex="-1"></a>and pragmatic decisions for computer vision applications. This classification,</span>
<span id="cb16-1190"><a href="#cb16-1190" aria-hidden="true" tabindex="-1"></a>depicted in @fig-trash-categories-image can be understood as follows.</span>
<span id="cb16-1191"><a href="#cb16-1191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1192"><a href="#cb16-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1193"><a href="#cb16-1193" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We define a set of frequently observed classes that annotateors can choose from, divided into: </span>
<span id="cb16-1194"><a href="#cb16-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1195"><a href="#cb16-1195" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Classes for rigid and easily recognisable items which are often observed and have definite shapes</span>
<span id="cb16-1196"><a href="#cb16-1196" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Classes for fragmented objects which are often found along river banks but whose aspects are more varied</span>
<span id="cb16-1197"><a href="#cb16-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1198"><a href="#cb16-1198" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We define two supplementary categories used whenever the annotater cannot classify the item they are observing in an image using classes given in 1.</span>
<span id="cb16-1199"><a href="#cb16-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1200"><a href="#cb16-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1201"><a href="#cb16-1201" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>A first category is used whenever the item is clearly identifiable but its class is not proposed.</span>
<span id="cb16-1202"><a href="#cb16-1202" aria-hidden="true" tabindex="-1"></a>This will ensure that our classification can be improved in the future, as images with items in this category will be checked regularly to decide whether a new class needs to be created.</span>
<span id="cb16-1203"><a href="#cb16-1203" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Another category is used whenever the annotater does not understand the item they are seeing.</span>
<span id="cb16-1204"><a href="#cb16-1204" aria-hidden="true" tabindex="-1"></a>Images containing items denoted as such will not be used for applications involving classification.</span>
<span id="cb16-1205"><a href="#cb16-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1206"><a href="#cb16-1206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1207"><a href="#cb16-1207" aria-hidden="true" tabindex="-1"></a>::: {#fig-trash-categories-image}</span>
<span id="cb16-1208"><a href="#cb16-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1209"><a href="#cb16-1209" aria-hidden="true" tabindex="-1"></a><span class="al">![](figures/trash_categories.png)</span></span>
<span id="cb16-1210"><a href="#cb16-1210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1211"><a href="#cb16-1211" aria-hidden="true" tabindex="-1"></a>Trash categories defined to facilitate porting to a counting system that allows trash identification</span>
<span id="cb16-1212"><a href="#cb16-1212" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1213"><a href="#cb16-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1214"><a href="#cb16-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1215"><a href="#cb16-1215" aria-hidden="true" tabindex="-1"></a><span class="fu">## Details on the evaluation videos {#sec-video-dataset-appendix}</span></span>
<span id="cb16-1216"><a href="#cb16-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1217"><a href="#cb16-1217" aria-hidden="true" tabindex="-1"></a><span class="fu">### River segments </span></span>
<span id="cb16-1218"><a href="#cb16-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1219"><a href="#cb16-1219" aria-hidden="true" tabindex="-1"></a>In this section, we provide further details on the evaluation material.</span>
<span id="cb16-1220"><a href="#cb16-1220" aria-hidden="true" tabindex="-1"></a>@fig-river-sections shows the setup and positioning of the three river</span>
<span id="cb16-1221"><a href="#cb16-1221" aria-hidden="true" tabindex="-1"></a>segments $S_1$, $S_2$ and $S_3$ used to evaluate the methods. The segments</span>
<span id="cb16-1222"><a href="#cb16-1222" aria-hidden="true" tabindex="-1"></a>differ in the following aspects.</span>
<span id="cb16-1223"><a href="#cb16-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1224"><a href="#cb16-1224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Segment 1: Medium current, high and dense vegetation not obstructing vision of the right riverbank from watercrafts, extra objects installed before the field experiment.</span>
<span id="cb16-1225"><a href="#cb16-1225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Segment 2: High current, low and dense vegetation obstructing vision of the right riverbank from watercrafts.</span>
<span id="cb16-1226"><a href="#cb16-1226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Segment 3: Medium current, high and little vegetation not obstructing vision  of the left riverbank from watercrafts.</span>
<span id="cb16-1227"><a href="#cb16-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1228"><a href="#cb16-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1229"><a href="#cb16-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1230"><a href="#cb16-1230" aria-hidden="true" tabindex="-1"></a>::: {#fig-river-sections}</span>
<span id="cb16-1231"><a href="#cb16-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1232"><a href="#cb16-1232" aria-hidden="true" tabindex="-1"></a><span class="al">![](figures/river_sections.png)</span></span>
<span id="cb16-1233"><a href="#cb16-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1234"><a href="#cb16-1234" aria-hidden="true" tabindex="-1"></a>Aerial view of the three river segments of the evaluation material</span>
<span id="cb16-1235"><a href="#cb16-1235" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1236"><a href="#cb16-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1237"><a href="#cb16-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1238"><a href="#cb16-1238" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Track annotation protocol</span></span>
<span id="cb16-1239"><a href="#cb16-1239" aria-hidden="true" tabindex="-1"></a>To annotate tracks on the evaluation sequences, we used the online tool "CVAT" which allows to locate bounding boxes on video frames and propagate them in time.</span>
<span id="cb16-1240"><a href="#cb16-1240" aria-hidden="true" tabindex="-1"></a>The following items provide further details on the exact annotation process.</span>
<span id="cb16-1241"><a href="#cb16-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1242"><a href="#cb16-1242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Object tracks start whenever a litter item becomes fully visible and identifiable by the naked eye.</span>
<span id="cb16-1243"><a href="#cb16-1243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Positions and sizes of objects are given at nearly every second of the video with automatic interpolation for frames in-between: this yields clean tracks with precise positions at 24fps.</span>
<span id="cb16-1244"><a href="#cb16-1244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We do not provide inferred locations when an object is fully occluded, but tracks restart with the same identity whenever the object becomes visible again.</span>
<span id="cb16-1245"><a href="#cb16-1245" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tracks stop whenever an object becomes indistinguishable and will not reappear again.</span>
<span id="cb16-1246"><a href="#cb16-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1247"><a href="#cb16-1247" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implementation details for the tracking module {#sec-tracking_module_appendix}</span></span>
<span id="cb16-1248"><a href="#cb16-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1249"><a href="#cb16-1249" aria-hidden="true" tabindex="-1"></a><span class="fu">### Covariance matrices for state and observation noises {#sec-covariance_matrices}</span></span>
<span id="cb16-1250"><a href="#cb16-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1251"><a href="#cb16-1251" aria-hidden="true" tabindex="-1"></a>In our state space model, $Q$ models the noise associated with the movement</span>
<span id="cb16-1252"><a href="#cb16-1252" aria-hidden="true" tabindex="-1"></a>model we posit in @sec-bayesian_tracking involving optical flow estimates,</span>
<span id="cb16-1253"><a href="#cb16-1253" aria-hidden="true" tabindex="-1"></a>while $R$ models the noise associated with the observation of the true</span>
<span id="cb16-1254"><a href="#cb16-1254" aria-hidden="true" tabindex="-1"></a>position via our object detector. An attempt to estimate the diagonal values</span>
<span id="cb16-1255"><a href="#cb16-1255" aria-hidden="true" tabindex="-1"></a>of these matrices was the following.</span>
<span id="cb16-1256"><a href="#cb16-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1257"><a href="#cb16-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1258"><a href="#cb16-1258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>To estimate $R$, we computed a mean $L_2$ error between the known positions of objects and the associated predictions by the object detector, for images in our training dataset.</span>
<span id="cb16-1259"><a href="#cb16-1259" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>To estimate $Q$, we built a small synthetic dataset of consecutive frames taken from videos, where positions of objects in two consecutive frames are known.</span>
<span id="cb16-1260"><a href="#cb16-1260" aria-hidden="true" tabindex="-1"></a>We computed a mean $L_2$ error between the known positions in the second frame and the positions estimated by shifting the positions in the first frame with the estimated optical flow values.</span>
<span id="cb16-1261"><a href="#cb16-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1262"><a href="#cb16-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1263"><a href="#cb16-1263" aria-hidden="true" tabindex="-1"></a>This led to $R_{00} = R_{11} = 1.1$, $Q_{00} = 4.7$ and $Q_{11} = 0.9$, for grids of dimensions $\lfloor w/p\rfloor \times \lfloor h/p\rfloor = 480 \times 270$.</span>
<span id="cb16-1264"><a href="#cb16-1264" aria-hidden="true" tabindex="-1"></a>All other coefficients were not estimated and supposed to be 0.</span>
<span id="cb16-1265"><a href="#cb16-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1266"><a href="#cb16-1266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1267"><a href="#cb16-1267" aria-hidden="true" tabindex="-1"></a>An important remark is that though we use these values in practice, we found that tracking results are largely unaffected by small variations of $R$ and $Q$.</span>
<span id="cb16-1268"><a href="#cb16-1268" aria-hidden="true" tabindex="-1"></a>As long as values are meaningful relative to the image dimensions and the size of the objects, most noise levels show relatively similar performance.</span>
<span id="cb16-1269"><a href="#cb16-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1270"><a href="#cb16-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1271"><a href="#cb16-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1272"><a href="#cb16-1272" aria-hidden="true" tabindex="-1"></a><span class="fu">### Influence of $\tau$ and $\kappa$ {#sec-tau_kappa_appendix}</span></span>
<span id="cb16-1273"><a href="#cb16-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1274"><a href="#cb16-1274" aria-hidden="true" tabindex="-1"></a>An understanding of $\kappa$, $\tau$ and $\nu$ can be stated as follows.</span>
<span id="cb16-1275"><a href="#cb16-1275" aria-hidden="true" tabindex="-1"></a>For any track, given a value for $\kappa$ and $\nu$, an observation at time $n$ is only kept if there are also $\nu \cdot \kappa$ observations in the temporal window of size $\kappa$ that surrounds $n$ (windows are centered around $n$ except at the start and end of the track).</span>
<span id="cb16-1276"><a href="#cb16-1276" aria-hidden="true" tabindex="-1"></a>The track is only counted if the remaining number of observations is strictly higher than $\tau$.</span>
<span id="cb16-1277"><a href="#cb16-1277" aria-hidden="true" tabindex="-1"></a>At a given $\nu &gt; 0.5$, $\kappa$ and $\tau$ should ideally be chosen to jointly decrease $\mathsf{\hat{N}_{false}}$ and $\mathsf{\hat{N}_{red}}$ as much as possible without increasing $\mathsf{\hat{N}_{mis}}$ (true objects become uncounted if tracks are discarded too easily).</span>
<span id="cb16-1278"><a href="#cb16-1278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1279"><a href="#cb16-1279" aria-hidden="true" tabindex="-1"></a>In the following code cell, we plot the error decomposition of the counts for several values of $\kappa$ and $\tau$ with $\nu=0.6$ for the outputs of the three different trackers. We choose $\nu = 0.7$ and compute the optimal point as the one which minimizes the overall count error $\mathsf{\hat{N}} (= \mathsf{\hat{N}_{mis}} + \mathsf{\hat{N}_{red}} + \mathsf{\hat{N}_{false}})$.</span>
<span id="cb16-1280"><a href="#cb16-1280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1283"><a href="#cb16-1283" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1284"><a href="#cb16-1284" aria-hidden="true" tabindex="-1"></a>set_split(<span class="st">'val'</span>)</span>
<span id="cb16-1285"><a href="#cb16-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1286"><a href="#cb16-1286" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {<span class="st">'legend.fontsize'</span>: <span class="st">'x-large'</span>,</span>
<span id="cb16-1287"><a href="#cb16-1287" aria-hidden="true" tabindex="-1"></a>         <span class="st">'axes.labelsize'</span>: <span class="st">'x-large'</span>,</span>
<span id="cb16-1288"><a href="#cb16-1288" aria-hidden="true" tabindex="-1"></a>         <span class="st">'axes.titlesize'</span>:<span class="st">'x-large'</span>,</span>
<span id="cb16-1289"><a href="#cb16-1289" aria-hidden="true" tabindex="-1"></a>         <span class="st">'xtick.labelsize'</span>:<span class="st">'x-large'</span>,</span>
<span id="cb16-1290"><a href="#cb16-1290" aria-hidden="true" tabindex="-1"></a>         <span class="st">'ytick.labelsize'</span>:<span class="st">'x-large'</span>}</span>
<span id="cb16-1291"><a href="#cb16-1291" aria-hidden="true" tabindex="-1"></a>plt.rcParams.update(params)</span>
<span id="cb16-1292"><a href="#cb16-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1293"><a href="#cb16-1293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1294"><a href="#cb16-1294" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hyperparameters(method_name, pretty_method_name):</span>
<span id="cb16-1295"><a href="#cb16-1295" aria-hidden="true" tabindex="-1"></a>    tau_values <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">10</span>)]</span>
<span id="cb16-1296"><a href="#cb16-1296" aria-hidden="true" tabindex="-1"></a>    kappa_values <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>]</span>
<span id="cb16-1297"><a href="#cb16-1297" aria-hidden="true" tabindex="-1"></a>    fig, (ax0, ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb16-1298"><a href="#cb16-1298" aria-hidden="true" tabindex="-1"></a>    pretty_names<span class="op">=</span>[<span class="ss">f'$\kappa=</span><span class="sc">{</span>kappa<span class="sc">}</span><span class="ss">$'</span> <span class="cf">for</span> kappa <span class="kw">in</span> kappa_values]</span>
<span id="cb16-1299"><a href="#cb16-1299" aria-hidden="true" tabindex="-1"></a>    n_count <span class="op">=</span> {}</span>
<span id="cb16-1300"><a href="#cb16-1300" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> kappa, pretty_name <span class="kw">in</span> <span class="bu">zip</span>(kappa_values, pretty_names):</span>
<span id="cb16-1301"><a href="#cb16-1301" aria-hidden="true" tabindex="-1"></a>        tracker_names <span class="op">=</span> [<span class="ss">f'</span><span class="sc">{</span>method_name<span class="sc">}</span><span class="ss">_kappa_</span><span class="sc">{</span>kappa<span class="sc">}</span><span class="ss">_tau_</span><span class="sc">{</span>tau<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> tau <span class="kw">in</span> tau_values]</span>
<span id="cb16-1302"><a href="#cb16-1302" aria-hidden="true" tabindex="-1"></a>        all_results <span class="op">=</span> {tracker_name: pd.read_csv(os.path.join(eval_dir_short,<span class="st">'surfrider-test'</span>,tracker_name,<span class="st">'pedestrian_detailed.csv'</span>)).iloc[:<span class="op">-</span><span class="dv">1</span>] <span class="cf">for</span> tracker_name <span class="kw">in</span> tracker_names}</span>
<span id="cb16-1303"><a href="#cb16-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1304"><a href="#cb16-1304" aria-hidden="true" tabindex="-1"></a>        n_missing <span class="op">=</span> []</span>
<span id="cb16-1305"><a href="#cb16-1305" aria-hidden="true" tabindex="-1"></a>        n_false <span class="op">=</span> []</span>
<span id="cb16-1306"><a href="#cb16-1306" aria-hidden="true" tabindex="-1"></a>        n_redundant <span class="op">=</span> []</span>
<span id="cb16-1307"><a href="#cb16-1307" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> tracker_name, tracker_results <span class="kw">in</span> all_results.items():</span>
<span id="cb16-1308"><a href="#cb16-1308" aria-hidden="true" tabindex="-1"></a>            missing <span class="op">=</span> (tracker_results[<span class="st">'GT_IDs'</span>].<span class="bu">sum</span>() <span class="op">-</span> tracker_results[<span class="st">'Correct_IDs___50'</span>].<span class="bu">sum</span>())</span>
<span id="cb16-1309"><a href="#cb16-1309" aria-hidden="true" tabindex="-1"></a>            false <span class="op">=</span> tracker_results[<span class="st">'False_IDs___50'</span>].<span class="bu">sum</span>()</span>
<span id="cb16-1310"><a href="#cb16-1310" aria-hidden="true" tabindex="-1"></a>            redundant <span class="op">=</span> tracker_results[<span class="st">'Redundant_IDs___50'</span>].<span class="bu">sum</span>()</span>
<span id="cb16-1311"><a href="#cb16-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1312"><a href="#cb16-1312" aria-hidden="true" tabindex="-1"></a>            n_missing.append(missing)</span>
<span id="cb16-1313"><a href="#cb16-1313" aria-hidden="true" tabindex="-1"></a>            n_false.append(false)</span>
<span id="cb16-1314"><a href="#cb16-1314" aria-hidden="true" tabindex="-1"></a>            n_redundant.append(redundant)</span>
<span id="cb16-1315"><a href="#cb16-1315" aria-hidden="true" tabindex="-1"></a>            n_count[tracker_name] <span class="op">=</span> missing <span class="op">+</span> false <span class="op">+</span> redundant</span>
<span id="cb16-1316"><a href="#cb16-1316" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-1317"><a href="#cb16-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1318"><a href="#cb16-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1319"><a href="#cb16-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1320"><a href="#cb16-1320" aria-hidden="true" tabindex="-1"></a>        ax0.scatter(tau_values, n_missing)</span>
<span id="cb16-1321"><a href="#cb16-1321" aria-hidden="true" tabindex="-1"></a>        ax0.plot(tau_values, n_missing, label<span class="op">=</span>pretty_name, linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb16-1322"><a href="#cb16-1322" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ax0.set_xlabel('$\\tau$')</span></span>
<span id="cb16-1323"><a href="#cb16-1323" aria-hidden="true" tabindex="-1"></a>        ax0.set_ylabel(<span class="st">'$N_</span><span class="sc">{mis}</span><span class="st">$'</span>)</span>
<span id="cb16-1324"><a href="#cb16-1324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1325"><a href="#cb16-1325" aria-hidden="true" tabindex="-1"></a>        ax1.scatter(tau_values, n_false)</span>
<span id="cb16-1326"><a href="#cb16-1326" aria-hidden="true" tabindex="-1"></a>        ax1.plot(tau_values, n_false, linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb16-1327"><a href="#cb16-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1328"><a href="#cb16-1328" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ax1.set_xlabel('$\\tau$')</span></span>
<span id="cb16-1329"><a href="#cb16-1329" aria-hidden="true" tabindex="-1"></a>        ax1.set_ylabel(<span class="st">'$N_</span><span class="sc">{false}</span><span class="st">$'</span>)</span>
<span id="cb16-1330"><a href="#cb16-1330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1331"><a href="#cb16-1331" aria-hidden="true" tabindex="-1"></a>        ax2.scatter(tau_values, n_redundant)</span>
<span id="cb16-1332"><a href="#cb16-1332" aria-hidden="true" tabindex="-1"></a>        ax2.plot(tau_values, n_redundant, linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb16-1333"><a href="#cb16-1333" aria-hidden="true" tabindex="-1"></a>        ax2.set_xlabel(<span class="st">'$</span><span class="ch">\\</span><span class="st">tau$'</span>)</span>
<span id="cb16-1334"><a href="#cb16-1334" aria-hidden="true" tabindex="-1"></a>        ax2.set_ylabel(<span class="st">'$N_</span><span class="sc">{red}</span><span class="st">$'</span>)</span>
<span id="cb16-1335"><a href="#cb16-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1336"><a href="#cb16-1336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1337"><a href="#cb16-1337" aria-hidden="true" tabindex="-1"></a>    best_value <span class="op">=</span> np.inf</span>
<span id="cb16-1338"><a href="#cb16-1338" aria-hidden="true" tabindex="-1"></a>    best_key <span class="op">=</span> <span class="st">''</span></span>
<span id="cb16-1339"><a href="#cb16-1339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1340"><a href="#cb16-1340" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> n_count.items():</span>
<span id="cb16-1341"><a href="#cb16-1341" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> v <span class="op">&lt;</span> best_value: best_key <span class="op">=</span> k</span>
<span id="cb16-1342"><a href="#cb16-1342" aria-hidden="true" tabindex="-1"></a>        best_value <span class="op">=</span> v</span>
<span id="cb16-1343"><a href="#cb16-1343" aria-hidden="true" tabindex="-1"></a>    best_key <span class="op">=</span> best_key.split(<span class="st">'kappa'</span>)[<span class="dv">1</span>]</span>
<span id="cb16-1344"><a href="#cb16-1344" aria-hidden="true" tabindex="-1"></a>    best_kappa <span class="op">=</span> <span class="bu">int</span>(best_key.split(<span class="st">'_'</span>)[<span class="dv">1</span>])</span>
<span id="cb16-1345"><a href="#cb16-1345" aria-hidden="true" tabindex="-1"></a>    best_tau <span class="op">=</span> <span class="bu">int</span>(best_key.split(<span class="st">'_'</span>)[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb16-1346"><a href="#cb16-1346" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Best parameters for </span><span class="sc">{</span>pretty_method_name<span class="sc">}</span><span class="ss">: (kappa, tau) = (</span><span class="sc">{</span>best_kappa<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>best_tau<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb16-1347"><a href="#cb16-1347" aria-hidden="true" tabindex="-1"></a>    handles, labels <span class="op">=</span> ax0.get_legend_handles_labels()</span>
<span id="cb16-1348"><a href="#cb16-1348" aria-hidden="true" tabindex="-1"></a>    fig.legend(handles, labels, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb16-1349"><a href="#cb16-1349" aria-hidden="true" tabindex="-1"></a>    plt.autoscale(<span class="va">True</span>)</span>
<span id="cb16-1350"><a href="#cb16-1350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1351"><a href="#cb16-1351" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb16-1352"><a href="#cb16-1352" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb16-1353"><a href="#cb16-1353" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb16-1354"><a href="#cb16-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1355"><a href="#cb16-1355" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> method_name, pretty_method_name <span class="kw">in</span> <span class="bu">zip</span>(method_names, pretty_method_names):</span>
<span id="cb16-1356"><a href="#cb16-1356" aria-hidden="true" tabindex="-1"></a>    hyperparameters(method_name.split(<span class="st">'kappa'</span>)[<span class="dv">0</span>][:<span class="op">-</span><span class="dv">1</span>], pretty_method_name)</span>
<span id="cb16-1357"><a href="#cb16-1357" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1358"><a href="#cb16-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1359"><a href="#cb16-1359" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian filtering {#sec-bayesian_filtering}</span></span>
<span id="cb16-1360"><a href="#cb16-1360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1361"><a href="#cb16-1361" aria-hidden="true" tabindex="-1"></a>Considering a state space model with $(X_k, Z_k)_{k \geq 0}$ the random processes for the states and observations, respectively, the filtering recursions are given by:</span>
<span id="cb16-1362"><a href="#cb16-1362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1363"><a href="#cb16-1363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The predict step: $p(x_{k+1}|z_{1:k}) = \int p(x_{k+1}|x_k)p(x_k|z_{1:k})\mathrm{d}x_k.$</span>
<span id="cb16-1364"><a href="#cb16-1364" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The update step: $p(x_{k+1}|z_{1:k+1}) \propto p(z_{k+1} | x_{k+1})p(x_{k+1}|z_{1:k}).$</span>
<span id="cb16-1365"><a href="#cb16-1365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1366"><a href="#cb16-1366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1367"><a href="#cb16-1367" aria-hidden="true" tabindex="-1"></a>The recursions are intractable in most cases, but when the model is linear and Gaussian, i.e. such that: </span>
<span id="cb16-1368"><a href="#cb16-1368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1369"><a href="#cb16-1369" aria-hidden="true" tabindex="-1"></a>$$X_{k} = A_kX_{k-1} + a_k + \eta_k$$ </span>
<span id="cb16-1370"><a href="#cb16-1370" aria-hidden="true" tabindex="-1"></a>$$Z_{k} = B_kX_{k} + b_k + \epsilon_k$$</span>
<span id="cb16-1371"><a href="#cb16-1371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1372"><a href="#cb16-1372" aria-hidden="true" tabindex="-1"></a>with $\eta_k \sim \mathcal{N}(0,Q_k)$ and $\epsilon_k \sim \mathcal{N}(0,R_k)$, then the distribution of $X_k$ given $Z_{1:k}$ is a Gaussian $\mathcal{N}(\mu_k,\Sigma_k)$ following: </span>
<span id="cb16-1373"><a href="#cb16-1373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1374"><a href="#cb16-1374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu_{k|k-1} = A_k\mu_{k-1} + a_k$ and $\Sigma_{k|k-1} = A_k \Sigma_{k-1} A_k^T + Q_k$ (Kalman predict step),</span>
<span id="cb16-1375"><a href="#cb16-1375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu_{k} = \mu_{k|k-1} + K_k\left[Z_k - (B_k\mu_{k|k-1} + b_k)\right]$ and $\Sigma_{k} = (I - K_kB_k)\Sigma_{k|k-1}$ (Kalman update step),</span>
<span id="cb16-1376"><a href="#cb16-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1377"><a href="#cb16-1377" aria-hidden="true" tabindex="-1"></a>where $K_k = \Sigma_{k|k-1}B_k^T(B_k \Sigma_{k|k-1} B_k^T + R_k)^{-1}$.</span>
<span id="cb16-1378"><a href="#cb16-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1379"><a href="#cb16-1379" aria-hidden="true" tabindex="-1"></a>In the case of the linearized model in @sec-state_space_model, EKF consists in applying these updates with:</span>
<span id="cb16-1380"><a href="#cb16-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1381"><a href="#cb16-1381" aria-hidden="true" tabindex="-1"></a>$$A_k = (I + \partial_X\Delta_k(\lfloor \mu_{k-1} \rfloor),$$</span>
<span id="cb16-1382"><a href="#cb16-1382" aria-hidden="true" tabindex="-1"></a>$$a_k = \Delta_k(\lfloor \mu_{k-1} \rfloor) - \partial_X\Delta_k(\lfloor \mu_{k-1} \rfloor)\mu_{k-1},$$</span>
<span id="cb16-1383"><a href="#cb16-1383" aria-hidden="true" tabindex="-1"></a>$$Q_k = Q, R_k = R,$$</span>
<span id="cb16-1384"><a href="#cb16-1384" aria-hidden="true" tabindex="-1"></a>$$B_k = I, b_k = 0.$$</span>
<span id="cb16-1385"><a href="#cb16-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1386"><a href="#cb16-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1387"><a href="#cb16-1387" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computing the confidence regions {#sec-confidence_regions_appendix}</span></span>
<span id="cb16-1388"><a href="#cb16-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1389"><a href="#cb16-1389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1390"><a href="#cb16-1390" aria-hidden="true" tabindex="-1"></a>In words, $P(i,\ell)$ is the mass in $V_\delta(z_n^i) \subset \mathbb{R}^2$ of the</span>
<span id="cb16-1391"><a href="#cb16-1391" aria-hidden="true" tabindex="-1"></a>probability distribution of $Z_n^\ell$ given $Z_{1:n-1}^\ell$. It is related</span>
<span id="cb16-1392"><a href="#cb16-1392" aria-hidden="true" tabindex="-1"></a>to the filtering distribution at the previous timestep via </span>
<span id="cb16-1393"><a href="#cb16-1393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1394"><a href="#cb16-1394" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1395"><a href="#cb16-1395" aria-hidden="true" tabindex="-1"></a>p(z_n | z_{1:n-1}) = \int \int p(z_n | x_n) p(x_n | x_{n-1}) p(x_{n-1} | z_{1:n-1}) \mathrm{d} x_{n} \mathrm{d} x_{n-1}</span>
<span id="cb16-1396"><a href="#cb16-1396" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1397"><a href="#cb16-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1398"><a href="#cb16-1398" aria-hidden="true" tabindex="-1"></a>When using EKF, this distribution is a multivariate Gaussian whose moments can be analytically obtained from the filtering mean and variance and the parameters of the linear model, i.e. </span>
<span id="cb16-1399"><a href="#cb16-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1400"><a href="#cb16-1400" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1401"><a href="#cb16-1401" aria-hidden="true" tabindex="-1"></a>\mathbb{E} \left<span class="co">[</span><span class="ot">Z_n^\ell | Z_{1:n-1}^\ell \right</span><span class="co">]</span> = B_k (A_k \mu_{k-1} + a_k) + b_k </span>
<span id="cb16-1402"><a href="#cb16-1402" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1403"><a href="#cb16-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1404"><a href="#cb16-1404" aria-hidden="true" tabindex="-1"></a>and </span>
<span id="cb16-1405"><a href="#cb16-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1406"><a href="#cb16-1406" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1407"><a href="#cb16-1407" aria-hidden="true" tabindex="-1"></a>\mathbb{V} \left<span class="co">[</span><span class="ot">Z_n^\ell | Z_{1:n-1}^\ell \right</span><span class="co">]</span> = B_k (A_k \Sigma_k A_k^T + Q_k) B_k^T + R_k  </span>
<span id="cb16-1408"><a href="#cb16-1408" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1409"><a href="#cb16-1409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1410"><a href="#cb16-1410" aria-hidden="true" tabindex="-1"></a>following the previously introduced notation. Note that given the values of $A_k, B_k, a_k, b_k$ in our model these equations are simplified in practice, e.g. $B_k = I, b_k = 0$ and $A_k \mu_{k-1} + a_k = \mu_{k-1} + \Delta_k(\lfloor \mu_{k-1} \rfloor)$.</span>
<span id="cb16-1411"><a href="#cb16-1411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1412"><a href="#cb16-1412" aria-hidden="true" tabindex="-1"></a>In $\mathbb{R}^2$, values of the cumulative distribution function (cdf) of a multivariate Gaussian distribution are easy to compute.</span>
<span id="cb16-1413"><a href="#cb16-1413" aria-hidden="true" tabindex="-1"></a>Denote with $F_n^\ell$ the cdf of $\mathbb{L}_n^\ell$.</span>
<span id="cb16-1414"><a href="#cb16-1414" aria-hidden="true" tabindex="-1"></a>If $V_\delta(z)$ is a squared neighborhood of size $\delta$ and centered on $z=(x,y) \in \mathbb{R}^2$, then, denoting with $\mathbb{L}_n^\ell$ the distribution of $Z_n^\ell$ given $Z_{1:n-1}^\ell$:</span>
<span id="cb16-1415"><a href="#cb16-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1416"><a href="#cb16-1416" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1417"><a href="#cb16-1417" aria-hidden="true" tabindex="-1"></a>\mathbb{L}_n^{\ell}(V_\delta(z)) = F_n^\ell(x+\delta,y+\delta) + F_n^\ell(x-\delta,y-\delta) - \left<span class="co">[</span><span class="ot">F_n^\ell(x+\delta,y-\delta) + F_n^\ell(x-\delta,y+\delta)\right</span><span class="co">]</span></span>
<span id="cb16-1418"><a href="#cb16-1418" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1419"><a href="#cb16-1419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1420"><a href="#cb16-1420" aria-hidden="true" tabindex="-1"></a>This allows easy computation of $P(i,\ell) = \mathbb{L}_n^\ell(V_\delta(z_n^i))$.</span>
<span id="cb16-1421"><a href="#cb16-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1422"><a href="#cb16-1422" aria-hidden="true" tabindex="-1"></a><span class="fu">### Impact of the filtering algorithm {#sec-impact-algorithm-appendix}</span></span>
<span id="cb16-1423"><a href="#cb16-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1424"><a href="#cb16-1424" aria-hidden="true" tabindex="-1"></a>An advantage of the data association method proposed in @sec-data_association</span>
<span id="cb16-1425"><a href="#cb16-1425" aria-hidden="true" tabindex="-1"></a>is that it is very generic and does not constrain the tracking solution to any</span>
<span id="cb16-1426"><a href="#cb16-1426" aria-hidden="true" tabindex="-1"></a>particular choice of filtering algorithm. As for EKF, UKF implementations are</span>
<span id="cb16-1427"><a href="#cb16-1427" aria-hidden="true" tabindex="-1"></a>already available to compute the distribution of $Z_k$ given $Z_{1:k-1}$ and</span>
<span id="cb16-1428"><a href="#cb16-1428" aria-hidden="true" tabindex="-1"></a>the corresponding confidence regions (see @sec-tracking_module_appendix above). We propose a solution to compute</span>
<span id="cb16-1429"><a href="#cb16-1429" aria-hidden="true" tabindex="-1"></a>this distribution when SMC is used, and performance comparisons between the</span>
<span id="cb16-1430"><a href="#cb16-1430" aria-hidden="true" tabindex="-1"></a>EKF, UKF and SMC versions of our trackers are discussed.</span>
<span id="cb16-1431"><a href="#cb16-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1432"><a href="#cb16-1432" aria-hidden="true" tabindex="-1"></a><span class="fu">### SMC-based tracking</span></span>
<span id="cb16-1433"><a href="#cb16-1433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1434"><a href="#cb16-1434" aria-hidden="true" tabindex="-1"></a>Denote $\mathbb{Q}_k$ the filtering distribution (ie. that of $Z_k$ given $X_{1:k}$) for the HMM $(X_k,Z_k)_{k \geq 1}$  (omitting the dependency on the observations for notation ease).</span>
<span id="cb16-1435"><a href="#cb16-1435" aria-hidden="true" tabindex="-1"></a>Using a set of samples $<span class="sc">\{</span>X_k^i<span class="sc">\}</span>_{1 \leq i \leq N}$ and importance weights $\{w_k^i\}_{1 \leq i \leq N}$, SMC methods build an approximation of the following form:</span>
<span id="cb16-1436"><a href="#cb16-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1437"><a href="#cb16-1437" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1438"><a href="#cb16-1438" aria-hidden="true" tabindex="-1"></a>\widehat{\mathbb{Q}}^{SMC}_k(\mathrm{d} x_k) = \sum_{i=1}^N w_k^i \delta_{X_k^i}(\mathrm{d} x_k) \,.</span>
<span id="cb16-1439"><a href="#cb16-1439" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-1440"><a href="#cb16-1440" aria-hidden="true" tabindex="-1"></a>Contrary to EKF and UKF, the distribution $\mathbb{L}_k$ of $Z_k$ given $Z_{1:k-1}$ is not directly available but can be obtained via an additional Monte Carlo sampling step.</span>
<span id="cb16-1441"><a href="#cb16-1441" aria-hidden="true" tabindex="-1"></a>Marginalizing over $(X_{k-1}$, $X_k)$ and using the conditional independence properties of HMMs, we decompose $\mathbb{L}_k$ using the conditional state transition $\mathbb{M}_k(x,\mathrm{d} x')$ and the likelihood of $Z_k$ given $X_k$, denoted by $\mathbb{G}_k(x, \mathrm{d} z)$:</span>
<span id="cb16-1442"><a href="#cb16-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1443"><a href="#cb16-1443" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1444"><a href="#cb16-1444" aria-hidden="true" tabindex="-1"></a>\mathbb{L}_k(\mathrm{d} z_k) = \int \int \mathbb{G}_k(x_k, \mathrm{d} z_k)\mathbb{M}_k(x_{k-1}, \mathrm{d} x_k)\mathbb{Q}_{k-1}(\mathrm{d} x_{k-1}) \,.</span>
<span id="cb16-1445"><a href="#cb16-1445" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-1446"><a href="#cb16-1446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1447"><a href="#cb16-1447" aria-hidden="true" tabindex="-1"></a>Replacing $\mathbb{Q}_{k-1}$ with $\widehat{\mathbb{Q}}^{SMC}_{k-1}$ into the previous equation yields</span>
<span id="cb16-1448"><a href="#cb16-1448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1449"><a href="#cb16-1449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1450"><a href="#cb16-1450" aria-hidden="true" tabindex="-1"></a>\widehat{\mathbb{L}}^{SMC}_k(\mathrm{d} z_k) = \sum_{k=1}^N w_k^i \int \mathbb{G}_k(x_k,\mathrm{d} z_k) \mathbb{M}_k(X_{k-1}^i, \mathrm{d} x_k)  \,.</span>
<span id="cb16-1451"><a href="#cb16-1451" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1452"><a href="#cb16-1452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1453"><a href="#cb16-1453" aria-hidden="true" tabindex="-1"></a>In our model, the state transition is Gaussian and therefore easy to sample from.</span>
<span id="cb16-1454"><a href="#cb16-1454" aria-hidden="true" tabindex="-1"></a>Thus an approximated predictive distribution $\widehat{\mathbb{L}}_k$ can be obtained using Monte Carlo estimates built from random samples $\{X_k^{i,j}\}_{1 \leq i \leq N}^{1 \leq j \leq M}$ drawn from $\mathbb{M}_k(X_{k-1}^i, \mathrm{d} x_k)$.</span>
<span id="cb16-1455"><a href="#cb16-1455" aria-hidden="true" tabindex="-1"></a>This leads to</span>
<span id="cb16-1456"><a href="#cb16-1456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1457"><a href="#cb16-1457" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1458"><a href="#cb16-1458" aria-hidden="true" tabindex="-1"></a>\widehat{\mathbb{L}}_k(\mathrm{d} z_k) = \sum_{i=1}^N \sum_{j=1}^M w_k^i \mathbb{G}_k(X_k^{i,j},\mathrm{d} z_k) \,.</span>
<span id="cb16-1459"><a href="#cb16-1459" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-1460"><a href="#cb16-1460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1461"><a href="#cb16-1461" aria-hidden="true" tabindex="-1"></a>Since the observation likelihood is also Gaussian, $\widehat{\mathbb{L}}_k$ is</span>
<span id="cb16-1462"><a href="#cb16-1462" aria-hidden="true" tabindex="-1"></a>a Gaussian mixture, thus values of $\widehat{\mathbb{L}}_k(\mathsf{A})$ for</span>
<span id="cb16-1463"><a href="#cb16-1463" aria-hidden="true" tabindex="-1"></a>any $\mathsf{A} \subset \mathbb{R}^2$ can be computed by applying the tools</span>
<span id="cb16-1464"><a href="#cb16-1464" aria-hidden="true" tabindex="-1"></a>from @sec-confidence_regions_appendix to all mixture components.</span>
<span id="cb16-1465"><a href="#cb16-1465" aria-hidden="true" tabindex="-1"></a>Similar to EKF and UKF, this approximated predictive distribution is used to</span>
<span id="cb16-1466"><a href="#cb16-1466" aria-hidden="true" tabindex="-1"></a>recover object identities via $\widehat{\mathbb{L}}_n^{\ell}(V_\delta(z_n^i))$</span>
<span id="cb16-1467"><a href="#cb16-1467" aria-hidden="true" tabindex="-1"></a>computed for all incoming detections $\mathcal{D}_n = \{z_n^i\}_{1 \leq i \leq D_n}$ and each of the $1 \leq \ell \leq L_n$ filters, where $\widehat{\mathbb{L}}_n^{\ell}$ is the predictive distribution associated with the $\ell$-th filter.</span>
<span id="cb16-1468"><a href="#cb16-1468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1469"><a href="#cb16-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1470"><a href="#cb16-1470" aria-hidden="true" tabindex="-1"></a><span class="fu">### Performance comparison</span></span>
<span id="cb16-1471"><a href="#cb16-1471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1472"><a href="#cb16-1472" aria-hidden="true" tabindex="-1"></a>In theory, sampling-based methods like UKF and SMC are better suited for</span>
<span id="cb16-1473"><a href="#cb16-1473" aria-hidden="true" tabindex="-1"></a>nonlinear state space models like the one we propose in @sec-state_space_model.</span>
<span id="cb16-1474"><a href="#cb16-1474" aria-hidden="true" tabindex="-1"></a>However, we observe very few differences in count results when upgrading from EKF to UKF to SMC.</span>
<span id="cb16-1475"><a href="#cb16-1475" aria-hidden="true" tabindex="-1"></a>In practise, there is no difference at all between our EKF and UKF implementations, which show strictly identical values for $\mathsf{\hat{N}_{true}}$, $\mathsf{\hat{N}_{false}}$ and $\mathsf{\hat{N}_{red}}$.</span>
<span id="cb16-1476"><a href="#cb16-1476" aria-hidden="true" tabindex="-1"></a>For the SMC version, values for $\mathsf{\hat{N}_{false}}$ and $\mathsf{\hat{N}_{red}}$ improve by a very small amount (2 and 1, respectively), but $\mathsf{\hat{N}_{mis}}$ is slightly worse (one more object missed), and these results depend loosely on the number of samples used to approximate the filtering distributions and the number of samples for the Monte Carlo scheme.</span>
<span id="cb16-1477"><a href="#cb16-1477" aria-hidden="true" tabindex="-1"></a>Therefore, our motion estimates via the optical flow $\Delta_n$ prove very reliable in our application context, so much that EKF, though suboptimal, brings equivalent results.</span>
<span id="cb16-1478"><a href="#cb16-1478" aria-hidden="true" tabindex="-1"></a>This comforts us into keeping it as a faster and computationally simpler option.</span>
<span id="cb16-1479"><a href="#cb16-1479" aria-hidden="true" tabindex="-1"></a>That said, this conclusion might not hold in scenarios where camera motion is even stronger, which was our main motivation to develop a flexible tracking solution and to provide implementations of UKF and SMC versions.</span>
<span id="cb16-1480"><a href="#cb16-1480" aria-hidden="true" tabindex="-1"></a>This allows easier extension of our work to more challenging data.</span>
<span id="cb16-1481"><a href="#cb16-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1482"><a href="#cb16-1482" aria-hidden="true" tabindex="-1"></a><span class="fu"># References {.unnumbered}</span></span>
<span id="cb16-1483"><a href="#cb16-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1484"><a href="#cb16-1484" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb16-1485"><a href="#cb16-1485" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>